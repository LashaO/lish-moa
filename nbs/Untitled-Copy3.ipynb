{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import copy\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = pd.read_csv('data/train_features.csv')\n",
    "train_targets_scored = pd.read_csv('data/train_targets_scored.csv')\n",
    "train_targets_nonscored = pd.read_csv('data/train_targets_nonscored.csv')\n",
    "\n",
    "test_features = pd.read_csv('data/test_features.csv')\n",
    "sample_submission = pd.read_csv('data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENES = [col for col in train_features.columns if col.startswith('g-')]\n",
    "CELLS = [col for col in train_features.columns if col.startswith('c-')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "atp-sensitive_potassium_channel_antagonist      1\n",
       "erbb2_inhibitor                                 1\n",
       "diuretic                                        6\n",
       "autotaxin_inhibitor                             6\n",
       "protein_phosphatase_inhibitor                   6\n",
       "                                             ... \n",
       "serotonin_receptor_antagonist                 404\n",
       "dopamine_receptor_antagonist                  424\n",
       "cyclooxygenase_inhibitor                      435\n",
       "proteasome_inhibitor                          726\n",
       "nfkb_inhibitor                                832\n",
       "Length: 206, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_targets_scored.sum()[1:].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['trt_cp', 'ctl_vehicle'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features['cp_type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27796, 772)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_comp = 250\n",
    "\n",
    "data = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])\n",
    "print(data.shape)\n",
    "pca = (PCA(n_components=n_comp, random_state=42).fit(data[GENES]))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.7 s, sys: 10 s, total: 21.7 s\n",
      "Wall time: 1.98 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# GENES\n",
    "n_comp = 100\n",
    "\n",
    "data = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])\n",
    "data2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[GENES]))\n",
    "train2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n",
    "\n",
    "train2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n",
    "test2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n",
    "\n",
    "# drop_cols = [f'c-{i}' for i in range(n_comp,len(GENES))]\n",
    "train_features = pd.concat((train_features, train2), axis=1)\n",
    "test_features = pd.concat((test_features, test2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27796, 100)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_comp = 60\n",
    "\n",
    "data = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])\n",
    "print(data.shape)\n",
    "pca = (PCA(n_components=n_comp, random_state=42).fit(data[CELLS]))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 792 ms, sys: 1.35 s, total: 2.14 s\n",
      "Wall time: 180 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#CELLS\n",
    "n_comp = 15\n",
    "\n",
    "data = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])\n",
    "data2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[CELLS]))\n",
    "train2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n",
    "\n",
    "train2 = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n",
    "test2 = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n",
    "\n",
    "# drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]\n",
    "train_features = pd.concat((train_features, train2), axis=1)\n",
    "test_features = pd.concat((test_features, test2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sig_id</th>\n",
       "      <th>cp_type</th>\n",
       "      <th>cp_time</th>\n",
       "      <th>cp_dose</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>...</th>\n",
       "      <th>967</th>\n",
       "      <th>968</th>\n",
       "      <th>969</th>\n",
       "      <th>970</th>\n",
       "      <th>971</th>\n",
       "      <th>972</th>\n",
       "      <th>973</th>\n",
       "      <th>974</th>\n",
       "      <th>975</th>\n",
       "      <th>976</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_000644bb2</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>24</td>\n",
       "      <td>D1</td>\n",
       "      <td>1.0620</td>\n",
       "      <td>0.5577</td>\n",
       "      <td>-0.2479</td>\n",
       "      <td>-0.6208</td>\n",
       "      <td>-0.1944</td>\n",
       "      <td>-1.0120</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.450285</td>\n",
       "      <td>-0.176778</td>\n",
       "      <td>-1.262943</td>\n",
       "      <td>0.219107</td>\n",
       "      <td>-0.890670</td>\n",
       "      <td>0.393604</td>\n",
       "      <td>-0.703376</td>\n",
       "      <td>-0.615139</td>\n",
       "      <td>0.174407</td>\n",
       "      <td>0.082941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_000779bfc</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>72</td>\n",
       "      <td>D1</td>\n",
       "      <td>0.0743</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.2991</td>\n",
       "      <td>0.0604</td>\n",
       "      <td>1.0190</td>\n",
       "      <td>0.5207</td>\n",
       "      <td>...</td>\n",
       "      <td>0.063234</td>\n",
       "      <td>0.658824</td>\n",
       "      <td>0.429385</td>\n",
       "      <td>-0.226422</td>\n",
       "      <td>0.271831</td>\n",
       "      <td>0.863835</td>\n",
       "      <td>0.003597</td>\n",
       "      <td>0.669397</td>\n",
       "      <td>0.447651</td>\n",
       "      <td>1.207365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_000a6266a</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>48</td>\n",
       "      <td>D1</td>\n",
       "      <td>0.6280</td>\n",
       "      <td>0.5817</td>\n",
       "      <td>1.5540</td>\n",
       "      <td>-0.0764</td>\n",
       "      <td>-0.0323</td>\n",
       "      <td>1.2390</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.115802</td>\n",
       "      <td>0.726273</td>\n",
       "      <td>-0.212644</td>\n",
       "      <td>-0.902482</td>\n",
       "      <td>-0.118799</td>\n",
       "      <td>-0.336548</td>\n",
       "      <td>0.015536</td>\n",
       "      <td>0.572233</td>\n",
       "      <td>-0.261651</td>\n",
       "      <td>-0.638141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_0015fd391</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>48</td>\n",
       "      <td>D1</td>\n",
       "      <td>-0.5138</td>\n",
       "      <td>-0.2491</td>\n",
       "      <td>-0.2656</td>\n",
       "      <td>0.5288</td>\n",
       "      <td>4.0620</td>\n",
       "      <td>-0.8095</td>\n",
       "      <td>...</td>\n",
       "      <td>0.590366</td>\n",
       "      <td>0.698760</td>\n",
       "      <td>0.050321</td>\n",
       "      <td>-0.793301</td>\n",
       "      <td>0.295411</td>\n",
       "      <td>0.147857</td>\n",
       "      <td>0.056161</td>\n",
       "      <td>0.689218</td>\n",
       "      <td>-1.433683</td>\n",
       "      <td>1.323147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_001626bd3</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>72</td>\n",
       "      <td>D2</td>\n",
       "      <td>-0.3254</td>\n",
       "      <td>-0.4009</td>\n",
       "      <td>0.9700</td>\n",
       "      <td>0.6919</td>\n",
       "      <td>1.4180</td>\n",
       "      <td>-0.8244</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000223</td>\n",
       "      <td>-0.287454</td>\n",
       "      <td>-0.110246</td>\n",
       "      <td>-0.105291</td>\n",
       "      <td>-0.396913</td>\n",
       "      <td>0.090983</td>\n",
       "      <td>-0.211590</td>\n",
       "      <td>0.350304</td>\n",
       "      <td>-0.326626</td>\n",
       "      <td>-0.344389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23809</th>\n",
       "      <td>id_fffb1ceed</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>24</td>\n",
       "      <td>D2</td>\n",
       "      <td>0.1394</td>\n",
       "      <td>-0.0636</td>\n",
       "      <td>-0.1112</td>\n",
       "      <td>-0.5080</td>\n",
       "      <td>-0.4713</td>\n",
       "      <td>0.7201</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.492270</td>\n",
       "      <td>0.802396</td>\n",
       "      <td>0.332499</td>\n",
       "      <td>-0.204876</td>\n",
       "      <td>0.238577</td>\n",
       "      <td>-0.483204</td>\n",
       "      <td>0.585078</td>\n",
       "      <td>0.173586</td>\n",
       "      <td>-0.611718</td>\n",
       "      <td>1.607084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23810</th>\n",
       "      <td>id_fffb70c0c</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>24</td>\n",
       "      <td>D2</td>\n",
       "      <td>-1.3260</td>\n",
       "      <td>0.3478</td>\n",
       "      <td>-0.3743</td>\n",
       "      <td>0.9905</td>\n",
       "      <td>-0.7178</td>\n",
       "      <td>0.6621</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.364079</td>\n",
       "      <td>-0.375444</td>\n",
       "      <td>-1.433534</td>\n",
       "      <td>-0.858483</td>\n",
       "      <td>1.072457</td>\n",
       "      <td>0.101450</td>\n",
       "      <td>0.435098</td>\n",
       "      <td>-0.219500</td>\n",
       "      <td>0.377156</td>\n",
       "      <td>0.555680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23811</th>\n",
       "      <td>id_fffc1c3f4</td>\n",
       "      <td>ctl_vehicle</td>\n",
       "      <td>48</td>\n",
       "      <td>D2</td>\n",
       "      <td>0.3942</td>\n",
       "      <td>0.3756</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>-0.7389</td>\n",
       "      <td>0.5505</td>\n",
       "      <td>-0.0159</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.511130</td>\n",
       "      <td>-0.035609</td>\n",
       "      <td>-0.310135</td>\n",
       "      <td>-0.166686</td>\n",
       "      <td>-0.458886</td>\n",
       "      <td>-0.003948</td>\n",
       "      <td>0.292592</td>\n",
       "      <td>0.331622</td>\n",
       "      <td>-0.006669</td>\n",
       "      <td>0.081750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23812</th>\n",
       "      <td>id_fffcb9e7c</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>24</td>\n",
       "      <td>D1</td>\n",
       "      <td>0.6660</td>\n",
       "      <td>0.2324</td>\n",
       "      <td>0.4392</td>\n",
       "      <td>0.2044</td>\n",
       "      <td>0.8531</td>\n",
       "      <td>-0.0343</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.129357</td>\n",
       "      <td>0.020524</td>\n",
       "      <td>-0.043233</td>\n",
       "      <td>-0.440007</td>\n",
       "      <td>0.302835</td>\n",
       "      <td>0.776086</td>\n",
       "      <td>-1.737516</td>\n",
       "      <td>-0.531532</td>\n",
       "      <td>-0.351892</td>\n",
       "      <td>0.542268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23813</th>\n",
       "      <td>id_ffffdd77b</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>72</td>\n",
       "      <td>D1</td>\n",
       "      <td>-0.8598</td>\n",
       "      <td>1.0240</td>\n",
       "      <td>-0.1361</td>\n",
       "      <td>0.7952</td>\n",
       "      <td>-0.3611</td>\n",
       "      <td>-3.6750</td>\n",
       "      <td>...</td>\n",
       "      <td>3.549881</td>\n",
       "      <td>0.367554</td>\n",
       "      <td>1.124858</td>\n",
       "      <td>2.358549</td>\n",
       "      <td>4.598061</td>\n",
       "      <td>0.405195</td>\n",
       "      <td>0.448931</td>\n",
       "      <td>3.509945</td>\n",
       "      <td>1.710206</td>\n",
       "      <td>-2.434251</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23814 rows Ã— 981 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             sig_id      cp_type cp_time cp_dose       0       1       2  \\\n",
       "0      id_000644bb2       trt_cp      24      D1  1.0620  0.5577 -0.2479   \n",
       "1      id_000779bfc       trt_cp      72      D1  0.0743  0.4087  0.2991   \n",
       "2      id_000a6266a       trt_cp      48      D1  0.6280  0.5817  1.5540   \n",
       "3      id_0015fd391       trt_cp      48      D1 -0.5138 -0.2491 -0.2656   \n",
       "4      id_001626bd3       trt_cp      72      D2 -0.3254 -0.4009  0.9700   \n",
       "...             ...          ...     ...     ...     ...     ...     ...   \n",
       "23809  id_fffb1ceed       trt_cp      24      D2  0.1394 -0.0636 -0.1112   \n",
       "23810  id_fffb70c0c       trt_cp      24      D2 -1.3260  0.3478 -0.3743   \n",
       "23811  id_fffc1c3f4  ctl_vehicle      48      D2  0.3942  0.3756  0.3109   \n",
       "23812  id_fffcb9e7c       trt_cp      24      D1  0.6660  0.2324  0.4392   \n",
       "23813  id_ffffdd77b       trt_cp      72      D1 -0.8598  1.0240 -0.1361   \n",
       "\n",
       "            3       4       5  ...       967       968       969       970  \\\n",
       "0     -0.6208 -0.1944 -1.0120  ... -0.450285 -0.176778 -1.262943  0.219107   \n",
       "1      0.0604  1.0190  0.5207  ...  0.063234  0.658824  0.429385 -0.226422   \n",
       "2     -0.0764 -0.0323  1.2390  ... -0.115802  0.726273 -0.212644 -0.902482   \n",
       "3      0.5288  4.0620 -0.8095  ...  0.590366  0.698760  0.050321 -0.793301   \n",
       "4      0.6919  1.4180 -0.8244  ... -0.000223 -0.287454 -0.110246 -0.105291   \n",
       "...       ...     ...     ...  ...       ...       ...       ...       ...   \n",
       "23809 -0.5080 -0.4713  0.7201  ... -0.492270  0.802396  0.332499 -0.204876   \n",
       "23810  0.9905 -0.7178  0.6621  ... -1.364079 -0.375444 -1.433534 -0.858483   \n",
       "23811 -0.7389  0.5505 -0.0159  ... -0.511130 -0.035609 -0.310135 -0.166686   \n",
       "23812  0.2044  0.8531 -0.0343  ... -1.129357  0.020524 -0.043233 -0.440007   \n",
       "23813  0.7952 -0.3611 -3.6750  ...  3.549881  0.367554  1.124858  2.358549   \n",
       "\n",
       "            971       972       973       974       975       976  \n",
       "0     -0.890670  0.393604 -0.703376 -0.615139  0.174407  0.082941  \n",
       "1      0.271831  0.863835  0.003597  0.669397  0.447651  1.207365  \n",
       "2     -0.118799 -0.336548  0.015536  0.572233 -0.261651 -0.638141  \n",
       "3      0.295411  0.147857  0.056161  0.689218 -1.433683  1.323147  \n",
       "4     -0.396913  0.090983 -0.211590  0.350304 -0.326626 -0.344389  \n",
       "...         ...       ...       ...       ...       ...       ...  \n",
       "23809  0.238577 -0.483204  0.585078  0.173586 -0.611718  1.607084  \n",
       "23810  1.072457  0.101450  0.435098 -0.219500  0.377156  0.555680  \n",
       "23811 -0.458886 -0.003948  0.292592  0.331622 -0.006669  0.081750  \n",
       "23812  0.302835  0.776086 -1.737516 -0.531532 -0.351892  0.542268  \n",
       "23813  4.598061  0.405195  0.448931  3.509945  1.710206 -2.434251  \n",
       "\n",
       "[23814 rows x 981 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "\n",
    "var_thresh = VarianceThreshold(threshold=0.5)\n",
    "data = train_features.append(test_features)\n",
    "data_transformed = var_thresh.fit_transform(data.iloc[:, 4:])\n",
    "\n",
    "train_features_transformed = data_transformed[ : train_features.shape[0]]\n",
    "test_features_transformed = data_transformed[-test_features.shape[0] : ]\n",
    "\n",
    "\n",
    "train_features = pd.DataFrame(train_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n",
    "                              columns=['sig_id','cp_type','cp_time','cp_dose'])\n",
    "\n",
    "train_features = pd.concat([train_features, pd.DataFrame(train_features_transformed)], axis=1)\n",
    "\n",
    "\n",
    "test_features = pd.DataFrame(test_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n",
    "                             columns=['sig_id','cp_type','cp_time','cp_dose'])\n",
    "\n",
    "test_features = pd.concat([test_features, pd.DataFrame(test_features_transformed)], axis=1)\n",
    "\n",
    "train_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_features.merge(train_targets_scored, on='sig_id')\n",
    "train = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n",
    "test = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n",
    "\n",
    "target = train[train_targets_scored.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop('cp_type', axis=1)\n",
    "test = test.drop('cp_type', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sig_id</th>\n",
       "      <th>cp_time</th>\n",
       "      <th>cp_dose</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>...</th>\n",
       "      <th>tropomyosin_receptor_kinase_inhibitor</th>\n",
       "      <th>trpv_agonist</th>\n",
       "      <th>trpv_antagonist</th>\n",
       "      <th>tubulin_inhibitor</th>\n",
       "      <th>tyrosine_kinase_inhibitor</th>\n",
       "      <th>ubiquitin_specific_protease_inhibitor</th>\n",
       "      <th>vegfr_inhibitor</th>\n",
       "      <th>vitamin_b</th>\n",
       "      <th>vitamin_d_receptor_agonist</th>\n",
       "      <th>wnt_inhibitor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_000644bb2</td>\n",
       "      <td>24</td>\n",
       "      <td>D1</td>\n",
       "      <td>1.0620</td>\n",
       "      <td>0.5577</td>\n",
       "      <td>-0.2479</td>\n",
       "      <td>-0.6208</td>\n",
       "      <td>-0.1944</td>\n",
       "      <td>-1.0120</td>\n",
       "      <td>-1.0220</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_000779bfc</td>\n",
       "      <td>72</td>\n",
       "      <td>D1</td>\n",
       "      <td>0.0743</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.2991</td>\n",
       "      <td>0.0604</td>\n",
       "      <td>1.0190</td>\n",
       "      <td>0.5207</td>\n",
       "      <td>0.2341</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_000a6266a</td>\n",
       "      <td>48</td>\n",
       "      <td>D1</td>\n",
       "      <td>0.6280</td>\n",
       "      <td>0.5817</td>\n",
       "      <td>1.5540</td>\n",
       "      <td>-0.0764</td>\n",
       "      <td>-0.0323</td>\n",
       "      <td>1.2390</td>\n",
       "      <td>0.1715</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_0015fd391</td>\n",
       "      <td>48</td>\n",
       "      <td>D1</td>\n",
       "      <td>-0.5138</td>\n",
       "      <td>-0.2491</td>\n",
       "      <td>-0.2656</td>\n",
       "      <td>0.5288</td>\n",
       "      <td>4.0620</td>\n",
       "      <td>-0.8095</td>\n",
       "      <td>-1.9590</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_001626bd3</td>\n",
       "      <td>72</td>\n",
       "      <td>D2</td>\n",
       "      <td>-0.3254</td>\n",
       "      <td>-0.4009</td>\n",
       "      <td>0.9700</td>\n",
       "      <td>0.6919</td>\n",
       "      <td>1.4180</td>\n",
       "      <td>-0.8244</td>\n",
       "      <td>-0.2800</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21943</th>\n",
       "      <td>id_fff8c2444</td>\n",
       "      <td>72</td>\n",
       "      <td>D1</td>\n",
       "      <td>0.1608</td>\n",
       "      <td>-1.0500</td>\n",
       "      <td>0.2551</td>\n",
       "      <td>-0.2239</td>\n",
       "      <td>-0.2431</td>\n",
       "      <td>0.4256</td>\n",
       "      <td>-0.1166</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21944</th>\n",
       "      <td>id_fffb1ceed</td>\n",
       "      <td>24</td>\n",
       "      <td>D2</td>\n",
       "      <td>0.1394</td>\n",
       "      <td>-0.0636</td>\n",
       "      <td>-0.1112</td>\n",
       "      <td>-0.5080</td>\n",
       "      <td>-0.4713</td>\n",
       "      <td>0.7201</td>\n",
       "      <td>0.5773</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21945</th>\n",
       "      <td>id_fffb70c0c</td>\n",
       "      <td>24</td>\n",
       "      <td>D2</td>\n",
       "      <td>-1.3260</td>\n",
       "      <td>0.3478</td>\n",
       "      <td>-0.3743</td>\n",
       "      <td>0.9905</td>\n",
       "      <td>-0.7178</td>\n",
       "      <td>0.6621</td>\n",
       "      <td>-0.2252</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21946</th>\n",
       "      <td>id_fffcb9e7c</td>\n",
       "      <td>24</td>\n",
       "      <td>D1</td>\n",
       "      <td>0.6660</td>\n",
       "      <td>0.2324</td>\n",
       "      <td>0.4392</td>\n",
       "      <td>0.2044</td>\n",
       "      <td>0.8531</td>\n",
       "      <td>-0.0343</td>\n",
       "      <td>0.0323</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21947</th>\n",
       "      <td>id_ffffdd77b</td>\n",
       "      <td>72</td>\n",
       "      <td>D1</td>\n",
       "      <td>-0.8598</td>\n",
       "      <td>1.0240</td>\n",
       "      <td>-0.1361</td>\n",
       "      <td>0.7952</td>\n",
       "      <td>-0.3611</td>\n",
       "      <td>-3.6750</td>\n",
       "      <td>-1.2420</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21948 rows Ã— 1186 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             sig_id cp_time cp_dose       0       1       2       3       4  \\\n",
       "0      id_000644bb2      24      D1  1.0620  0.5577 -0.2479 -0.6208 -0.1944   \n",
       "1      id_000779bfc      72      D1  0.0743  0.4087  0.2991  0.0604  1.0190   \n",
       "2      id_000a6266a      48      D1  0.6280  0.5817  1.5540 -0.0764 -0.0323   \n",
       "3      id_0015fd391      48      D1 -0.5138 -0.2491 -0.2656  0.5288  4.0620   \n",
       "4      id_001626bd3      72      D2 -0.3254 -0.4009  0.9700  0.6919  1.4180   \n",
       "...             ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "21943  id_fff8c2444      72      D1  0.1608 -1.0500  0.2551 -0.2239 -0.2431   \n",
       "21944  id_fffb1ceed      24      D2  0.1394 -0.0636 -0.1112 -0.5080 -0.4713   \n",
       "21945  id_fffb70c0c      24      D2 -1.3260  0.3478 -0.3743  0.9905 -0.7178   \n",
       "21946  id_fffcb9e7c      24      D1  0.6660  0.2324  0.4392  0.2044  0.8531   \n",
       "21947  id_ffffdd77b      72      D1 -0.8598  1.0240 -0.1361  0.7952 -0.3611   \n",
       "\n",
       "            5       6  ...  tropomyosin_receptor_kinase_inhibitor  \\\n",
       "0     -1.0120 -1.0220  ...                                      0   \n",
       "1      0.5207  0.2341  ...                                      0   \n",
       "2      1.2390  0.1715  ...                                      0   \n",
       "3     -0.8095 -1.9590  ...                                      0   \n",
       "4     -0.8244 -0.2800  ...                                      0   \n",
       "...       ...     ...  ...                                    ...   \n",
       "21943  0.4256 -0.1166  ...                                      0   \n",
       "21944  0.7201  0.5773  ...                                      0   \n",
       "21945  0.6621 -0.2252  ...                                      0   \n",
       "21946 -0.0343  0.0323  ...                                      0   \n",
       "21947 -3.6750 -1.2420  ...                                      0   \n",
       "\n",
       "       trpv_agonist  trpv_antagonist  tubulin_inhibitor  \\\n",
       "0                 0                0                  0   \n",
       "1                 0                0                  0   \n",
       "2                 0                0                  0   \n",
       "3                 0                0                  0   \n",
       "4                 0                0                  0   \n",
       "...             ...              ...                ...   \n",
       "21943             0                0                  0   \n",
       "21944             0                0                  0   \n",
       "21945             0                0                  0   \n",
       "21946             0                0                  0   \n",
       "21947             0                0                  0   \n",
       "\n",
       "       tyrosine_kinase_inhibitor  ubiquitin_specific_protease_inhibitor  \\\n",
       "0                              0                                      0   \n",
       "1                              0                                      0   \n",
       "2                              0                                      0   \n",
       "3                              0                                      0   \n",
       "4                              0                                      0   \n",
       "...                          ...                                    ...   \n",
       "21943                          0                                      0   \n",
       "21944                          0                                      0   \n",
       "21945                          0                                      0   \n",
       "21946                          0                                      0   \n",
       "21947                          0                                      0   \n",
       "\n",
       "       vegfr_inhibitor  vitamin_b  vitamin_d_receptor_agonist  wnt_inhibitor  \n",
       "0                    0          0                           0              0  \n",
       "1                    0          0                           0              0  \n",
       "2                    0          0                           0              0  \n",
       "3                    0          0                           0              0  \n",
       "4                    0          0                           0              0  \n",
       "...                ...        ...                         ...            ...  \n",
       "21943                0          0                           0              0  \n",
       "21944                0          0                           0              0  \n",
       "21945                0          0                           0              0  \n",
       "21946                0          0                           0              0  \n",
       "21947                0          0                           0              0  \n",
       "\n",
       "[21948 rows x 1186 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binning\n",
    "\n",
    "# for col in GENES:\n",
    "#     train.loc[:, f'{col}_bin'] = pd.cut(train[col], bins=3, labels=False)\n",
    "#     test.loc[:, f'{col}_bin'] = pd.cut(test[col], bins=3, labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution plots\n",
    "\n",
    "# plt.figure(figsize=(16,16))\n",
    "# sns.set_style(\"whitegrid\")\n",
    "\n",
    "# gene_choice = np.random.choice(len(GENES), 16)\n",
    "# for i, col in enumerate(gene_choice):\n",
    "#     plt.subplot(4, 4, i+1)\n",
    "#     plt.hist(train_features.loc[:, GENES[col]],bins=100, color='orange')\n",
    "#     plt.title(GENES[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ = train.copy() [Didn't wanted to actually normalize, so created a copy and normalized that for further calculation]\n",
    "# for col in GENES:\n",
    "    \n",
    "# #     train_[col] = (train[col]-np.mean(train[col])) / (np.std(train[col]))\n",
    "    \n",
    "#     mean = train_[col].mean()\n",
    "#     std = train_[col].std()\n",
    "\n",
    "#     std_r = mean + 4*std\n",
    "#     std_l = mean - 4*std\n",
    "\n",
    "#     drop = train_[col][(train_[col]>std_r) | (train_[col]<std_l)].index.values\n",
    "\n",
    "# train = train.drop(drop).reset_index(drop=True)\n",
    "# # folds = folds.drop(drop).reset_index(drop=True)\n",
    "# target = target.drop(drop).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_comp = 50\n",
    "\n",
    "# data = pd.concat([pd.DataFrame(train[CELLS]), pd.DataFrame(test[CELLS])])\n",
    "# data2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[CELLS]))\n",
    "# train2 = data2[:train.shape[0]]; test2 = data2[train.shape[0]:]\n",
    "\n",
    "# train2 = pd.DataFrame(train2, columns=[f'c-{i}' for i in range(n_comp)])\n",
    "# test2 = pd.DataFrame(test2, columns=[f'c-{i}' for i in range(n_comp)])\n",
    "\n",
    "# drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]\n",
    "# train = train.drop(columns=drop_cols)\n",
    "# test = test.drop(columns=drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols = target.drop('sig_id', axis=1).columns.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sig_id</th>\n",
       "      <th>cp_time</th>\n",
       "      <th>cp_dose</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>...</th>\n",
       "      <th>trpv_agonist</th>\n",
       "      <th>trpv_antagonist</th>\n",
       "      <th>tubulin_inhibitor</th>\n",
       "      <th>tyrosine_kinase_inhibitor</th>\n",
       "      <th>ubiquitin_specific_protease_inhibitor</th>\n",
       "      <th>vegfr_inhibitor</th>\n",
       "      <th>vitamin_b</th>\n",
       "      <th>vitamin_d_receptor_agonist</th>\n",
       "      <th>wnt_inhibitor</th>\n",
       "      <th>kfold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_000644bb2</td>\n",
       "      <td>24</td>\n",
       "      <td>D1</td>\n",
       "      <td>1.0620</td>\n",
       "      <td>0.5577</td>\n",
       "      <td>-0.2479</td>\n",
       "      <td>-0.6208</td>\n",
       "      <td>-0.1944</td>\n",
       "      <td>-1.0120</td>\n",
       "      <td>-1.0220</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_000779bfc</td>\n",
       "      <td>72</td>\n",
       "      <td>D1</td>\n",
       "      <td>0.0743</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.2991</td>\n",
       "      <td>0.0604</td>\n",
       "      <td>1.0190</td>\n",
       "      <td>0.5207</td>\n",
       "      <td>0.2341</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_000a6266a</td>\n",
       "      <td>48</td>\n",
       "      <td>D1</td>\n",
       "      <td>0.6280</td>\n",
       "      <td>0.5817</td>\n",
       "      <td>1.5540</td>\n",
       "      <td>-0.0764</td>\n",
       "      <td>-0.0323</td>\n",
       "      <td>1.2390</td>\n",
       "      <td>0.1715</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_0015fd391</td>\n",
       "      <td>48</td>\n",
       "      <td>D1</td>\n",
       "      <td>-0.5138</td>\n",
       "      <td>-0.2491</td>\n",
       "      <td>-0.2656</td>\n",
       "      <td>0.5288</td>\n",
       "      <td>4.0620</td>\n",
       "      <td>-0.8095</td>\n",
       "      <td>-1.9590</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_001626bd3</td>\n",
       "      <td>72</td>\n",
       "      <td>D2</td>\n",
       "      <td>-0.3254</td>\n",
       "      <td>-0.4009</td>\n",
       "      <td>0.9700</td>\n",
       "      <td>0.6919</td>\n",
       "      <td>1.4180</td>\n",
       "      <td>-0.8244</td>\n",
       "      <td>-0.2800</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21943</th>\n",
       "      <td>id_fff8c2444</td>\n",
       "      <td>72</td>\n",
       "      <td>D1</td>\n",
       "      <td>0.1608</td>\n",
       "      <td>-1.0500</td>\n",
       "      <td>0.2551</td>\n",
       "      <td>-0.2239</td>\n",
       "      <td>-0.2431</td>\n",
       "      <td>0.4256</td>\n",
       "      <td>-0.1166</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21944</th>\n",
       "      <td>id_fffb1ceed</td>\n",
       "      <td>24</td>\n",
       "      <td>D2</td>\n",
       "      <td>0.1394</td>\n",
       "      <td>-0.0636</td>\n",
       "      <td>-0.1112</td>\n",
       "      <td>-0.5080</td>\n",
       "      <td>-0.4713</td>\n",
       "      <td>0.7201</td>\n",
       "      <td>0.5773</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21945</th>\n",
       "      <td>id_fffb70c0c</td>\n",
       "      <td>24</td>\n",
       "      <td>D2</td>\n",
       "      <td>-1.3260</td>\n",
       "      <td>0.3478</td>\n",
       "      <td>-0.3743</td>\n",
       "      <td>0.9905</td>\n",
       "      <td>-0.7178</td>\n",
       "      <td>0.6621</td>\n",
       "      <td>-0.2252</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21946</th>\n",
       "      <td>id_fffcb9e7c</td>\n",
       "      <td>24</td>\n",
       "      <td>D1</td>\n",
       "      <td>0.6660</td>\n",
       "      <td>0.2324</td>\n",
       "      <td>0.4392</td>\n",
       "      <td>0.2044</td>\n",
       "      <td>0.8531</td>\n",
       "      <td>-0.0343</td>\n",
       "      <td>0.0323</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21947</th>\n",
       "      <td>id_ffffdd77b</td>\n",
       "      <td>72</td>\n",
       "      <td>D1</td>\n",
       "      <td>-0.8598</td>\n",
       "      <td>1.0240</td>\n",
       "      <td>-0.1361</td>\n",
       "      <td>0.7952</td>\n",
       "      <td>-0.3611</td>\n",
       "      <td>-3.6750</td>\n",
       "      <td>-1.2420</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21948 rows Ã— 1187 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             sig_id cp_time cp_dose       0       1       2       3       4  \\\n",
       "0      id_000644bb2      24      D1  1.0620  0.5577 -0.2479 -0.6208 -0.1944   \n",
       "1      id_000779bfc      72      D1  0.0743  0.4087  0.2991  0.0604  1.0190   \n",
       "2      id_000a6266a      48      D1  0.6280  0.5817  1.5540 -0.0764 -0.0323   \n",
       "3      id_0015fd391      48      D1 -0.5138 -0.2491 -0.2656  0.5288  4.0620   \n",
       "4      id_001626bd3      72      D2 -0.3254 -0.4009  0.9700  0.6919  1.4180   \n",
       "...             ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "21943  id_fff8c2444      72      D1  0.1608 -1.0500  0.2551 -0.2239 -0.2431   \n",
       "21944  id_fffb1ceed      24      D2  0.1394 -0.0636 -0.1112 -0.5080 -0.4713   \n",
       "21945  id_fffb70c0c      24      D2 -1.3260  0.3478 -0.3743  0.9905 -0.7178   \n",
       "21946  id_fffcb9e7c      24      D1  0.6660  0.2324  0.4392  0.2044  0.8531   \n",
       "21947  id_ffffdd77b      72      D1 -0.8598  1.0240 -0.1361  0.7952 -0.3611   \n",
       "\n",
       "            5       6  ...  trpv_agonist  trpv_antagonist  tubulin_inhibitor  \\\n",
       "0     -1.0120 -1.0220  ...             0                0                  0   \n",
       "1      0.5207  0.2341  ...             0                0                  0   \n",
       "2      1.2390  0.1715  ...             0                0                  0   \n",
       "3     -0.8095 -1.9590  ...             0                0                  0   \n",
       "4     -0.8244 -0.2800  ...             0                0                  0   \n",
       "...       ...     ...  ...           ...              ...                ...   \n",
       "21943  0.4256 -0.1166  ...             0                0                  0   \n",
       "21944  0.7201  0.5773  ...             0                0                  0   \n",
       "21945  0.6621 -0.2252  ...             0                0                  0   \n",
       "21946 -0.0343  0.0323  ...             0                0                  0   \n",
       "21947 -3.6750 -1.2420  ...             0                0                  0   \n",
       "\n",
       "       tyrosine_kinase_inhibitor  ubiquitin_specific_protease_inhibitor  \\\n",
       "0                              0                                      0   \n",
       "1                              0                                      0   \n",
       "2                              0                                      0   \n",
       "3                              0                                      0   \n",
       "4                              0                                      0   \n",
       "...                          ...                                    ...   \n",
       "21943                          0                                      0   \n",
       "21944                          0                                      0   \n",
       "21945                          0                                      0   \n",
       "21946                          0                                      0   \n",
       "21947                          0                                      0   \n",
       "\n",
       "       vegfr_inhibitor  vitamin_b  vitamin_d_receptor_agonist  wnt_inhibitor  \\\n",
       "0                    0          0                           0              0   \n",
       "1                    0          0                           0              0   \n",
       "2                    0          0                           0              0   \n",
       "3                    0          0                           0              0   \n",
       "4                    0          0                           0              0   \n",
       "...                ...        ...                         ...            ...   \n",
       "21943                0          0                           0              0   \n",
       "21944                0          0                           0              0   \n",
       "21945                0          0                           0              0   \n",
       "21946                0          0                           0              0   \n",
       "21947                0          0                           0              0   \n",
       "\n",
       "       kfold  \n",
       "0          0  \n",
       "1          2  \n",
       "2          1  \n",
       "3          2  \n",
       "4          2  \n",
       "...      ...  \n",
       "21943      0  \n",
       "21944      4  \n",
       "21945      0  \n",
       "21946      1  \n",
       "21947      2  \n",
       "\n",
       "[21948 rows x 1187 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folds = train.copy()\n",
    "\n",
    "mskf = MultilabelStratifiedKFold(n_splits=5)\n",
    "\n",
    "for f, (t_idx, v_idx) in enumerate(mskf.split(X=train, y=target)):\n",
    "    folds.loc[v_idx, 'kfold'] = int(f)\n",
    "\n",
    "folds['kfold'] = folds['kfold'].astype(int)\n",
    "folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21948, 1186)\n",
      "(21948, 1187)\n",
      "(3624, 980)\n",
      "(21948, 207)\n",
      "(3982, 207)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(folds.shape)\n",
    "print(test.shape)\n",
    "print(target.shape)\n",
    "print(sample_submission.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoADataset:\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.features.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dct = {\n",
    "            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n",
    "            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n",
    "        }\n",
    "        return dct\n",
    "    \n",
    "class TestDataset:\n",
    "    def __init__(self, features):\n",
    "        self.features = features\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.features.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dct = {\n",
    "            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n",
    "        }\n",
    "        return dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n",
    "    model.train()\n",
    "    final_loss = 0\n",
    "    \n",
    "    for data in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs, targets = data['x'].to(device), data['y'].to(device)\n",
    "#         print(inputs.shape)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        final_loss += loss.item()\n",
    "        \n",
    "    final_loss /= len(dataloader)\n",
    "    \n",
    "    return final_loss\n",
    "\n",
    "\n",
    "def valid_fn(model, loss_fn, dataloader, device):\n",
    "    model.eval()\n",
    "    final_loss = 0\n",
    "    valid_preds = []\n",
    "    \n",
    "    for data in dataloader:\n",
    "        inputs, targets = data['x'].to(device), data['y'].to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        \n",
    "        final_loss += loss.item()\n",
    "        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
    "        \n",
    "    final_loss /= len(dataloader)\n",
    "    valid_preds = np.concatenate(valid_preds)\n",
    "    \n",
    "    return final_loss, valid_preds\n",
    "\n",
    "def inference_fn(model, dataloader, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    \n",
    "    for data in dataloader:\n",
    "        inputs = data['x'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "        \n",
    "        preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
    "        \n",
    "    preds = np.concatenate(preds)\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.tabular import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TabularModel(\n",
       "  (embeds): ModuleList(\n",
       "    (0): Embedding(4, 2)\n",
       "    (1): Embedding(17, 8)\n",
       "    (2): Embedding(17, 8)\n",
       "  )\n",
       "  (emb_drop): Dropout(p=0.04, inplace=False)\n",
       "  (bn_cont): BatchNorm1d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=18, out_features=1024, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.3, inplace=False)\n",
       "    (4): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Dropout(p=0.3, inplace=False)\n",
       "    (8): Linear(in_features=1024, out_features=200, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_szs = [(4,2), (17,8), (17,8)]\n",
    "#m = TabularModel(emb_szs, n_cont=2, out_sz=2, layers=[1000,500])\n",
    "\n",
    "m = TabularModel(emb_szs, n_cont=0, out_sz=200, layers=[1024,1024], ps=[0.3,0.3],\n",
    "                 emb_drop=0.04, y_range=None, use_bn=True, bn_final=False)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.245155855096883\n"
     ]
    }
   ],
   "source": [
    "#formula\n",
    "ns = train.shape[0]\n",
    "ni = target.shape[1]\n",
    "no = test.shape[1]\n",
    "\n",
    "alpha = 2\n",
    "\n",
    "\n",
    "nh = ns/(alpha * (ni + no))\n",
    "\n",
    "print(nh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "980"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "207"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21948"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.29"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "21948 / 1200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "numhidden = 18.29\n",
    "alpha = 2\n",
    "def calcHiddenLayer(alpha=alpha, numHiddenLayers=2000):\n",
    "\n",
    "    nio = ni+no\n",
    "    return [(ns//(alpha*(nio)))//numHiddenLayers]*numHiddenLayers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# I usually do whatever is found and then either do 2-3 of that size,\n",
    "# or Iâ€™ll half it each time (usually it) so layers=[NH, NH/2, NH/4] (sometimes the /4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "982"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "206"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784.08"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(982 + 206) * 0.66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, num_features, num_targets, hidden_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.batch_norm1 = nn.BatchNorm1d(num_features)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, num_features))\n",
    "        \n",
    "        self.batch_norm2 = nn.BatchNorm1d(num_features)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.dense2 = nn.utils.weight_norm(nn.Linear(num_features, 500))\n",
    "        \n",
    "        self.batch_norm3 = nn.BatchNorm1d(500)\n",
    "        self.dropout3 = nn.Dropout(0.5)\n",
    "        self.dense3 = nn.utils.weight_norm(nn.Linear(500, num_targets))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.dense1(x))\n",
    "        \n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = F.relu(self.dense2(x))\n",
    "        \n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.dense3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data):\n",
    "    \n",
    "    data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n",
    "#     data.loc[:, 'cp_time'] = data.loc[:, 'cp_time'].map({24: 0, 48: 1, 72: 2})\n",
    "#     data.loc[:, 'cp_dose'] = data.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})\n",
    "\n",
    "# --------------------- Normalize ---------------------\n",
    "#     for col in GENES:\n",
    "#         data[col] = (data[col]-np.mean(data[col])) / (np.std(data[col]))\n",
    "    \n",
    "#     for col in CELLS:\n",
    "#         data[col] = (data[col]-np.mean(data[col])) / (np.std(data[col]))\n",
    "    \n",
    "#--------------------- Removing Skewness ---------------------\n",
    "#     for col in GENES + CELLS:\n",
    "#         if(abs(data[col].skew()) > 0.75):\n",
    "            \n",
    "#             if(data[col].skew() < 0): # neg-skewness\n",
    "#                 data[col] = data[col].max() - data[col] + 1\n",
    "#                 data[col] = np.sqrt(data[col])\n",
    "            \n",
    "#             else:\n",
    "#                 data[col] = np.sqrt(data[col])\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "982"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_cols = [c for c in process_data(folds).columns if c not in target_cols]\n",
    "feature_cols = [c for c in feature_cols if c not in ['kfold','sig_id']]\n",
    "len(feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HyperParameters\n",
    "\n",
    "DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 1e-2\n",
    "WEIGHT_DECAY = 1e-5\n",
    "NFOLDS = 5\n",
    "EARLY_STOPPING_STEPS = 10\n",
    "EARLY_STOP = False\n",
    "\n",
    "num_features=len(feature_cols)\n",
    "num_targets=len(target_cols)\n",
    "hidden_size=1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09c032ce48674b06b1820d837cbddb30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping early, the loss has diverged\n",
      "Learning rate search finished. See the graph with {finder_name}.plot()\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "seed = 0\n",
    "fold = 0 \n",
    "\n",
    "seed_everything(seed)\n",
    "\n",
    "train = process_data(folds)\n",
    "test_ = process_data(test)\n",
    "\n",
    "trn_idx = train[train['kfold'] != fold].index\n",
    "val_idx = train[train['kfold'] == fold].index\n",
    "\n",
    "train_df = train[train['kfold'] != fold].reset_index(drop=True)\n",
    "valid_df = train[train['kfold'] == fold].reset_index(drop=True)\n",
    "\n",
    "x_train, y_train  = train_df[feature_cols].values, train_df[target_cols].values\n",
    "x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values\n",
    "\n",
    "class MoADataset_:\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.features.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dct = {\n",
    "            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n",
    "            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n",
    "        }\n",
    "        return (dct['x'], dct['y'])#dct\n",
    "\n",
    "train_dataset = MoADataset_(x_train, y_train)\n",
    "valid_dataset = MoADataset_(x_valid, y_valid)\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = Model(\n",
    "    num_features=num_features,\n",
    "    num_targets=num_targets,\n",
    "    hidden_size=hidden_size,\n",
    ")\n",
    "\n",
    "#model.to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-7, weight_decay=WEIGHT_DECAY)\n",
    "# scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n",
    "#                                           max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "from torch_lr_finder import LRFinder\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = optim.Adam(model.parameters(), lr=1e-7, weight_decay=1e-2)\n",
    "lr_finder = LRFinder(model, optimizer, criterion, device=\"cuda\")\n",
    "lr_finder.range_test(trainloader, end_lr=10, num_iter=100)\n",
    "lr_finder.plot() # to inspect the loss-learning rate graph\n",
    "lr_finder.reset() # to reset the model and optimizer to their initial state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(fold, seed):\n",
    "    \n",
    "    seed_everything(seed)\n",
    "    \n",
    "    train = process_data(folds)\n",
    "    test_ = process_data(test)\n",
    "    \n",
    "    trn_idx = train[train['kfold'] != fold].index\n",
    "    val_idx = train[train['kfold'] == fold].index\n",
    "    \n",
    "    train_df = train[train['kfold'] != fold].reset_index(drop=True)\n",
    "    valid_df = train[train['kfold'] == fold].reset_index(drop=True)\n",
    "    \n",
    "    x_train, y_train  = train_df[feature_cols].values, train_df[target_cols].values\n",
    "    x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values\n",
    "    \n",
    "    train_dataset = MoADataset(x_train, y_train)\n",
    "    valid_dataset = MoADataset(x_valid, y_valid)\n",
    "    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    model = Model(\n",
    "        num_features=num_features,\n",
    "        num_targets=num_targets,\n",
    "        hidden_size=hidden_size,\n",
    "    )\n",
    "    \n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n",
    "                                              max_lr=LEARNING_RATE, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n",
    "    \n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    early_stopping_steps = EARLY_STOPPING_STEPS\n",
    "    early_step = 0\n",
    "    \n",
    "    oof = np.zeros((len(train), target.iloc[:, 1:].shape[1]))\n",
    "    best_loss = np.inf\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        \n",
    "        train_loss = train_fn(model, optimizer,scheduler, loss_fn, trainloader, DEVICE)\n",
    "        print(f\"FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss}\")\n",
    "        valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n",
    "        print(f\"FOLD: {fold}, EPOCH: {epoch}, valid_loss: {valid_loss}\")\n",
    "        \n",
    "        if valid_loss < best_loss:\n",
    "            \n",
    "            best_loss = valid_loss\n",
    "            oof[val_idx] = valid_preds\n",
    "            torch.save(model.state_dict(), f\"FOLD{fold}_.pth\")\n",
    "        \n",
    "        elif(EARLY_STOP == True):\n",
    "            \n",
    "            early_step += 1\n",
    "            if (early_step >= early_stopping_steps):\n",
    "                break\n",
    "            \n",
    "    \n",
    "    #--------------------- PREDICTION---------------------\n",
    "    x_test = test_[feature_cols].values\n",
    "    testdataset = TestDataset(x_test)\n",
    "    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    model = Model(\n",
    "        num_features=num_features,\n",
    "        num_targets=num_targets,\n",
    "        hidden_size=hidden_size,\n",
    "    )\n",
    "    \n",
    "    model.load_state_dict(torch.load(f\"FOLD{fold}_.pth\"))\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    predictions = np.zeros((len(test_), target.iloc[:, 1:].shape[1]))\n",
    "    predictions = inference_fn(model, testloader, DEVICE)\n",
    "    \n",
    "    return oof, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_k_fold(NFOLDS, seed):\n",
    "    oof = np.zeros((len(train), len(target_cols)))\n",
    "    predictions = np.zeros((len(test), len(target_cols)))\n",
    "    \n",
    "    for fold in range(NFOLDS):\n",
    "        oof_, pred_ = run_training(fold, seed)\n",
    "        \n",
    "        predictions += pred_ / NFOLDS\n",
    "        oof += oof_\n",
    "        \n",
    "    return oof, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 0, EPOCH: 0, train_loss: 0.6361924636623134\n",
      "FOLD: 0, EPOCH: 0, valid_loss: 0.12679658085107803\n",
      "FOLD: 0, EPOCH: 1, train_loss: 0.03631133617212375\n",
      "FOLD: 0, EPOCH: 1, valid_loss: 0.02126673837857587\n",
      "FOLD: 0, EPOCH: 2, train_loss: 0.021404008597027565\n",
      "FOLD: 0, EPOCH: 2, valid_loss: 0.018931861647537778\n",
      "FOLD: 0, EPOCH: 3, train_loss: 0.019319192803316357\n",
      "FOLD: 0, EPOCH: 3, valid_loss: 0.017945447404469763\n",
      "FOLD: 0, EPOCH: 4, train_loss: 0.01848986051787717\n",
      "FOLD: 0, EPOCH: 4, valid_loss: 0.01765371627573456\n",
      "FOLD: 0, EPOCH: 5, train_loss: 0.01812400357743752\n",
      "FOLD: 0, EPOCH: 5, valid_loss: 0.01741944681853056\n",
      "FOLD: 0, EPOCH: 6, train_loss: 0.017864147288913744\n",
      "FOLD: 0, EPOCH: 6, valid_loss: 0.01736761767949377\n",
      "FOLD: 0, EPOCH: 7, train_loss: 0.017680573749585426\n",
      "FOLD: 0, EPOCH: 7, valid_loss: 0.01741527672857046\n",
      "FOLD: 0, EPOCH: 8, train_loss: 0.017691371314551518\n",
      "FOLD: 0, EPOCH: 8, valid_loss: 0.01727643563811268\n",
      "FOLD: 0, EPOCH: 9, train_loss: 0.017627246623885803\n",
      "FOLD: 0, EPOCH: 9, valid_loss: 0.017156232653984003\n",
      "FOLD: 0, EPOCH: 10, train_loss: 0.01766929969839428\n",
      "FOLD: 0, EPOCH: 10, valid_loss: 0.017153046333364078\n",
      "FOLD: 0, EPOCH: 11, train_loss: 0.01762889300648501\n",
      "FOLD: 0, EPOCH: 11, valid_loss: 0.017038470347012794\n",
      "FOLD: 0, EPOCH: 12, train_loss: 0.017663856563360794\n",
      "FOLD: 0, EPOCH: 12, valid_loss: 0.01717656095113073\n",
      "FOLD: 0, EPOCH: 13, train_loss: 0.0176291905168066\n",
      "FOLD: 0, EPOCH: 13, valid_loss: 0.017299038810389383\n",
      "FOLD: 0, EPOCH: 14, train_loss: 0.01760070642078484\n",
      "FOLD: 0, EPOCH: 14, valid_loss: 0.017149236984550953\n",
      "FOLD: 0, EPOCH: 15, train_loss: 0.017531866794856993\n",
      "FOLD: 0, EPOCH: 15, valid_loss: 0.017251968516835144\n",
      "FOLD: 0, EPOCH: 16, train_loss: 0.01741013858143402\n",
      "FOLD: 0, EPOCH: 16, valid_loss: 0.01707065427409751\n",
      "FOLD: 0, EPOCH: 17, train_loss: 0.01741013506535387\n",
      "FOLD: 0, EPOCH: 17, valid_loss: 0.016982630720095974\n",
      "FOLD: 0, EPOCH: 18, train_loss: 0.017320179488455902\n",
      "FOLD: 0, EPOCH: 18, valid_loss: 0.01672235232378755\n",
      "FOLD: 0, EPOCH: 19, train_loss: 0.01717086274014867\n",
      "FOLD: 0, EPOCH: 19, valid_loss: 0.016984335705637933\n",
      "FOLD: 0, EPOCH: 20, train_loss: 0.017073537047574486\n",
      "FOLD: 0, EPOCH: 20, valid_loss: 0.016657137817570142\n",
      "FOLD: 0, EPOCH: 21, train_loss: 0.016891845582943897\n",
      "FOLD: 0, EPOCH: 21, valid_loss: 0.016542738170496056\n",
      "FOLD: 0, EPOCH: 22, train_loss: 0.016718000504255728\n",
      "FOLD: 0, EPOCH: 22, valid_loss: 0.01643316407821008\n",
      "FOLD: 0, EPOCH: 23, train_loss: 0.01653911773979232\n",
      "FOLD: 0, EPOCH: 23, valid_loss: 0.016343217582574912\n",
      "FOLD: 0, EPOCH: 24, train_loss: 0.01632412805802364\n",
      "FOLD: 0, EPOCH: 24, valid_loss: 0.01629650928080082\n",
      "FOLD: 0, EPOCH: 25, train_loss: 0.016083651087314323\n",
      "FOLD: 0, EPOCH: 25, valid_loss: 0.016214651029024807\n",
      "FOLD: 0, EPOCH: 26, train_loss: 0.015941735006112984\n",
      "FOLD: 0, EPOCH: 26, valid_loss: 0.016129078636212008\n",
      "FOLD: 0, EPOCH: 27, train_loss: 0.01580160381137461\n",
      "FOLD: 0, EPOCH: 27, valid_loss: 0.016089783608913423\n",
      "FOLD: 0, EPOCH: 28, train_loss: 0.015643242226940565\n",
      "FOLD: 0, EPOCH: 28, valid_loss: 0.01607239432632923\n",
      "FOLD: 0, EPOCH: 29, train_loss: 0.015656921533408804\n",
      "FOLD: 0, EPOCH: 29, valid_loss: 0.01605361992759364\n",
      "FOLD: 1, EPOCH: 0, train_loss: 0.6341339749270591\n",
      "FOLD: 1, EPOCH: 0, valid_loss: 0.13234268980366842\n",
      "FOLD: 1, EPOCH: 1, train_loss: 0.03613166937577552\n",
      "FOLD: 1, EPOCH: 1, valid_loss: 0.0216677119157144\n",
      "FOLD: 1, EPOCH: 2, train_loss: 0.021150322617504044\n",
      "FOLD: 1, EPOCH: 2, valid_loss: 0.01926029787531921\n",
      "FOLD: 1, EPOCH: 3, train_loss: 0.01949002164537492\n",
      "FOLD: 1, EPOCH: 3, valid_loss: 0.01821984127163887\n",
      "FOLD: 1, EPOCH: 4, train_loss: 0.01852157608727398\n",
      "FOLD: 1, EPOCH: 4, valid_loss: 0.01773112543991634\n",
      "FOLD: 1, EPOCH: 5, train_loss: 0.018088251689745895\n",
      "FOLD: 1, EPOCH: 5, valid_loss: 0.017488450556993485\n",
      "FOLD: 1, EPOCH: 6, train_loss: 0.01792385688294535\n",
      "FOLD: 1, EPOCH: 6, valid_loss: 0.017365262338093348\n",
      "FOLD: 1, EPOCH: 7, train_loss: 0.017738574096744043\n",
      "FOLD: 1, EPOCH: 7, valid_loss: 0.017258589874420846\n",
      "FOLD: 1, EPOCH: 8, train_loss: 0.017773761528719595\n",
      "FOLD: 1, EPOCH: 8, valid_loss: 0.01734462095690625\n",
      "FOLD: 1, EPOCH: 9, train_loss: 0.017814732440139935\n",
      "FOLD: 1, EPOCH: 9, valid_loss: 0.017211734689772128\n",
      "FOLD: 1, EPOCH: 10, train_loss: 0.017687101760690195\n",
      "FOLD: 1, EPOCH: 10, valid_loss: 0.017239928245544434\n",
      "FOLD: 1, EPOCH: 11, train_loss: 0.01768287011435714\n",
      "FOLD: 1, EPOCH: 11, valid_loss: 0.01736637989857367\n",
      "FOLD: 1, EPOCH: 12, train_loss: 0.017618346486942493\n",
      "FOLD: 1, EPOCH: 12, valid_loss: 0.017188601036156926\n",
      "FOLD: 1, EPOCH: 13, train_loss: 0.017587574455293194\n",
      "FOLD: 1, EPOCH: 13, valid_loss: 0.017064978341971127\n",
      "FOLD: 1, EPOCH: 14, train_loss: 0.017555187280843224\n",
      "FOLD: 1, EPOCH: 14, valid_loss: 0.017079495798264233\n",
      "FOLD: 1, EPOCH: 15, train_loss: 0.017536598203730755\n",
      "FOLD: 1, EPOCH: 15, valid_loss: 0.017037932761013508\n",
      "FOLD: 1, EPOCH: 16, train_loss: 0.017450089748624876\n",
      "FOLD: 1, EPOCH: 16, valid_loss: 0.01714286357164383\n",
      "FOLD: 1, EPOCH: 17, train_loss: 0.01731261243854744\n",
      "FOLD: 1, EPOCH: 17, valid_loss: 0.01691665090620518\n",
      "FOLD: 1, EPOCH: 18, train_loss: 0.01726173355743505\n",
      "FOLD: 1, EPOCH: 18, valid_loss: 0.01687132902443409\n",
      "FOLD: 1, EPOCH: 19, train_loss: 0.017151017106421616\n",
      "FOLD: 1, EPOCH: 19, valid_loss: 0.01672270039894751\n",
      "FOLD: 1, EPOCH: 20, train_loss: 0.017000493465288393\n",
      "FOLD: 1, EPOCH: 20, valid_loss: 0.016655107055391583\n",
      "FOLD: 1, EPOCH: 21, train_loss: 0.016810493725959375\n",
      "FOLD: 1, EPOCH: 21, valid_loss: 0.016563603548066956\n",
      "FOLD: 1, EPOCH: 22, train_loss: 0.016651224317973938\n",
      "FOLD: 1, EPOCH: 22, valid_loss: 0.016498370042869023\n",
      "FOLD: 1, EPOCH: 23, train_loss: 0.01652002474967984\n",
      "FOLD: 1, EPOCH: 23, valid_loss: 0.016381595470011234\n",
      "FOLD: 1, EPOCH: 24, train_loss: 0.016339629044945257\n",
      "FOLD: 1, EPOCH: 24, valid_loss: 0.016294325994593756\n",
      "FOLD: 1, EPOCH: 25, train_loss: 0.01607350081178373\n",
      "FOLD: 1, EPOCH: 25, valid_loss: 0.01625254518751587\n",
      "FOLD: 1, EPOCH: 26, train_loss: 0.01585169347084087\n",
      "FOLD: 1, EPOCH: 26, valid_loss: 0.0161774999329022\n",
      "FOLD: 1, EPOCH: 27, train_loss: 0.015671078459886106\n",
      "FOLD: 1, EPOCH: 27, valid_loss: 0.0161708212058459\n",
      "FOLD: 1, EPOCH: 28, train_loss: 0.015527389498184557\n",
      "FOLD: 1, EPOCH: 28, valid_loss: 0.016138870136014054\n",
      "FOLD: 1, EPOCH: 29, train_loss: 0.015475284470164257\n",
      "FOLD: 1, EPOCH: 29, valid_loss: 0.016124559859079974\n",
      "FOLD: 2, EPOCH: 0, train_loss: 0.6381957631396211\n",
      "FOLD: 2, EPOCH: 0, valid_loss: 0.13103889397212437\n",
      "FOLD: 2, EPOCH: 1, train_loss: 0.0361471783666723\n",
      "FOLD: 2, EPOCH: 1, valid_loss: 0.021220480065260616\n",
      "FOLD: 2, EPOCH: 2, train_loss: 0.02104146774534298\n",
      "FOLD: 2, EPOCH: 2, valid_loss: 0.019088639744690487\n",
      "FOLD: 2, EPOCH: 3, train_loss: 0.019464570988455544\n",
      "FOLD: 2, EPOCH: 3, valid_loss: 0.018376252241432668\n",
      "FOLD: 2, EPOCH: 4, train_loss: 0.018630003030209438\n",
      "FOLD: 2, EPOCH: 4, valid_loss: 0.01761620994657278\n",
      "FOLD: 2, EPOCH: 5, train_loss: 0.01818940613715761\n",
      "FOLD: 2, EPOCH: 5, valid_loss: 0.01746335362217256\n",
      "FOLD: 2, EPOCH: 6, train_loss: 0.017864041253114523\n",
      "FOLD: 2, EPOCH: 6, valid_loss: 0.01732278982443469\n",
      "FOLD: 2, EPOCH: 7, train_loss: 0.017831912075263866\n",
      "FOLD: 2, EPOCH: 7, valid_loss: 0.01714731814073665\n",
      "FOLD: 2, EPOCH: 8, train_loss: 0.017669504461135122\n",
      "FOLD: 2, EPOCH: 8, valid_loss: 0.017309469037822316\n",
      "FOLD: 2, EPOCH: 9, train_loss: 0.0177273852750659\n",
      "FOLD: 2, EPOCH: 9, valid_loss: 0.01715848839708737\n",
      "FOLD: 2, EPOCH: 10, train_loss: 0.017801099463595427\n",
      "FOLD: 2, EPOCH: 10, valid_loss: 0.017140470737857478\n",
      "FOLD: 2, EPOCH: 11, train_loss: 0.017659482013002253\n",
      "FOLD: 2, EPOCH: 11, valid_loss: 0.01715794876217842\n",
      "FOLD: 2, EPOCH: 12, train_loss: 0.01766172681759665\n",
      "FOLD: 2, EPOCH: 12, valid_loss: 0.01707565917500428\n",
      "FOLD: 2, EPOCH: 13, train_loss: 0.017628246513397797\n",
      "FOLD: 2, EPOCH: 13, valid_loss: 0.01710936605398144\n",
      "FOLD: 2, EPOCH: 14, train_loss: 0.01758112715881156\n",
      "FOLD: 2, EPOCH: 14, valid_loss: 0.017094402919922556\n",
      "FOLD: 2, EPOCH: 15, train_loss: 0.017530655346648848\n",
      "FOLD: 2, EPOCH: 15, valid_loss: 0.01697829525385584\n",
      "FOLD: 2, EPOCH: 16, train_loss: 0.017550736470012995\n",
      "FOLD: 2, EPOCH: 16, valid_loss: 0.016914664581418038\n",
      "FOLD: 2, EPOCH: 17, train_loss: 0.017471184967544632\n",
      "FOLD: 2, EPOCH: 17, valid_loss: 0.016866870543786456\n",
      "FOLD: 2, EPOCH: 18, train_loss: 0.017323565338670775\n",
      "FOLD: 2, EPOCH: 18, valid_loss: 0.016889279681657043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 2, EPOCH: 19, train_loss: 0.01726602841222632\n",
      "FOLD: 2, EPOCH: 19, valid_loss: 0.016787780954369478\n",
      "FOLD: 2, EPOCH: 20, train_loss: 0.017112264544635578\n",
      "FOLD: 2, EPOCH: 20, valid_loss: 0.01673452133046729\n",
      "FOLD: 2, EPOCH: 21, train_loss: 0.016931434465653223\n",
      "FOLD: 2, EPOCH: 21, valid_loss: 0.016595536789723805\n",
      "FOLD: 2, EPOCH: 22, train_loss: 0.016736421018730904\n",
      "FOLD: 2, EPOCH: 22, valid_loss: 0.016465044527181557\n",
      "FOLD: 2, EPOCH: 23, train_loss: 0.01656306407454869\n",
      "FOLD: 2, EPOCH: 23, valid_loss: 0.016361435316503047\n",
      "FOLD: 2, EPOCH: 24, train_loss: 0.016384119086939354\n",
      "FOLD: 2, EPOCH: 24, valid_loss: 0.01628030264484031\n",
      "FOLD: 2, EPOCH: 25, train_loss: 0.01617307774722576\n",
      "FOLD: 2, EPOCH: 25, valid_loss: 0.016150407626160553\n",
      "FOLD: 2, EPOCH: 26, train_loss: 0.016034217801053022\n",
      "FOLD: 2, EPOCH: 26, valid_loss: 0.01608435921370983\n",
      "FOLD: 2, EPOCH: 27, train_loss: 0.015805273067098165\n",
      "FOLD: 2, EPOCH: 27, valid_loss: 0.016008143659148898\n",
      "FOLD: 2, EPOCH: 28, train_loss: 0.015701762751500675\n",
      "FOLD: 2, EPOCH: 28, valid_loss: 0.016013727762869426\n",
      "FOLD: 2, EPOCH: 29, train_loss: 0.015570282969839764\n",
      "FOLD: 2, EPOCH: 29, valid_loss: 0.01600999462285212\n",
      "FOLD: 3, EPOCH: 0, train_loss: 0.6373432046477345\n",
      "FOLD: 3, EPOCH: 0, valid_loss: 0.1462160302059991\n",
      "FOLD: 3, EPOCH: 1, train_loss: 0.037128778026047825\n",
      "FOLD: 3, EPOCH: 1, valid_loss: 0.02112859593970435\n",
      "FOLD: 3, EPOCH: 2, train_loss: 0.020854640711584816\n",
      "FOLD: 3, EPOCH: 2, valid_loss: 0.0191420671663114\n",
      "FOLD: 3, EPOCH: 3, train_loss: 0.01925516791263784\n",
      "FOLD: 3, EPOCH: 3, valid_loss: 0.018269906352673258\n",
      "FOLD: 3, EPOCH: 4, train_loss: 0.018449258018770943\n",
      "FOLD: 3, EPOCH: 4, valid_loss: 0.017718485769416606\n",
      "FOLD: 3, EPOCH: 5, train_loss: 0.01804352798026757\n",
      "FOLD: 3, EPOCH: 5, valid_loss: 0.017426613505397523\n",
      "FOLD: 3, EPOCH: 6, train_loss: 0.017950646538773308\n",
      "FOLD: 3, EPOCH: 6, valid_loss: 0.017441294555153166\n",
      "FOLD: 3, EPOCH: 7, train_loss: 0.017693159879063784\n",
      "FOLD: 3, EPOCH: 7, valid_loss: 0.017447381972202233\n",
      "FOLD: 3, EPOCH: 8, train_loss: 0.017760203341427055\n",
      "FOLD: 3, EPOCH: 8, valid_loss: 0.017417346898998532\n",
      "FOLD: 3, EPOCH: 9, train_loss: 0.01765776142154051\n",
      "FOLD: 3, EPOCH: 9, valid_loss: 0.017382968456617423\n",
      "FOLD: 3, EPOCH: 10, train_loss: 0.017605765236784584\n",
      "FOLD: 3, EPOCH: 10, valid_loss: 0.017336417548358442\n",
      "FOLD: 3, EPOCH: 11, train_loss: 0.017601812622793343\n",
      "FOLD: 3, EPOCH: 11, valid_loss: 0.017280549263315542\n",
      "FOLD: 3, EPOCH: 12, train_loss: 0.01752524993692835\n",
      "FOLD: 3, EPOCH: 12, valid_loss: 0.01739501306521041\n",
      "FOLD: 3, EPOCH: 13, train_loss: 0.017493600882859766\n",
      "FOLD: 3, EPOCH: 13, valid_loss: 0.01721264493784734\n",
      "FOLD: 3, EPOCH: 14, train_loss: 0.01751230112479433\n",
      "FOLD: 3, EPOCH: 14, valid_loss: 0.01721145385610206\n",
      "FOLD: 3, EPOCH: 15, train_loss: 0.017468179663832205\n",
      "FOLD: 3, EPOCH: 15, valid_loss: 0.017155981409762586\n",
      "FOLD: 3, EPOCH: 16, train_loss: 0.01745400295012455\n",
      "FOLD: 3, EPOCH: 16, valid_loss: 0.017152059530573232\n",
      "FOLD: 3, EPOCH: 17, train_loss: 0.017350740244855053\n",
      "FOLD: 3, EPOCH: 17, valid_loss: 0.01705169738935573\n",
      "FOLD: 3, EPOCH: 18, train_loss: 0.017278913590733126\n",
      "FOLD: 3, EPOCH: 18, valid_loss: 0.017097458083714757\n",
      "FOLD: 3, EPOCH: 19, train_loss: 0.017165427366136642\n",
      "FOLD: 3, EPOCH: 19, valid_loss: 0.016902475378343038\n",
      "FOLD: 3, EPOCH: 20, train_loss: 0.017064829701152834\n",
      "FOLD: 3, EPOCH: 20, valid_loss: 0.016893814238054413\n",
      "FOLD: 3, EPOCH: 21, train_loss: 0.016951929480917213\n",
      "FOLD: 3, EPOCH: 21, valid_loss: 0.016875087256942478\n",
      "FOLD: 3, EPOCH: 22, train_loss: 0.01673791110785543\n",
      "FOLD: 3, EPOCH: 22, valid_loss: 0.01666597191776548\n",
      "FOLD: 3, EPOCH: 23, train_loss: 0.016529388165614313\n",
      "FOLD: 3, EPOCH: 23, valid_loss: 0.016576884128153325\n",
      "FOLD: 3, EPOCH: 24, train_loss: 0.016310013628200344\n",
      "FOLD: 3, EPOCH: 24, valid_loss: 0.016450305575770992\n",
      "FOLD: 3, EPOCH: 25, train_loss: 0.01613751267069492\n",
      "FOLD: 3, EPOCH: 25, valid_loss: 0.016473086417785714\n",
      "FOLD: 3, EPOCH: 26, train_loss: 0.015952253997649834\n",
      "FOLD: 3, EPOCH: 26, valid_loss: 0.016344781246568477\n",
      "FOLD: 3, EPOCH: 27, train_loss: 0.015811912141794313\n",
      "FOLD: 3, EPOCH: 27, valid_loss: 0.01628329109932695\n",
      "FOLD: 3, EPOCH: 28, train_loss: 0.015648981016399204\n",
      "FOLD: 3, EPOCH: 28, valid_loss: 0.016294730428074087\n",
      "FOLD: 3, EPOCH: 29, train_loss: 0.01565149838131839\n",
      "FOLD: 3, EPOCH: 29, valid_loss: 0.016289516165852547\n",
      "FOLD: 4, EPOCH: 0, train_loss: 0.6361095585684845\n",
      "FOLD: 4, EPOCH: 0, valid_loss: 0.1347675438438143\n",
      "FOLD: 4, EPOCH: 1, train_loss: 0.03653831090238215\n",
      "FOLD: 4, EPOCH: 1, valid_loss: 0.023259184349860462\n",
      "FOLD: 4, EPOCH: 2, train_loss: 0.020911387613286144\n",
      "FOLD: 4, EPOCH: 2, valid_loss: 0.019593145538653645\n",
      "FOLD: 4, EPOCH: 3, train_loss: 0.01947293915124475\n",
      "FOLD: 4, EPOCH: 3, valid_loss: 0.018081814449812685\n",
      "FOLD: 4, EPOCH: 4, train_loss: 0.01857557533767776\n",
      "FOLD: 4, EPOCH: 4, valid_loss: 0.017634000948497226\n",
      "FOLD: 4, EPOCH: 5, train_loss: 0.018141360912957916\n",
      "FOLD: 4, EPOCH: 5, valid_loss: 0.017297332813697202\n",
      "FOLD: 4, EPOCH: 6, train_loss: 0.017886583153428375\n",
      "FOLD: 4, EPOCH: 6, valid_loss: 0.017237426260752337\n",
      "FOLD: 4, EPOCH: 7, train_loss: 0.017805172785090796\n",
      "FOLD: 4, EPOCH: 7, valid_loss: 0.017345911769994667\n",
      "FOLD: 4, EPOCH: 8, train_loss: 0.017734364208265924\n",
      "FOLD: 4, EPOCH: 8, valid_loss: 0.017176840720432147\n",
      "FOLD: 4, EPOCH: 9, train_loss: 0.01779989761205903\n",
      "FOLD: 4, EPOCH: 9, valid_loss: 0.017459455797714848\n",
      "FOLD: 4, EPOCH: 10, train_loss: 0.017699524193354275\n",
      "FOLD: 4, EPOCH: 10, valid_loss: 0.01725970110190766\n",
      "FOLD: 4, EPOCH: 11, train_loss: 0.017623226941171764\n",
      "FOLD: 4, EPOCH: 11, valid_loss: 0.01715096187378679\n",
      "FOLD: 4, EPOCH: 12, train_loss: 0.017665626514918993\n",
      "FOLD: 4, EPOCH: 12, valid_loss: 0.017055562111948218\n",
      "FOLD: 4, EPOCH: 13, train_loss: 0.01755214669485239\n",
      "FOLD: 4, EPOCH: 13, valid_loss: 0.017099953575858046\n",
      "FOLD: 4, EPOCH: 14, train_loss: 0.01758486292311463\n",
      "FOLD: 4, EPOCH: 14, valid_loss: 0.017204090979482445\n",
      "FOLD: 4, EPOCH: 15, train_loss: 0.017528414679016325\n",
      "FOLD: 4, EPOCH: 15, valid_loss: 0.01700616649218968\n",
      "FOLD: 4, EPOCH: 16, train_loss: 0.017491228777267363\n",
      "FOLD: 4, EPOCH: 16, valid_loss: 0.016878304231379714\n",
      "FOLD: 4, EPOCH: 17, train_loss: 0.0174690964024352\n",
      "FOLD: 4, EPOCH: 17, valid_loss: 0.016880373204393045\n",
      "FOLD: 4, EPOCH: 18, train_loss: 0.017252658122637564\n",
      "FOLD: 4, EPOCH: 18, valid_loss: 0.0168179702013731\n",
      "FOLD: 4, EPOCH: 19, train_loss: 0.017182263343230537\n",
      "FOLD: 4, EPOCH: 19, valid_loss: 0.016699461532490596\n",
      "FOLD: 4, EPOCH: 20, train_loss: 0.017031836478660505\n",
      "FOLD: 4, EPOCH: 20, valid_loss: 0.01653540858200618\n",
      "FOLD: 4, EPOCH: 21, train_loss: 0.016924934562943552\n",
      "FOLD: 4, EPOCH: 21, valid_loss: 0.01649913119950465\n",
      "FOLD: 4, EPOCH: 22, train_loss: 0.01677075635128911\n",
      "FOLD: 4, EPOCH: 22, valid_loss: 0.01641809887119702\n",
      "FOLD: 4, EPOCH: 23, train_loss: 0.016626961967921343\n",
      "FOLD: 4, EPOCH: 23, valid_loss: 0.016327350293951375\n",
      "FOLD: 4, EPOCH: 24, train_loss: 0.01632311961789062\n",
      "FOLD: 4, EPOCH: 24, valid_loss: 0.016211489854114396\n",
      "FOLD: 4, EPOCH: 25, train_loss: 0.01615969782722169\n",
      "FOLD: 4, EPOCH: 25, valid_loss: 0.01615917475095817\n",
      "FOLD: 4, EPOCH: 26, train_loss: 0.015981229683519272\n",
      "FOLD: 4, EPOCH: 26, valid_loss: 0.01604965821440731\n",
      "FOLD: 4, EPOCH: 27, train_loss: 0.01575880447078658\n",
      "FOLD: 4, EPOCH: 27, valid_loss: 0.016042908201260225\n",
      "FOLD: 4, EPOCH: 28, train_loss: 0.015710327639311985\n",
      "FOLD: 4, EPOCH: 28, valid_loss: 0.015991313861949102\n",
      "FOLD: 4, EPOCH: 29, train_loss: 0.015593550822603098\n",
      "FOLD: 4, EPOCH: 29, valid_loss: 0.01599767341145447\n",
      "FOLD: 0, EPOCH: 0, train_loss: 0.6369705785443818\n",
      "FOLD: 0, EPOCH: 0, valid_loss: 0.137152960896492\n",
      "FOLD: 0, EPOCH: 1, train_loss: 0.03584350233870572\n",
      "FOLD: 0, EPOCH: 1, valid_loss: 0.021215983320559774\n",
      "FOLD: 0, EPOCH: 2, train_loss: 0.020725032328155594\n",
      "FOLD: 0, EPOCH: 2, valid_loss: 0.018867234513163567\n",
      "FOLD: 0, EPOCH: 3, train_loss: 0.019415340926228226\n",
      "FOLD: 0, EPOCH: 3, valid_loss: 0.01808749581021922\n",
      "FOLD: 0, EPOCH: 4, train_loss: 0.01851878363126214\n",
      "FOLD: 0, EPOCH: 4, valid_loss: 0.017455603581454072\n",
      "FOLD: 0, EPOCH: 5, train_loss: 0.018118752612043983\n",
      "FOLD: 0, EPOCH: 5, valid_loss: 0.017273655108043125\n",
      "FOLD: 0, EPOCH: 6, train_loss: 0.017835294692844585\n",
      "FOLD: 0, EPOCH: 6, valid_loss: 0.01740628986486367\n",
      "FOLD: 0, EPOCH: 7, train_loss: 0.017784805921162817\n",
      "FOLD: 0, EPOCH: 7, valid_loss: 0.017391175989593777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 0, EPOCH: 8, train_loss: 0.017797908091080793\n",
      "FOLD: 0, EPOCH: 8, valid_loss: 0.01748248740498509\n",
      "FOLD: 0, EPOCH: 9, train_loss: 0.01776954544492174\n",
      "FOLD: 0, EPOCH: 9, valid_loss: 0.017231564782559872\n",
      "FOLD: 0, EPOCH: 10, train_loss: 0.01775025454201344\n",
      "FOLD: 0, EPOCH: 10, valid_loss: 0.01720104943960905\n",
      "FOLD: 0, EPOCH: 11, train_loss: 0.01772005899903783\n",
      "FOLD: 0, EPOCH: 11, valid_loss: 0.017221748616014207\n",
      "FOLD: 0, EPOCH: 12, train_loss: 0.017665545375126858\n",
      "FOLD: 0, EPOCH: 12, valid_loss: 0.01729200003402574\n",
      "FOLD: 0, EPOCH: 13, train_loss: 0.017648558598011732\n",
      "FOLD: 0, EPOCH: 13, valid_loss: 0.01714056432247162\n",
      "FOLD: 0, EPOCH: 14, train_loss: 0.0175016467810433\n",
      "FOLD: 0, EPOCH: 14, valid_loss: 0.017387959440904006\n",
      "FOLD: 0, EPOCH: 15, train_loss: 0.01746188753378996\n",
      "FOLD: 0, EPOCH: 15, valid_loss: 0.01701790719692196\n",
      "FOLD: 0, EPOCH: 16, train_loss: 0.017514628931826006\n",
      "FOLD: 0, EPOCH: 16, valid_loss: 0.01697228726531778\n",
      "FOLD: 0, EPOCH: 17, train_loss: 0.017399485142010708\n",
      "FOLD: 0, EPOCH: 17, valid_loss: 0.016981522898588862\n",
      "FOLD: 0, EPOCH: 18, train_loss: 0.0173031657434784\n",
      "FOLD: 0, EPOCH: 18, valid_loss: 0.01685093218194587\n",
      "FOLD: 0, EPOCH: 19, train_loss: 0.017134643753693588\n",
      "FOLD: 0, EPOCH: 19, valid_loss: 0.016751117605183805\n",
      "FOLD: 0, EPOCH: 20, train_loss: 0.01711147468186159\n",
      "FOLD: 0, EPOCH: 20, valid_loss: 0.01662369942558663\n",
      "FOLD: 0, EPOCH: 21, train_loss: 0.01687482096578764\n",
      "FOLD: 0, EPOCH: 21, valid_loss: 0.016527661521519934\n",
      "FOLD: 0, EPOCH: 22, train_loss: 0.016702864838737078\n",
      "FOLD: 0, EPOCH: 22, valid_loss: 0.01644621342420578\n",
      "FOLD: 0, EPOCH: 23, train_loss: 0.01655857108425403\n",
      "FOLD: 0, EPOCH: 23, valid_loss: 0.01644349933734962\n",
      "FOLD: 0, EPOCH: 24, train_loss: 0.016359719096858433\n",
      "FOLD: 0, EPOCH: 24, valid_loss: 0.01623917639787708\n",
      "FOLD: 0, EPOCH: 25, train_loss: 0.01616748809085592\n",
      "FOLD: 0, EPOCH: 25, valid_loss: 0.016194932482072284\n",
      "FOLD: 0, EPOCH: 26, train_loss: 0.015969101845732202\n",
      "FOLD: 0, EPOCH: 26, valid_loss: 0.01611969329948936\n",
      "FOLD: 0, EPOCH: 27, train_loss: 0.015814728548993236\n",
      "FOLD: 0, EPOCH: 27, valid_loss: 0.016091010426836354\n",
      "FOLD: 0, EPOCH: 28, train_loss: 0.015676209345405947\n",
      "FOLD: 0, EPOCH: 28, valid_loss: 0.01605455364499773\n",
      "FOLD: 0, EPOCH: 29, train_loss: 0.015629837606642126\n",
      "FOLD: 0, EPOCH: 29, valid_loss: 0.01609521774309022\n",
      "FOLD: 1, EPOCH: 0, train_loss: 0.6370419596416362\n",
      "FOLD: 1, EPOCH: 0, valid_loss: 0.1348488794905799\n",
      "FOLD: 1, EPOCH: 1, train_loss: 0.0370338371536438\n",
      "FOLD: 1, EPOCH: 1, valid_loss: 0.02100600696035794\n",
      "FOLD: 1, EPOCH: 2, train_loss: 0.0210651812022147\n",
      "FOLD: 1, EPOCH: 2, valid_loss: 0.019368885244641986\n",
      "FOLD: 1, EPOCH: 3, train_loss: 0.019503410001271877\n",
      "FOLD: 1, EPOCH: 3, valid_loss: 0.01824364135307925\n",
      "FOLD: 1, EPOCH: 4, train_loss: 0.018562119616114574\n",
      "FOLD: 1, EPOCH: 4, valid_loss: 0.017786472610064916\n",
      "FOLD: 1, EPOCH: 5, train_loss: 0.01806607812076159\n",
      "FOLD: 1, EPOCH: 5, valid_loss: 0.017533639445900917\n",
      "FOLD: 1, EPOCH: 6, train_loss: 0.017862611713454775\n",
      "FOLD: 1, EPOCH: 6, valid_loss: 0.01722834368369409\n",
      "FOLD: 1, EPOCH: 7, train_loss: 0.017762902030802292\n",
      "FOLD: 1, EPOCH: 7, valid_loss: 0.01712792624852487\n",
      "FOLD: 1, EPOCH: 8, train_loss: 0.01760173939249438\n",
      "FOLD: 1, EPOCH: 8, valid_loss: 0.01716469465089696\n",
      "FOLD: 1, EPOCH: 9, train_loss: 0.01761656840318355\n",
      "FOLD: 1, EPOCH: 9, valid_loss: 0.017162788632724968\n",
      "FOLD: 1, EPOCH: 10, train_loss: 0.017632883143327806\n",
      "FOLD: 1, EPOCH: 10, valid_loss: 0.017197933500366552\n",
      "FOLD: 1, EPOCH: 11, train_loss: 0.01762642342246313\n",
      "FOLD: 1, EPOCH: 11, valid_loss: 0.017141066757696017\n",
      "FOLD: 1, EPOCH: 12, train_loss: 0.017637008699871923\n",
      "FOLD: 1, EPOCH: 12, valid_loss: 0.017020355377878462\n",
      "FOLD: 1, EPOCH: 13, train_loss: 0.017601284421170534\n",
      "FOLD: 1, EPOCH: 13, valid_loss: 0.017182787880301476\n",
      "FOLD: 1, EPOCH: 14, train_loss: 0.017498832034028095\n",
      "FOLD: 1, EPOCH: 14, valid_loss: 0.017101213868175234\n",
      "FOLD: 1, EPOCH: 15, train_loss: 0.01746481703594327\n",
      "FOLD: 1, EPOCH: 15, valid_loss: 0.017093050879027162\n",
      "FOLD: 1, EPOCH: 16, train_loss: 0.01741839073382426\n",
      "FOLD: 1, EPOCH: 16, valid_loss: 0.017006483221692698\n",
      "FOLD: 1, EPOCH: 17, train_loss: 0.017322197650977665\n",
      "FOLD: 1, EPOCH: 17, valid_loss: 0.016932143962809017\n",
      "FOLD: 1, EPOCH: 18, train_loss: 0.01728387712838425\n",
      "FOLD: 1, EPOCH: 18, valid_loss: 0.01678451688161918\n",
      "FOLD: 1, EPOCH: 19, train_loss: 0.01713506072975587\n",
      "FOLD: 1, EPOCH: 19, valid_loss: 0.016733691575271742\n",
      "FOLD: 1, EPOCH: 20, train_loss: 0.016997977423117212\n",
      "FOLD: 1, EPOCH: 20, valid_loss: 0.016670955903828145\n",
      "FOLD: 1, EPOCH: 21, train_loss: 0.01688627213023711\n",
      "FOLD: 1, EPOCH: 21, valid_loss: 0.016597059981099196\n",
      "FOLD: 1, EPOCH: 22, train_loss: 0.01669887637557543\n",
      "FOLD: 1, EPOCH: 22, valid_loss: 0.016497715855283396\n",
      "FOLD: 1, EPOCH: 23, train_loss: 0.01648705650417917\n",
      "FOLD: 1, EPOCH: 23, valid_loss: 0.016362672565238816\n",
      "FOLD: 1, EPOCH: 24, train_loss: 0.01625546680736369\n",
      "FOLD: 1, EPOCH: 24, valid_loss: 0.016343185172549315\n",
      "FOLD: 1, EPOCH: 25, train_loss: 0.016105569730364325\n",
      "FOLD: 1, EPOCH: 25, valid_loss: 0.016302953181522233\n",
      "FOLD: 1, EPOCH: 26, train_loss: 0.015863104905609205\n",
      "FOLD: 1, EPOCH: 26, valid_loss: 0.016206152683922222\n",
      "FOLD: 1, EPOCH: 27, train_loss: 0.01569925010393279\n",
      "FOLD: 1, EPOCH: 27, valid_loss: 0.01619507617184094\n",
      "FOLD: 1, EPOCH: 28, train_loss: 0.015568624446303516\n",
      "FOLD: 1, EPOCH: 28, valid_loss: 0.016129456806395735\n",
      "FOLD: 1, EPOCH: 29, train_loss: 0.015540200015664965\n",
      "FOLD: 1, EPOCH: 29, valid_loss: 0.016155191590743405\n",
      "FOLD: 2, EPOCH: 0, train_loss: 0.6380353655288185\n",
      "FOLD: 2, EPOCH: 0, valid_loss: 0.1326876631804875\n",
      "FOLD: 2, EPOCH: 1, train_loss: 0.03598555516235638\n",
      "FOLD: 2, EPOCH: 1, valid_loss: 0.02076687552034855\n",
      "FOLD: 2, EPOCH: 2, train_loss: 0.020735024380079216\n",
      "FOLD: 2, EPOCH: 2, valid_loss: 0.019120400079659053\n",
      "FOLD: 2, EPOCH: 3, train_loss: 0.019698041828646175\n",
      "FOLD: 2, EPOCH: 3, valid_loss: 0.01811225496764694\n",
      "FOLD: 2, EPOCH: 4, train_loss: 0.018541019895802372\n",
      "FOLD: 2, EPOCH: 4, valid_loss: 0.01789629254490137\n",
      "FOLD: 2, EPOCH: 5, train_loss: 0.01809484718799375\n",
      "FOLD: 2, EPOCH: 5, valid_loss: 0.017395935979272636\n",
      "FOLD: 2, EPOCH: 6, train_loss: 0.01783947945585933\n",
      "FOLD: 2, EPOCH: 6, valid_loss: 0.017230776457914283\n",
      "FOLD: 2, EPOCH: 7, train_loss: 0.017730076561101538\n",
      "FOLD: 2, EPOCH: 7, valid_loss: 0.017229420505464076\n",
      "FOLD: 2, EPOCH: 8, train_loss: 0.017648343104815136\n",
      "FOLD: 2, EPOCH: 8, valid_loss: 0.017508239671587943\n",
      "FOLD: 2, EPOCH: 9, train_loss: 0.01759600043431788\n",
      "FOLD: 2, EPOCH: 9, valid_loss: 0.017214197346142362\n",
      "FOLD: 2, EPOCH: 10, train_loss: 0.01763675553535206\n",
      "FOLD: 2, EPOCH: 10, valid_loss: 0.017272517191512245\n",
      "FOLD: 2, EPOCH: 11, train_loss: 0.0176371807448458\n",
      "FOLD: 2, EPOCH: 11, valid_loss: 0.01716599427163601\n",
      "FOLD: 2, EPOCH: 12, train_loss: 0.017586241273776344\n",
      "FOLD: 2, EPOCH: 12, valid_loss: 0.01724479536392859\n",
      "FOLD: 2, EPOCH: 13, train_loss: 0.01757078722416275\n",
      "FOLD: 2, EPOCH: 13, valid_loss: 0.01713651269674301\n",
      "FOLD: 2, EPOCH: 14, train_loss: 0.017582688716820616\n",
      "FOLD: 2, EPOCH: 14, valid_loss: 0.017143904976546765\n",
      "FOLD: 2, EPOCH: 15, train_loss: 0.017593692751952272\n",
      "FOLD: 2, EPOCH: 15, valid_loss: 0.01729025047804628\n",
      "FOLD: 2, EPOCH: 16, train_loss: 0.017465233694815983\n",
      "FOLD: 2, EPOCH: 16, valid_loss: 0.016910153866878577\n",
      "FOLD: 2, EPOCH: 17, train_loss: 0.01736961123601034\n",
      "FOLD: 2, EPOCH: 17, valid_loss: 0.016847025070871626\n",
      "FOLD: 2, EPOCH: 18, train_loss: 0.017290334662665493\n",
      "FOLD: 2, EPOCH: 18, valid_loss: 0.01685713276799236\n",
      "FOLD: 2, EPOCH: 19, train_loss: 0.017171922436766865\n",
      "FOLD: 2, EPOCH: 19, valid_loss: 0.016735763129379067\n",
      "FOLD: 2, EPOCH: 20, train_loss: 0.01705448313926657\n",
      "FOLD: 2, EPOCH: 20, valid_loss: 0.016575235607368604\n",
      "FOLD: 2, EPOCH: 21, train_loss: 0.016875186118472746\n",
      "FOLD: 2, EPOCH: 21, valid_loss: 0.016534942388534545\n",
      "FOLD: 2, EPOCH: 22, train_loss: 0.016742664220594408\n",
      "FOLD: 2, EPOCH: 22, valid_loss: 0.01643992734274694\n",
      "FOLD: 2, EPOCH: 23, train_loss: 0.01654762129528799\n",
      "FOLD: 2, EPOCH: 23, valid_loss: 0.01629443038254976\n",
      "FOLD: 2, EPOCH: 24, train_loss: 0.01634202024265044\n",
      "FOLD: 2, EPOCH: 24, valid_loss: 0.016271355774785792\n",
      "FOLD: 2, EPOCH: 25, train_loss: 0.01608307964667894\n",
      "FOLD: 2, EPOCH: 25, valid_loss: 0.016223168772246156\n",
      "FOLD: 2, EPOCH: 26, train_loss: 0.015905223530379757\n",
      "FOLD: 2, EPOCH: 26, valid_loss: 0.01614051897610937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 2, EPOCH: 27, train_loss: 0.01575222390744349\n",
      "FOLD: 2, EPOCH: 27, valid_loss: 0.01601972622530801\n",
      "FOLD: 2, EPOCH: 28, train_loss: 0.015648807540697897\n",
      "FOLD: 2, EPOCH: 28, valid_loss: 0.016045930316405638\n",
      "FOLD: 2, EPOCH: 29, train_loss: 0.015577029308601133\n",
      "FOLD: 2, EPOCH: 29, valid_loss: 0.016040150954255036\n",
      "FOLD: 3, EPOCH: 0, train_loss: 0.6357311725184538\n",
      "FOLD: 3, EPOCH: 0, valid_loss: 0.1334306320973805\n",
      "FOLD: 3, EPOCH: 1, train_loss: 0.0364256819602156\n",
      "FOLD: 3, EPOCH: 1, valid_loss: 0.021363991977913038\n",
      "FOLD: 3, EPOCH: 2, train_loss: 0.02091571476742409\n",
      "FOLD: 3, EPOCH: 2, valid_loss: 0.019744733135615075\n",
      "FOLD: 3, EPOCH: 3, train_loss: 0.019254236090658367\n",
      "FOLD: 3, EPOCH: 3, valid_loss: 0.018186297640204428\n",
      "FOLD: 3, EPOCH: 4, train_loss: 0.01850948187833031\n",
      "FOLD: 3, EPOCH: 4, valid_loss: 0.017856461767639432\n",
      "FOLD: 3, EPOCH: 5, train_loss: 0.01808792163712391\n",
      "FOLD: 3, EPOCH: 5, valid_loss: 0.01754667687096766\n",
      "FOLD: 3, EPOCH: 6, train_loss: 0.01781815515183236\n",
      "FOLD: 3, EPOCH: 6, valid_loss: 0.017447791807353497\n",
      "FOLD: 3, EPOCH: 7, train_loss: 0.0176560599491864\n",
      "FOLD: 3, EPOCH: 7, valid_loss: 0.017459063471427987\n",
      "FOLD: 3, EPOCH: 8, train_loss: 0.017698270545435556\n",
      "FOLD: 3, EPOCH: 8, valid_loss: 0.0172612157783338\n",
      "FOLD: 3, EPOCH: 9, train_loss: 0.017661781178490408\n",
      "FOLD: 3, EPOCH: 9, valid_loss: 0.017262540172253336\n",
      "FOLD: 3, EPOCH: 10, train_loss: 0.017575514583807926\n",
      "FOLD: 3, EPOCH: 10, valid_loss: 0.017233032227626868\n",
      "FOLD: 3, EPOCH: 11, train_loss: 0.017622017618808626\n",
      "FOLD: 3, EPOCH: 11, valid_loss: 0.017263307342571872\n",
      "FOLD: 3, EPOCH: 12, train_loss: 0.01758524853790152\n",
      "FOLD: 3, EPOCH: 12, valid_loss: 0.017404401488602162\n",
      "FOLD: 3, EPOCH: 13, train_loss: 0.01757312227256488\n",
      "FOLD: 3, EPOCH: 13, valid_loss: 0.01735114747924464\n",
      "FOLD: 3, EPOCH: 14, train_loss: 0.017623787631105253\n",
      "FOLD: 3, EPOCH: 14, valid_loss: 0.017153908099446977\n",
      "FOLD: 3, EPOCH: 15, train_loss: 0.01747676483153001\n",
      "FOLD: 3, EPOCH: 15, valid_loss: 0.01718045949403729\n",
      "FOLD: 3, EPOCH: 16, train_loss: 0.017319356894417517\n",
      "FOLD: 3, EPOCH: 16, valid_loss: 0.01711282708815166\n",
      "FOLD: 3, EPOCH: 17, train_loss: 0.01731839381914208\n",
      "FOLD: 3, EPOCH: 17, valid_loss: 0.017041290551424025\n",
      "FOLD: 3, EPOCH: 18, train_loss: 0.0172244176754485\n",
      "FOLD: 3, EPOCH: 18, valid_loss: 0.01704990275736366\n",
      "FOLD: 3, EPOCH: 19, train_loss: 0.01712553792943557\n",
      "FOLD: 3, EPOCH: 19, valid_loss: 0.016845519867326533\n",
      "FOLD: 3, EPOCH: 20, train_loss: 0.01699068751591055\n",
      "FOLD: 3, EPOCH: 20, valid_loss: 0.01679137617881809\n",
      "FOLD: 3, EPOCH: 21, train_loss: 0.01684680303045805\n",
      "FOLD: 3, EPOCH: 21, valid_loss: 0.016744230501353742\n",
      "FOLD: 3, EPOCH: 22, train_loss: 0.01667213954868308\n",
      "FOLD: 3, EPOCH: 22, valid_loss: 0.016617713762181147\n",
      "FOLD: 3, EPOCH: 23, train_loss: 0.01647035598727888\n",
      "FOLD: 3, EPOCH: 23, valid_loss: 0.016495073294000965\n",
      "FOLD: 3, EPOCH: 24, train_loss: 0.01625603544053392\n",
      "FOLD: 3, EPOCH: 24, valid_loss: 0.01642865589154618\n",
      "FOLD: 3, EPOCH: 25, train_loss: 0.016080041928891686\n",
      "FOLD: 3, EPOCH: 25, valid_loss: 0.01632248011550733\n",
      "FOLD: 3, EPOCH: 26, train_loss: 0.01587676408741137\n",
      "FOLD: 3, EPOCH: 26, valid_loss: 0.01626795862934419\n",
      "FOLD: 3, EPOCH: 27, train_loss: 0.015683852905488533\n",
      "FOLD: 3, EPOCH: 27, valid_loss: 0.016237723774143627\n",
      "FOLD: 3, EPOCH: 28, train_loss: 0.0155804927420357\n",
      "FOLD: 3, EPOCH: 28, valid_loss: 0.01622163038700819\n",
      "FOLD: 3, EPOCH: 29, train_loss: 0.015522890083113874\n",
      "FOLD: 3, EPOCH: 29, valid_loss: 0.016193806832390172\n",
      "FOLD: 4, EPOCH: 0, train_loss: 0.6352605806744617\n",
      "FOLD: 4, EPOCH: 0, valid_loss: 0.13843750229903629\n",
      "FOLD: 4, EPOCH: 1, train_loss: 0.03584965425071077\n",
      "FOLD: 4, EPOCH: 1, valid_loss: 0.02113627206001963\n",
      "FOLD: 4, EPOCH: 2, train_loss: 0.02096170000731945\n",
      "FOLD: 4, EPOCH: 2, valid_loss: 0.01908166717205729\n",
      "FOLD: 4, EPOCH: 3, train_loss: 0.01943299124368291\n",
      "FOLD: 4, EPOCH: 3, valid_loss: 0.018118069267698697\n",
      "FOLD: 4, EPOCH: 4, train_loss: 0.018516676061773214\n",
      "FOLD: 4, EPOCH: 4, valid_loss: 0.0177003324297922\n",
      "FOLD: 4, EPOCH: 5, train_loss: 0.018201519606931917\n",
      "FOLD: 4, EPOCH: 5, valid_loss: 0.0174368895058121\n",
      "FOLD: 4, EPOCH: 6, train_loss: 0.017904952303438946\n",
      "FOLD: 4, EPOCH: 6, valid_loss: 0.017363582311996393\n",
      "FOLD: 4, EPOCH: 7, train_loss: 0.017822351266184578\n",
      "FOLD: 4, EPOCH: 7, valid_loss: 0.017329869445945535\n",
      "FOLD: 4, EPOCH: 8, train_loss: 0.0177574773804973\n",
      "FOLD: 4, EPOCH: 8, valid_loss: 0.017169713122504097\n",
      "FOLD: 4, EPOCH: 9, train_loss: 0.01768006395170654\n",
      "FOLD: 4, EPOCH: 9, valid_loss: 0.017109105629580363\n",
      "FOLD: 4, EPOCH: 10, train_loss: 0.01769691360844434\n",
      "FOLD: 4, EPOCH: 10, valid_loss: 0.017169915991170067\n",
      "FOLD: 4, EPOCH: 11, train_loss: 0.017675761646334675\n",
      "FOLD: 4, EPOCH: 11, valid_loss: 0.017311366088688374\n",
      "FOLD: 4, EPOCH: 12, train_loss: 0.01765623218987299\n",
      "FOLD: 4, EPOCH: 12, valid_loss: 0.017325442417391708\n",
      "FOLD: 4, EPOCH: 13, train_loss: 0.017700486607255712\n",
      "FOLD: 4, EPOCH: 13, valid_loss: 0.017131721707327026\n",
      "FOLD: 4, EPOCH: 14, train_loss: 0.01761746588770462\n",
      "FOLD: 4, EPOCH: 14, valid_loss: 0.01704575794615916\n",
      "FOLD: 4, EPOCH: 15, train_loss: 0.01754437432880851\n",
      "FOLD: 4, EPOCH: 15, valid_loss: 0.0171345056699855\n",
      "FOLD: 4, EPOCH: 16, train_loss: 0.01751338608185018\n",
      "FOLD: 4, EPOCH: 16, valid_loss: 0.017060252678181443\n",
      "FOLD: 4, EPOCH: 17, train_loss: 0.017390984900133764\n",
      "FOLD: 4, EPOCH: 17, valid_loss: 0.01698554227394717\n",
      "FOLD: 4, EPOCH: 18, train_loss: 0.017246980759976566\n",
      "FOLD: 4, EPOCH: 18, valid_loss: 0.01683999358543328\n",
      "FOLD: 4, EPOCH: 19, train_loss: 0.01722212462429551\n",
      "FOLD: 4, EPOCH: 19, valid_loss: 0.01671577566968543\n",
      "FOLD: 4, EPOCH: 20, train_loss: 0.017095134208869676\n",
      "FOLD: 4, EPOCH: 20, valid_loss: 0.016710084756570202\n",
      "FOLD: 4, EPOCH: 21, train_loss: 0.016932780429234972\n",
      "FOLD: 4, EPOCH: 21, valid_loss: 0.01654695610382727\n",
      "FOLD: 4, EPOCH: 22, train_loss: 0.01675183159312692\n",
      "FOLD: 4, EPOCH: 22, valid_loss: 0.01648417783102819\n",
      "FOLD: 4, EPOCH: 23, train_loss: 0.016579698825228043\n",
      "FOLD: 4, EPOCH: 23, valid_loss: 0.01635408606380224\n",
      "FOLD: 4, EPOCH: 24, train_loss: 0.016402082504245682\n",
      "FOLD: 4, EPOCH: 24, valid_loss: 0.01629702549959932\n",
      "FOLD: 4, EPOCH: 25, train_loss: 0.01621323349494217\n",
      "FOLD: 4, EPOCH: 25, valid_loss: 0.016153385995754173\n",
      "FOLD: 4, EPOCH: 26, train_loss: 0.015942639920968508\n",
      "FOLD: 4, EPOCH: 26, valid_loss: 0.016130529716610907\n",
      "FOLD: 4, EPOCH: 27, train_loss: 0.01581230035483621\n",
      "FOLD: 4, EPOCH: 27, valid_loss: 0.016046084623251643\n",
      "FOLD: 4, EPOCH: 28, train_loss: 0.015677109961330458\n",
      "FOLD: 4, EPOCH: 28, valid_loss: 0.01605194229632616\n",
      "FOLD: 4, EPOCH: 29, train_loss: 0.015587252888666548\n",
      "FOLD: 4, EPOCH: 29, valid_loss: 0.016034362571580068\n",
      "FOLD: 0, EPOCH: 0, train_loss: 0.6343683136114175\n",
      "FOLD: 0, EPOCH: 0, valid_loss: 0.14288836760180337\n",
      "FOLD: 0, EPOCH: 1, train_loss: 0.03531576197702384\n",
      "FOLD: 0, EPOCH: 1, valid_loss: 0.021332688363535062\n",
      "FOLD: 0, EPOCH: 2, train_loss: 0.020794326358515282\n",
      "FOLD: 0, EPOCH: 2, valid_loss: 0.019224850460886955\n",
      "FOLD: 0, EPOCH: 3, train_loss: 0.019367767713856007\n",
      "FOLD: 0, EPOCH: 3, valid_loss: 0.018056274365101543\n",
      "FOLD: 0, EPOCH: 4, train_loss: 0.018536129939383354\n",
      "FOLD: 0, EPOCH: 4, valid_loss: 0.017453337886503766\n",
      "FOLD: 0, EPOCH: 5, train_loss: 0.018123879509073668\n",
      "FOLD: 0, EPOCH: 5, valid_loss: 0.017330661336226124\n",
      "FOLD: 0, EPOCH: 6, train_loss: 0.017876114581536123\n",
      "FOLD: 0, EPOCH: 6, valid_loss: 0.01734047005219119\n",
      "FOLD: 0, EPOCH: 7, train_loss: 0.01782777699628386\n",
      "FOLD: 0, EPOCH: 7, valid_loss: 0.017266501326646123\n",
      "FOLD: 0, EPOCH: 8, train_loss: 0.017805953780054184\n",
      "FOLD: 0, EPOCH: 8, valid_loss: 0.017233390573944365\n",
      "FOLD: 0, EPOCH: 9, train_loss: 0.01766365782722183\n",
      "FOLD: 0, EPOCH: 9, valid_loss: 0.01741716414690018\n",
      "FOLD: 0, EPOCH: 10, train_loss: 0.017757363475697195\n",
      "FOLD: 0, EPOCH: 10, valid_loss: 0.01710063498467207\n",
      "FOLD: 0, EPOCH: 11, train_loss: 0.017713725128197584\n",
      "FOLD: 0, EPOCH: 11, valid_loss: 0.017209305880325182\n",
      "FOLD: 0, EPOCH: 12, train_loss: 0.017700307374901098\n",
      "FOLD: 0, EPOCH: 12, valid_loss: 0.017168547133249897\n",
      "FOLD: 0, EPOCH: 13, train_loss: 0.017622715226658012\n",
      "FOLD: 0, EPOCH: 13, valid_loss: 0.01700586351965155\n",
      "FOLD: 0, EPOCH: 14, train_loss: 0.01751771970125644\n",
      "FOLD: 0, EPOCH: 14, valid_loss: 0.017038466887814657\n",
      "FOLD: 0, EPOCH: 15, train_loss: 0.01755140563203157\n",
      "FOLD: 0, EPOCH: 15, valid_loss: 0.016982734362993922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 0, EPOCH: 16, train_loss: 0.017406056155923052\n",
      "FOLD: 0, EPOCH: 16, valid_loss: 0.016940288112631867\n",
      "FOLD: 0, EPOCH: 17, train_loss: 0.017428269798772923\n",
      "FOLD: 0, EPOCH: 17, valid_loss: 0.016983022327933993\n",
      "FOLD: 0, EPOCH: 18, train_loss: 0.017354623225612053\n",
      "FOLD: 0, EPOCH: 18, valid_loss: 0.01688423076910632\n",
      "FOLD: 0, EPOCH: 19, train_loss: 0.017191048306615456\n",
      "FOLD: 0, EPOCH: 19, valid_loss: 0.01682417406035321\n",
      "FOLD: 0, EPOCH: 20, train_loss: 0.017103889822096065\n",
      "FOLD: 0, EPOCH: 20, valid_loss: 0.016680089277880533\n",
      "FOLD: 0, EPOCH: 21, train_loss: 0.016981349974546745\n",
      "FOLD: 0, EPOCH: 21, valid_loss: 0.016630347818136215\n",
      "FOLD: 0, EPOCH: 22, train_loss: 0.016723293034980696\n",
      "FOLD: 0, EPOCH: 22, valid_loss: 0.016507816713835513\n",
      "FOLD: 0, EPOCH: 23, train_loss: 0.01656119391133172\n",
      "FOLD: 0, EPOCH: 23, valid_loss: 0.01637924797832966\n",
      "FOLD: 0, EPOCH: 24, train_loss: 0.01641524795010902\n",
      "FOLD: 0, EPOCH: 24, valid_loss: 0.0162867024274809\n",
      "FOLD: 0, EPOCH: 25, train_loss: 0.016152501997092495\n",
      "FOLD: 0, EPOCH: 25, valid_loss: 0.016300259929682526\n",
      "FOLD: 0, EPOCH: 26, train_loss: 0.01596724478851842\n",
      "FOLD: 0, EPOCH: 26, valid_loss: 0.01617823626313891\n",
      "FOLD: 0, EPOCH: 27, train_loss: 0.01585393081135724\n",
      "FOLD: 0, EPOCH: 27, valid_loss: 0.016115034664315835\n",
      "FOLD: 0, EPOCH: 28, train_loss: 0.01570737378779745\n",
      "FOLD: 0, EPOCH: 28, valid_loss: 0.0161072041573269\n",
      "FOLD: 0, EPOCH: 29, train_loss: 0.015750654271223408\n",
      "FOLD: 0, EPOCH: 29, valid_loss: 0.016074054901088985\n",
      "FOLD: 1, EPOCH: 0, train_loss: 0.6372712289077648\n",
      "FOLD: 1, EPOCH: 0, valid_loss: 0.1388824258531843\n",
      "FOLD: 1, EPOCH: 1, train_loss: 0.03523970020098099\n",
      "FOLD: 1, EPOCH: 1, valid_loss: 0.02147285837147917\n",
      "FOLD: 1, EPOCH: 2, train_loss: 0.021045125035596066\n",
      "FOLD: 1, EPOCH: 2, valid_loss: 0.019048008482371058\n",
      "FOLD: 1, EPOCH: 3, train_loss: 0.01924880931450837\n",
      "FOLD: 1, EPOCH: 3, valid_loss: 0.01824622731655836\n",
      "FOLD: 1, EPOCH: 4, train_loss: 0.018575942258526018\n",
      "FOLD: 1, EPOCH: 4, valid_loss: 0.017544177786580153\n",
      "FOLD: 1, EPOCH: 5, train_loss: 0.018033465444771708\n",
      "FOLD: 1, EPOCH: 5, valid_loss: 0.017366592293339117\n",
      "FOLD: 1, EPOCH: 6, train_loss: 0.0177877323459024\n",
      "FOLD: 1, EPOCH: 6, valid_loss: 0.017347244120069914\n",
      "FOLD: 1, EPOCH: 7, train_loss: 0.017774860037193783\n",
      "FOLD: 1, EPOCH: 7, valid_loss: 0.017326971808714526\n",
      "FOLD: 1, EPOCH: 8, train_loss: 0.017727047812355602\n",
      "FOLD: 1, EPOCH: 8, valid_loss: 0.017229494425867284\n",
      "FOLD: 1, EPOCH: 9, train_loss: 0.01771441200559122\n",
      "FOLD: 1, EPOCH: 9, valid_loss: 0.017251152518604485\n",
      "FOLD: 1, EPOCH: 10, train_loss: 0.017676853082156267\n",
      "FOLD: 1, EPOCH: 10, valid_loss: 0.01730298498379333\n",
      "FOLD: 1, EPOCH: 11, train_loss: 0.017736012878679278\n",
      "FOLD: 1, EPOCH: 11, valid_loss: 0.017153628676065378\n",
      "FOLD: 1, EPOCH: 12, train_loss: 0.017574108725386686\n",
      "FOLD: 1, EPOCH: 12, valid_loss: 0.017122240497597627\n",
      "FOLD: 1, EPOCH: 13, train_loss: 0.017589680189131828\n",
      "FOLD: 1, EPOCH: 13, valid_loss: 0.017100227038775173\n",
      "FOLD: 1, EPOCH: 14, train_loss: 0.01754655321871025\n",
      "FOLD: 1, EPOCH: 14, valid_loss: 0.01707674070660557\n",
      "FOLD: 1, EPOCH: 15, train_loss: 0.017534489500457825\n",
      "FOLD: 1, EPOCH: 15, valid_loss: 0.01705065867198365\n",
      "FOLD: 1, EPOCH: 16, train_loss: 0.017479037346345358\n",
      "FOLD: 1, EPOCH: 16, valid_loss: 0.017081817771707263\n",
      "FOLD: 1, EPOCH: 17, train_loss: 0.01734617158797556\n",
      "FOLD: 1, EPOCH: 17, valid_loss: 0.017027551361492702\n",
      "FOLD: 1, EPOCH: 18, train_loss: 0.017272996884919165\n",
      "FOLD: 1, EPOCH: 18, valid_loss: 0.016817358854625906\n",
      "FOLD: 1, EPOCH: 19, train_loss: 0.017223683179126703\n",
      "FOLD: 1, EPOCH: 19, valid_loss: 0.016788384903754505\n",
      "FOLD: 1, EPOCH: 20, train_loss: 0.01703021368956652\n",
      "FOLD: 1, EPOCH: 20, valid_loss: 0.016783050767013006\n",
      "FOLD: 1, EPOCH: 21, train_loss: 0.016872931831934744\n",
      "FOLD: 1, EPOCH: 21, valid_loss: 0.016627840192190237\n",
      "FOLD: 1, EPOCH: 22, train_loss: 0.01672530137812314\n",
      "FOLD: 1, EPOCH: 22, valid_loss: 0.016592937681291785\n",
      "FOLD: 1, EPOCH: 23, train_loss: 0.01656351233765051\n",
      "FOLD: 1, EPOCH: 23, valid_loss: 0.016466341274125235\n",
      "FOLD: 1, EPOCH: 24, train_loss: 0.01634312733548923\n",
      "FOLD: 1, EPOCH: 24, valid_loss: 0.016320460183279855\n",
      "FOLD: 1, EPOCH: 25, train_loss: 0.016149658190113478\n",
      "FOLD: 1, EPOCH: 25, valid_loss: 0.01626873764076403\n",
      "FOLD: 1, EPOCH: 26, train_loss: 0.015965679916890636\n",
      "FOLD: 1, EPOCH: 26, valid_loss: 0.01623253503016063\n",
      "FOLD: 1, EPOCH: 27, train_loss: 0.015839101233776066\n",
      "FOLD: 1, EPOCH: 27, valid_loss: 0.016158874492560114\n",
      "FOLD: 1, EPOCH: 28, train_loss: 0.015668475021864626\n",
      "FOLD: 1, EPOCH: 28, valid_loss: 0.016141481058938163\n",
      "FOLD: 1, EPOCH: 29, train_loss: 0.015619498535828747\n",
      "FOLD: 1, EPOCH: 29, valid_loss: 0.01616775675543717\n",
      "FOLD: 2, EPOCH: 0, train_loss: 0.6345600681244463\n",
      "FOLD: 2, EPOCH: 0, valid_loss: 0.13682279927389962\n",
      "FOLD: 2, EPOCH: 1, train_loss: 0.03517040716943102\n",
      "FOLD: 2, EPOCH: 1, valid_loss: 0.021248151628034454\n",
      "FOLD: 2, EPOCH: 2, train_loss: 0.020804043859243393\n",
      "FOLD: 2, EPOCH: 2, valid_loss: 0.019079435190984182\n",
      "FOLD: 2, EPOCH: 3, train_loss: 0.01937812134164615\n",
      "FOLD: 2, EPOCH: 3, valid_loss: 0.01809920123113053\n",
      "FOLD: 2, EPOCH: 4, train_loss: 0.018483813372913046\n",
      "FOLD: 2, EPOCH: 4, valid_loss: 0.01768361103854009\n",
      "FOLD: 2, EPOCH: 5, train_loss: 0.01802984469205789\n",
      "FOLD: 2, EPOCH: 5, valid_loss: 0.01734440273472241\n",
      "FOLD: 2, EPOCH: 6, train_loss: 0.017820387721007715\n",
      "FOLD: 2, EPOCH: 6, valid_loss: 0.018027852262769428\n",
      "FOLD: 2, EPOCH: 7, train_loss: 0.017770115461578403\n",
      "FOLD: 2, EPOCH: 7, valid_loss: 0.017165659634130342\n",
      "FOLD: 2, EPOCH: 8, train_loss: 0.01768020754410089\n",
      "FOLD: 2, EPOCH: 8, valid_loss: 0.01717084792575666\n",
      "FOLD: 2, EPOCH: 9, train_loss: 0.017619977914390787\n",
      "FOLD: 2, EPOCH: 9, valid_loss: 0.01724858536784138\n",
      "FOLD: 2, EPOCH: 10, train_loss: 0.017656598503336958\n",
      "FOLD: 2, EPOCH: 10, valid_loss: 0.01752324838723455\n",
      "FOLD: 2, EPOCH: 11, train_loss: 0.01764542969159674\n",
      "FOLD: 2, EPOCH: 11, valid_loss: 0.017138189689389297\n",
      "FOLD: 2, EPOCH: 12, train_loss: 0.017690798004522272\n",
      "FOLD: 2, EPOCH: 12, valid_loss: 0.017113862772073066\n",
      "FOLD: 2, EPOCH: 13, train_loss: 0.017611046422920797\n",
      "FOLD: 2, EPOCH: 13, valid_loss: 0.01717614515551499\n",
      "FOLD: 2, EPOCH: 14, train_loss: 0.01755477824801768\n",
      "FOLD: 2, EPOCH: 14, valid_loss: 0.017038131185940334\n",
      "FOLD: 2, EPOCH: 15, train_loss: 0.017568250949346068\n",
      "FOLD: 2, EPOCH: 15, valid_loss: 0.017066087254456113\n",
      "FOLD: 2, EPOCH: 16, train_loss: 0.017484860872660858\n",
      "FOLD: 2, EPOCH: 16, valid_loss: 0.01698410888867719\n",
      "FOLD: 2, EPOCH: 17, train_loss: 0.01731442788993751\n",
      "FOLD: 2, EPOCH: 17, valid_loss: 0.016834750319165842\n",
      "FOLD: 2, EPOCH: 18, train_loss: 0.017282966706577852\n",
      "FOLD: 2, EPOCH: 18, valid_loss: 0.016957969564412322\n",
      "FOLD: 2, EPOCH: 19, train_loss: 0.0171410888703405\n",
      "FOLD: 2, EPOCH: 19, valid_loss: 0.01674233696290425\n",
      "FOLD: 2, EPOCH: 20, train_loss: 0.017057942118549694\n",
      "FOLD: 2, EPOCH: 20, valid_loss: 0.016580798184233053\n",
      "FOLD: 2, EPOCH: 21, train_loss: 0.0169198135305466\n",
      "FOLD: 2, EPOCH: 21, valid_loss: 0.016594650942300046\n",
      "FOLD: 2, EPOCH: 22, train_loss: 0.016728772113666586\n",
      "FOLD: 2, EPOCH: 22, valid_loss: 0.016459810866841246\n",
      "FOLD: 2, EPOCH: 23, train_loss: 0.016565514060304216\n",
      "FOLD: 2, EPOCH: 23, valid_loss: 0.01640389481825488\n",
      "FOLD: 2, EPOCH: 24, train_loss: 0.016350783603400854\n",
      "FOLD: 2, EPOCH: 24, valid_loss: 0.016238668880292347\n",
      "FOLD: 2, EPOCH: 25, train_loss: 0.016142912600459396\n",
      "FOLD: 2, EPOCH: 25, valid_loss: 0.01613944832767759\n",
      "FOLD: 2, EPOCH: 26, train_loss: 0.015918891296546528\n",
      "FOLD: 2, EPOCH: 26, valid_loss: 0.016036912133651122\n",
      "FOLD: 2, EPOCH: 27, train_loss: 0.015762481555018738\n",
      "FOLD: 2, EPOCH: 27, valid_loss: 0.01602304056286812\n",
      "FOLD: 2, EPOCH: 28, train_loss: 0.015666277552752392\n",
      "FOLD: 2, EPOCH: 28, valid_loss: 0.015986747108399868\n",
      "FOLD: 2, EPOCH: 29, train_loss: 0.015620544195121181\n",
      "FOLD: 2, EPOCH: 29, valid_loss: 0.016009890926735743\n",
      "FOLD: 3, EPOCH: 0, train_loss: 0.6334968044058137\n",
      "FOLD: 3, EPOCH: 0, valid_loss: 0.13681824590478625\n",
      "FOLD: 3, EPOCH: 1, train_loss: 0.036642113646519356\n",
      "FOLD: 3, EPOCH: 1, valid_loss: 0.02101888922708375\n",
      "FOLD: 3, EPOCH: 2, train_loss: 0.021151627832348797\n",
      "FOLD: 3, EPOCH: 2, valid_loss: 0.019753786602190564\n",
      "FOLD: 3, EPOCH: 3, train_loss: 0.019653006477038496\n",
      "FOLD: 3, EPOCH: 3, valid_loss: 0.01844408182161195\n",
      "FOLD: 3, EPOCH: 4, train_loss: 0.01880724610004952\n",
      "FOLD: 3, EPOCH: 4, valid_loss: 0.018018727829413755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 3, EPOCH: 5, train_loss: 0.018292571451730917\n",
      "FOLD: 3, EPOCH: 5, valid_loss: 0.017729481203215464\n",
      "FOLD: 3, EPOCH: 6, train_loss: 0.017903340568739004\n",
      "FOLD: 3, EPOCH: 6, valid_loss: 0.017528375850192138\n",
      "FOLD: 3, EPOCH: 7, train_loss: 0.017796800168150145\n",
      "FOLD: 3, EPOCH: 7, valid_loss: 0.017513979519052165\n",
      "FOLD: 3, EPOCH: 8, train_loss: 0.01765408325076535\n",
      "FOLD: 3, EPOCH: 8, valid_loss: 0.017331282634820255\n",
      "FOLD: 3, EPOCH: 9, train_loss: 0.017581056101598602\n",
      "FOLD: 3, EPOCH: 9, valid_loss: 0.017299326642283373\n",
      "FOLD: 3, EPOCH: 10, train_loss: 0.017661271542580664\n",
      "FOLD: 3, EPOCH: 10, valid_loss: 0.017290898439075265\n",
      "FOLD: 3, EPOCH: 11, train_loss: 0.01758090244687122\n",
      "FOLD: 3, EPOCH: 11, valid_loss: 0.01726261270897729\n",
      "FOLD: 3, EPOCH: 12, train_loss: 0.01758909582470854\n",
      "FOLD: 3, EPOCH: 12, valid_loss: 0.017260216043463776\n",
      "FOLD: 3, EPOCH: 13, train_loss: 0.0174499138366377\n",
      "FOLD: 3, EPOCH: 13, valid_loss: 0.017265091437314237\n",
      "FOLD: 3, EPOCH: 14, train_loss: 0.017467839440897755\n",
      "FOLD: 3, EPOCH: 14, valid_loss: 0.017560497085962978\n",
      "FOLD: 3, EPOCH: 15, train_loss: 0.017381292221176882\n",
      "FOLD: 3, EPOCH: 15, valid_loss: 0.01719192567148379\n",
      "FOLD: 3, EPOCH: 16, train_loss: 0.017365754042090713\n",
      "FOLD: 3, EPOCH: 16, valid_loss: 0.017169303047869885\n",
      "FOLD: 3, EPOCH: 17, train_loss: 0.017189322336428406\n",
      "FOLD: 3, EPOCH: 17, valid_loss: 0.0169436389580369\n",
      "FOLD: 3, EPOCH: 18, train_loss: 0.01719879736934883\n",
      "FOLD: 3, EPOCH: 18, valid_loss: 0.01692377563033785\n",
      "FOLD: 3, EPOCH: 19, train_loss: 0.017091794675081106\n",
      "FOLD: 3, EPOCH: 19, valid_loss: 0.016936636503253666\n",
      "FOLD: 3, EPOCH: 20, train_loss: 0.016925312619170418\n",
      "FOLD: 3, EPOCH: 20, valid_loss: 0.016783873231283258\n",
      "FOLD: 3, EPOCH: 21, train_loss: 0.016768602377640596\n",
      "FOLD: 3, EPOCH: 21, valid_loss: 0.016678447143307754\n",
      "FOLD: 3, EPOCH: 22, train_loss: 0.016568198347923117\n",
      "FOLD: 3, EPOCH: 22, valid_loss: 0.016574728888060365\n",
      "FOLD: 3, EPOCH: 23, train_loss: 0.016336067302989355\n",
      "FOLD: 3, EPOCH: 23, valid_loss: 0.016511156729289465\n",
      "FOLD: 3, EPOCH: 24, train_loss: 0.016195107231159574\n",
      "FOLD: 3, EPOCH: 24, valid_loss: 0.016486862488090993\n",
      "FOLD: 3, EPOCH: 25, train_loss: 0.015984699312273577\n",
      "FOLD: 3, EPOCH: 25, valid_loss: 0.016359715216926165\n",
      "FOLD: 3, EPOCH: 26, train_loss: 0.01575310888659695\n",
      "FOLD: 3, EPOCH: 26, valid_loss: 0.016265517499830042\n",
      "FOLD: 3, EPOCH: 27, train_loss: 0.015576955693625454\n",
      "FOLD: 3, EPOCH: 27, valid_loss: 0.016237491103155272\n",
      "FOLD: 3, EPOCH: 28, train_loss: 0.015511834224605042\n",
      "FOLD: 3, EPOCH: 28, valid_loss: 0.016197173962635654\n",
      "FOLD: 3, EPOCH: 29, train_loss: 0.015418257510316545\n",
      "FOLD: 3, EPOCH: 29, valid_loss: 0.016204001009464265\n",
      "FOLD: 4, EPOCH: 0, train_loss: 0.6350310310937356\n",
      "FOLD: 4, EPOCH: 0, valid_loss: 0.13107583267348152\n",
      "FOLD: 4, EPOCH: 1, train_loss: 0.0362483317680333\n",
      "FOLD: 4, EPOCH: 1, valid_loss: 0.020787789566176277\n",
      "FOLD: 4, EPOCH: 2, train_loss: 0.02110066885749499\n",
      "FOLD: 4, EPOCH: 2, valid_loss: 0.018970590670193944\n",
      "FOLD: 4, EPOCH: 3, train_loss: 0.01919791389229721\n",
      "FOLD: 4, EPOCH: 3, valid_loss: 0.01808074185890811\n",
      "FOLD: 4, EPOCH: 4, train_loss: 0.018518774304538965\n",
      "FOLD: 4, EPOCH: 4, valid_loss: 0.017557628798697676\n",
      "FOLD: 4, EPOCH: 5, train_loss: 0.018038206430071074\n",
      "FOLD: 4, EPOCH: 5, valid_loss: 0.017489947032715593\n",
      "FOLD: 4, EPOCH: 6, train_loss: 0.017889446059268885\n",
      "FOLD: 4, EPOCH: 6, valid_loss: 0.017190559101956233\n",
      "FOLD: 4, EPOCH: 7, train_loss: 0.01768111459829885\n",
      "FOLD: 4, EPOCH: 7, valid_loss: 0.01727774760552815\n",
      "FOLD: 4, EPOCH: 8, train_loss: 0.017767834531116314\n",
      "FOLD: 4, EPOCH: 8, valid_loss: 0.01720629757536309\n",
      "FOLD: 4, EPOCH: 9, train_loss: 0.01769669754160703\n",
      "FOLD: 4, EPOCH: 9, valid_loss: 0.017235853842326572\n",
      "FOLD: 4, EPOCH: 10, train_loss: 0.01775674092029964\n",
      "FOLD: 4, EPOCH: 10, valid_loss: 0.017210982633488518\n",
      "FOLD: 4, EPOCH: 11, train_loss: 0.017678171652706638\n",
      "FOLD: 4, EPOCH: 11, valid_loss: 0.017255911816443717\n",
      "FOLD: 4, EPOCH: 12, train_loss: 0.0176136190260666\n",
      "FOLD: 4, EPOCH: 12, valid_loss: 0.017069674895278046\n",
      "FOLD: 4, EPOCH: 13, train_loss: 0.017616615576696568\n",
      "FOLD: 4, EPOCH: 13, valid_loss: 0.017171507568231652\n",
      "FOLD: 4, EPOCH: 14, train_loss: 0.0176069603531041\n",
      "FOLD: 4, EPOCH: 14, valid_loss: 0.017114715836942195\n",
      "FOLD: 4, EPOCH: 15, train_loss: 0.017555536587547133\n",
      "FOLD: 4, EPOCH: 15, valid_loss: 0.016915543164525714\n",
      "FOLD: 4, EPOCH: 16, train_loss: 0.017547222522451826\n",
      "FOLD: 4, EPOCH: 16, valid_loss: 0.017020038861249174\n",
      "FOLD: 4, EPOCH: 17, train_loss: 0.017437627188105514\n",
      "FOLD: 4, EPOCH: 17, valid_loss: 0.016849385388195515\n",
      "FOLD: 4, EPOCH: 18, train_loss: 0.017362806642347055\n",
      "FOLD: 4, EPOCH: 18, valid_loss: 0.01677043767912047\n",
      "FOLD: 4, EPOCH: 19, train_loss: 0.01716096324009308\n",
      "FOLD: 4, EPOCH: 19, valid_loss: 0.01667648330330849\n",
      "FOLD: 4, EPOCH: 20, train_loss: 0.017085259091918884\n",
      "FOLD: 4, EPOCH: 20, valid_loss: 0.01662040688097477\n",
      "FOLD: 4, EPOCH: 21, train_loss: 0.01695834476824688\n",
      "FOLD: 4, EPOCH: 21, valid_loss: 0.01654202648039375\n",
      "FOLD: 4, EPOCH: 22, train_loss: 0.016774989070667736\n",
      "FOLD: 4, EPOCH: 22, valid_loss: 0.016479464088167462\n",
      "FOLD: 4, EPOCH: 23, train_loss: 0.01661670286262381\n",
      "FOLD: 4, EPOCH: 23, valid_loss: 0.016391384122627122\n",
      "FOLD: 4, EPOCH: 24, train_loss: 0.01641233542772091\n",
      "FOLD: 4, EPOCH: 24, valid_loss: 0.016252469750387327\n",
      "FOLD: 4, EPOCH: 25, train_loss: 0.016136095656648926\n",
      "FOLD: 4, EPOCH: 25, valid_loss: 0.01613669539136546\n",
      "FOLD: 4, EPOCH: 26, train_loss: 0.015978223246022844\n",
      "FOLD: 4, EPOCH: 26, valid_loss: 0.016142093257180284\n",
      "FOLD: 4, EPOCH: 27, train_loss: 0.01584461178171678\n",
      "FOLD: 4, EPOCH: 27, valid_loss: 0.01604886403573411\n",
      "FOLD: 4, EPOCH: 28, train_loss: 0.015695171174255833\n",
      "FOLD: 4, EPOCH: 28, valid_loss: 0.016037714960319655\n",
      "FOLD: 4, EPOCH: 29, train_loss: 0.015617028850576153\n",
      "FOLD: 4, EPOCH: 29, valid_loss: 0.016037616107080665\n",
      "FOLD: 0, EPOCH: 0, train_loss: 0.636672615033129\n",
      "FOLD: 0, EPOCH: 0, valid_loss: 0.1433011314698628\n",
      "FOLD: 0, EPOCH: 1, train_loss: 0.0366623629613415\n",
      "FOLD: 0, EPOCH: 1, valid_loss: 0.02082220491554056\n",
      "FOLD: 0, EPOCH: 2, train_loss: 0.020612879285075957\n",
      "FOLD: 0, EPOCH: 2, valid_loss: 0.018853939164962086\n",
      "FOLD: 0, EPOCH: 3, train_loss: 0.019273043596658154\n",
      "FOLD: 0, EPOCH: 3, valid_loss: 0.01788512614688703\n",
      "FOLD: 0, EPOCH: 4, train_loss: 0.018499896983089653\n",
      "FOLD: 0, EPOCH: 4, valid_loss: 0.017612802609801293\n",
      "FOLD: 0, EPOCH: 5, train_loss: 0.018098770397836746\n",
      "FOLD: 0, EPOCH: 5, valid_loss: 0.017392440512776376\n",
      "FOLD: 0, EPOCH: 6, train_loss: 0.017880094345605026\n",
      "FOLD: 0, EPOCH: 6, valid_loss: 0.01706566358251231\n",
      "FOLD: 0, EPOCH: 7, train_loss: 0.017762077074713896\n",
      "FOLD: 0, EPOCH: 7, valid_loss: 0.017149018496274947\n",
      "FOLD: 0, EPOCH: 8, train_loss: 0.017649487335828766\n",
      "FOLD: 0, EPOCH: 8, valid_loss: 0.017232719915253776\n",
      "FOLD: 0, EPOCH: 9, train_loss: 0.01775292360453286\n",
      "FOLD: 0, EPOCH: 9, valid_loss: 0.017155170813202857\n",
      "FOLD: 0, EPOCH: 10, train_loss: 0.017656298752442217\n",
      "FOLD: 0, EPOCH: 10, valid_loss: 0.017232930952949182\n",
      "FOLD: 0, EPOCH: 11, train_loss: 0.01765523222612514\n",
      "FOLD: 0, EPOCH: 11, valid_loss: 0.017060435962464128\n",
      "FOLD: 0, EPOCH: 12, train_loss: 0.0175834441948952\n",
      "FOLD: 0, EPOCH: 12, valid_loss: 0.017043149444673743\n",
      "FOLD: 0, EPOCH: 13, train_loss: 0.01757377124917896\n",
      "FOLD: 0, EPOCH: 13, valid_loss: 0.017127611967069762\n",
      "FOLD: 0, EPOCH: 14, train_loss: 0.01758602814262976\n",
      "FOLD: 0, EPOCH: 14, valid_loss: 0.01714167304869209\n",
      "FOLD: 0, EPOCH: 15, train_loss: 0.01753127045146581\n",
      "FOLD: 0, EPOCH: 15, valid_loss: 0.017247071276818002\n",
      "FOLD: 0, EPOCH: 16, train_loss: 0.017418503855773503\n",
      "FOLD: 0, EPOCH: 16, valid_loss: 0.016919550406081337\n",
      "FOLD: 0, EPOCH: 17, train_loss: 0.01738777999644694\n",
      "FOLD: 0, EPOCH: 17, valid_loss: 0.016916868276894093\n",
      "FOLD: 0, EPOCH: 18, train_loss: 0.017220991501665634\n",
      "FOLD: 0, EPOCH: 18, valid_loss: 0.016720119890357765\n",
      "FOLD: 0, EPOCH: 19, train_loss: 0.017074407820684322\n",
      "FOLD: 0, EPOCH: 19, valid_loss: 0.016758336339678083\n",
      "FOLD: 0, EPOCH: 20, train_loss: 0.017001272381647774\n",
      "FOLD: 0, EPOCH: 20, valid_loss: 0.016625679710081647\n",
      "FOLD: 0, EPOCH: 21, train_loss: 0.016913811615465776\n",
      "FOLD: 0, EPOCH: 21, valid_loss: 0.0165628476866654\n",
      "FOLD: 0, EPOCH: 22, train_loss: 0.016676837193739157\n",
      "FOLD: 0, EPOCH: 22, valid_loss: 0.016463033136512553\n",
      "FOLD: 0, EPOCH: 23, train_loss: 0.016490950579822496\n",
      "FOLD: 0, EPOCH: 23, valid_loss: 0.016338257571416243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 0, EPOCH: 24, train_loss: 0.0162845093594945\n",
      "FOLD: 0, EPOCH: 24, valid_loss: 0.016312060318887232\n",
      "FOLD: 0, EPOCH: 25, train_loss: 0.016067105709858562\n",
      "FOLD: 0, EPOCH: 25, valid_loss: 0.016173003134982926\n",
      "FOLD: 0, EPOCH: 26, train_loss: 0.01585450867001993\n",
      "FOLD: 0, EPOCH: 26, valid_loss: 0.016104704939893313\n",
      "FOLD: 0, EPOCH: 27, train_loss: 0.01567392460866899\n",
      "FOLD: 0, EPOCH: 27, valid_loss: 0.016083914626921925\n",
      "FOLD: 0, EPOCH: 28, train_loss: 0.015591973931515131\n",
      "FOLD: 0, EPOCH: 28, valid_loss: 0.01604442237211125\n",
      "FOLD: 0, EPOCH: 29, train_loss: 0.015477115862017525\n",
      "FOLD: 0, EPOCH: 29, valid_loss: 0.016055358148046903\n",
      "FOLD: 1, EPOCH: 0, train_loss: 0.6342874024657236\n",
      "FOLD: 1, EPOCH: 0, valid_loss: 0.12918611594608853\n",
      "FOLD: 1, EPOCH: 1, train_loss: 0.035637856987507446\n",
      "FOLD: 1, EPOCH: 1, valid_loss: 0.02094732102538858\n",
      "FOLD: 1, EPOCH: 2, train_loss: 0.021171640969164993\n",
      "FOLD: 1, EPOCH: 2, valid_loss: 0.019071066113454955\n",
      "FOLD: 1, EPOCH: 3, train_loss: 0.019390746222242065\n",
      "FOLD: 1, EPOCH: 3, valid_loss: 0.018118504248559476\n",
      "FOLD: 1, EPOCH: 4, train_loss: 0.018556719585119383\n",
      "FOLD: 1, EPOCH: 4, valid_loss: 0.017655858876449722\n",
      "FOLD: 1, EPOCH: 5, train_loss: 0.018184077230862516\n",
      "FOLD: 1, EPOCH: 5, valid_loss: 0.0175788159349135\n",
      "FOLD: 1, EPOCH: 6, train_loss: 0.0178975913727629\n",
      "FOLD: 1, EPOCH: 6, valid_loss: 0.017367824858852795\n",
      "FOLD: 1, EPOCH: 7, train_loss: 0.0177827974025538\n",
      "FOLD: 1, EPOCH: 7, valid_loss: 0.01724308455096824\n",
      "FOLD: 1, EPOCH: 8, train_loss: 0.01770600264190116\n",
      "FOLD: 1, EPOCH: 8, valid_loss: 0.017346354387700557\n",
      "FOLD: 1, EPOCH: 9, train_loss: 0.017740947882766308\n",
      "FOLD: 1, EPOCH: 9, valid_loss: 0.01715930306485721\n",
      "FOLD: 1, EPOCH: 10, train_loss: 0.017625829157675955\n",
      "FOLD: 1, EPOCH: 10, valid_loss: 0.01730816766087498\n",
      "FOLD: 1, EPOCH: 11, train_loss: 0.017650414089523794\n",
      "FOLD: 1, EPOCH: 11, valid_loss: 0.017134399233119827\n",
      "FOLD: 1, EPOCH: 12, train_loss: 0.017587866937822622\n",
      "FOLD: 1, EPOCH: 12, valid_loss: 0.017184909619390964\n",
      "FOLD: 1, EPOCH: 13, train_loss: 0.017531672033710755\n",
      "FOLD: 1, EPOCH: 13, valid_loss: 0.017149646207690238\n",
      "FOLD: 1, EPOCH: 14, train_loss: 0.017524427167423393\n",
      "FOLD: 1, EPOCH: 14, valid_loss: 0.017140200468046325\n",
      "FOLD: 1, EPOCH: 15, train_loss: 0.01745337338956154\n",
      "FOLD: 1, EPOCH: 15, valid_loss: 0.017113143471734864\n",
      "FOLD: 1, EPOCH: 16, train_loss: 0.017431236628065075\n",
      "FOLD: 1, EPOCH: 16, valid_loss: 0.01696710264576333\n",
      "FOLD: 1, EPOCH: 17, train_loss: 0.017304746332861807\n",
      "FOLD: 1, EPOCH: 17, valid_loss: 0.016929314844310282\n",
      "FOLD: 1, EPOCH: 18, train_loss: 0.017218582554841818\n",
      "FOLD: 1, EPOCH: 18, valid_loss: 0.016900363777365004\n",
      "FOLD: 1, EPOCH: 19, train_loss: 0.017177653505696333\n",
      "FOLD: 1, EPOCH: 19, valid_loss: 0.016732907694365296\n",
      "FOLD: 1, EPOCH: 20, train_loss: 0.01703442200559421\n",
      "FOLD: 1, EPOCH: 20, valid_loss: 0.016666416132024357\n",
      "FOLD: 1, EPOCH: 21, train_loss: 0.01690436530507345\n",
      "FOLD: 1, EPOCH: 21, valid_loss: 0.01661005916872195\n",
      "FOLD: 1, EPOCH: 22, train_loss: 0.01670053246282581\n",
      "FOLD: 1, EPOCH: 22, valid_loss: 0.016505617515317032\n",
      "FOLD: 1, EPOCH: 23, train_loss: 0.016502606345043667\n",
      "FOLD: 1, EPOCH: 23, valid_loss: 0.016348523141017982\n",
      "FOLD: 1, EPOCH: 24, train_loss: 0.016298433603799862\n",
      "FOLD: 1, EPOCH: 24, valid_loss: 0.016329067679388184\n",
      "FOLD: 1, EPOCH: 25, train_loss: 0.01607680029195288\n",
      "FOLD: 1, EPOCH: 25, valid_loss: 0.016263814909117564\n",
      "FOLD: 1, EPOCH: 26, train_loss: 0.015869047708701397\n",
      "FOLD: 1, EPOCH: 26, valid_loss: 0.01622085150863443\n",
      "FOLD: 1, EPOCH: 27, train_loss: 0.015692997143428394\n",
      "FOLD: 1, EPOCH: 27, valid_loss: 0.01615786688136203\n",
      "FOLD: 1, EPOCH: 28, train_loss: 0.01561030786673444\n",
      "FOLD: 1, EPOCH: 28, valid_loss: 0.016149518133274147\n",
      "FOLD: 1, EPOCH: 29, train_loss: 0.01554169667803723\n",
      "FOLD: 1, EPOCH: 29, valid_loss: 0.016150975573275772\n",
      "FOLD: 2, EPOCH: 0, train_loss: 0.634245029286198\n",
      "FOLD: 2, EPOCH: 0, valid_loss: 0.12764676937035152\n",
      "FOLD: 2, EPOCH: 1, train_loss: 0.03590693135840305\n",
      "FOLD: 2, EPOCH: 1, valid_loss: 0.020704339444637298\n",
      "FOLD: 2, EPOCH: 2, train_loss: 0.020948943385071514\n",
      "FOLD: 2, EPOCH: 2, valid_loss: 0.019036935163395746\n",
      "FOLD: 2, EPOCH: 3, train_loss: 0.01942067949668221\n",
      "FOLD: 2, EPOCH: 3, valid_loss: 0.01804999645267214\n",
      "FOLD: 2, EPOCH: 4, train_loss: 0.018540237949270268\n",
      "FOLD: 2, EPOCH: 4, valid_loss: 0.01763576136103698\n",
      "FOLD: 2, EPOCH: 5, train_loss: 0.018072941293265078\n",
      "FOLD: 2, EPOCH: 5, valid_loss: 0.01751358397305012\n",
      "FOLD: 2, EPOCH: 6, train_loss: 0.017788240665812857\n",
      "FOLD: 2, EPOCH: 6, valid_loss: 0.01725271615598883\n",
      "FOLD: 2, EPOCH: 7, train_loss: 0.01768730303434574\n",
      "FOLD: 2, EPOCH: 7, valid_loss: 0.01734092738479376\n",
      "FOLD: 2, EPOCH: 8, train_loss: 0.017691841220780127\n",
      "FOLD: 2, EPOCH: 8, valid_loss: 0.017468212278825897\n",
      "FOLD: 2, EPOCH: 9, train_loss: 0.01767166635102552\n",
      "FOLD: 2, EPOCH: 9, valid_loss: 0.01714235238198723\n",
      "FOLD: 2, EPOCH: 10, train_loss: 0.01771028613185753\n",
      "FOLD: 2, EPOCH: 10, valid_loss: 0.017234086564608984\n",
      "FOLD: 2, EPOCH: 11, train_loss: 0.01761274248280603\n",
      "FOLD: 2, EPOCH: 11, valid_loss: 0.01717462438557829\n",
      "FOLD: 2, EPOCH: 12, train_loss: 0.017595388346176216\n",
      "FOLD: 2, EPOCH: 12, valid_loss: 0.017305164810802257\n",
      "FOLD: 2, EPOCH: 13, train_loss: 0.017529919649055904\n",
      "FOLD: 2, EPOCH: 13, valid_loss: 0.016987669547753676\n",
      "FOLD: 2, EPOCH: 14, train_loss: 0.01757350526885062\n",
      "FOLD: 2, EPOCH: 14, valid_loss: 0.017112614986087595\n",
      "FOLD: 2, EPOCH: 15, train_loss: 0.017474598129806312\n",
      "FOLD: 2, EPOCH: 15, valid_loss: 0.017279155658824104\n",
      "FOLD: 2, EPOCH: 16, train_loss: 0.01749251027038132\n",
      "FOLD: 2, EPOCH: 16, valid_loss: 0.017020066614661898\n",
      "FOLD: 2, EPOCH: 17, train_loss: 0.01742184322759293\n",
      "FOLD: 2, EPOCH: 17, valid_loss: 0.016940800632749286\n",
      "FOLD: 2, EPOCH: 18, train_loss: 0.017287530335665182\n",
      "FOLD: 2, EPOCH: 18, valid_loss: 0.016964590097112316\n",
      "FOLD: 2, EPOCH: 19, train_loss: 0.01716809307454505\n",
      "FOLD: 2, EPOCH: 19, valid_loss: 0.01676763631403446\n",
      "FOLD: 2, EPOCH: 20, train_loss: 0.0170357474260896\n",
      "FOLD: 2, EPOCH: 20, valid_loss: 0.01653377472289971\n",
      "FOLD: 2, EPOCH: 21, train_loss: 0.016895638765308304\n",
      "FOLD: 2, EPOCH: 21, valid_loss: 0.016503005394978183\n",
      "FOLD: 2, EPOCH: 22, train_loss: 0.016712856539727552\n",
      "FOLD: 2, EPOCH: 22, valid_loss: 0.01644808729844434\n",
      "FOLD: 2, EPOCH: 23, train_loss: 0.016536425704649395\n",
      "FOLD: 2, EPOCH: 23, valid_loss: 0.016353575379720757\n",
      "FOLD: 2, EPOCH: 24, train_loss: 0.01630374823656419\n",
      "FOLD: 2, EPOCH: 24, valid_loss: 0.016273011320403645\n",
      "FOLD: 2, EPOCH: 25, train_loss: 0.016108359466644302\n",
      "FOLD: 2, EPOCH: 25, valid_loss: 0.01614572698516505\n",
      "FOLD: 2, EPOCH: 26, train_loss: 0.01594312128651401\n",
      "FOLD: 2, EPOCH: 26, valid_loss: 0.016107881149011\n",
      "FOLD: 2, EPOCH: 27, train_loss: 0.015734916579896126\n",
      "FOLD: 2, EPOCH: 27, valid_loss: 0.016059860720166137\n",
      "FOLD: 2, EPOCH: 28, train_loss: 0.015600916153440872\n",
      "FOLD: 2, EPOCH: 28, valid_loss: 0.016021462636334555\n",
      "FOLD: 2, EPOCH: 29, train_loss: 0.015565567541921484\n",
      "FOLD: 2, EPOCH: 29, valid_loss: 0.01607942126159157\n",
      "FOLD: 3, EPOCH: 0, train_loss: 0.6364813223481178\n",
      "FOLD: 3, EPOCH: 0, valid_loss: 0.14111183285713197\n",
      "FOLD: 3, EPOCH: 1, train_loss: 0.03594470431731231\n",
      "FOLD: 3, EPOCH: 1, valid_loss: 0.02099953219294548\n",
      "FOLD: 3, EPOCH: 2, train_loss: 0.02097799484550521\n",
      "FOLD: 3, EPOCH: 2, valid_loss: 0.01941777127129691\n",
      "FOLD: 3, EPOCH: 3, train_loss: 0.019494764290858006\n",
      "FOLD: 3, EPOCH: 3, valid_loss: 0.01832880074424403\n",
      "FOLD: 3, EPOCH: 4, train_loss: 0.018589633031059868\n",
      "FOLD: 3, EPOCH: 4, valid_loss: 0.01799278972404344\n",
      "FOLD: 3, EPOCH: 5, train_loss: 0.018177855132228655\n",
      "FOLD: 3, EPOCH: 5, valid_loss: 0.0175488327230726\n",
      "FOLD: 3, EPOCH: 6, train_loss: 0.017943877551326717\n",
      "FOLD: 3, EPOCH: 6, valid_loss: 0.01749064155987331\n",
      "FOLD: 3, EPOCH: 7, train_loss: 0.017645633705230295\n",
      "FOLD: 3, EPOCH: 7, valid_loss: 0.017490051739982197\n",
      "FOLD: 3, EPOCH: 8, train_loss: 0.01764117530884518\n",
      "FOLD: 3, EPOCH: 8, valid_loss: 0.017567103755261215\n",
      "FOLD: 3, EPOCH: 9, train_loss: 0.017597425074411043\n",
      "FOLD: 3, EPOCH: 9, valid_loss: 0.017321219188826426\n",
      "FOLD: 3, EPOCH: 10, train_loss: 0.0176233175629075\n",
      "FOLD: 3, EPOCH: 10, valid_loss: 0.017342934251895974\n",
      "FOLD: 3, EPOCH: 11, train_loss: 0.01766658267951098\n",
      "FOLD: 3, EPOCH: 11, valid_loss: 0.01754008810967207\n",
      "FOLD: 3, EPOCH: 12, train_loss: 0.017667322399337656\n",
      "FOLD: 3, EPOCH: 12, valid_loss: 0.01730045388851847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 3, EPOCH: 13, train_loss: 0.017466556449569223\n",
      "FOLD: 3, EPOCH: 13, valid_loss: 0.017433598744017738\n",
      "FOLD: 3, EPOCH: 14, train_loss: 0.017430314652459776\n",
      "FOLD: 3, EPOCH: 14, valid_loss: 0.017155653557607106\n",
      "FOLD: 3, EPOCH: 15, train_loss: 0.017394927600263687\n",
      "FOLD: 3, EPOCH: 15, valid_loss: 0.017228461376258306\n",
      "FOLD: 3, EPOCH: 16, train_loss: 0.017315842292231064\n",
      "FOLD: 3, EPOCH: 16, valid_loss: 0.017210461172674385\n",
      "FOLD: 3, EPOCH: 17, train_loss: 0.017283777902037768\n",
      "FOLD: 3, EPOCH: 17, valid_loss: 0.017101695628038475\n",
      "FOLD: 3, EPOCH: 18, train_loss: 0.017146753477931456\n",
      "FOLD: 3, EPOCH: 18, valid_loss: 0.016889447239892823\n",
      "FOLD: 3, EPOCH: 19, train_loss: 0.017092089033753113\n",
      "FOLD: 3, EPOCH: 19, valid_loss: 0.016877833913479533\n",
      "FOLD: 3, EPOCH: 20, train_loss: 0.016898686538679875\n",
      "FOLD: 3, EPOCH: 20, valid_loss: 0.01677028103066342\n",
      "FOLD: 3, EPOCH: 21, train_loss: 0.016785534861349108\n",
      "FOLD: 3, EPOCH: 21, valid_loss: 0.0166994929845844\n",
      "FOLD: 3, EPOCH: 22, train_loss: 0.016667546481703936\n",
      "FOLD: 3, EPOCH: 22, valid_loss: 0.016603217433605877\n",
      "FOLD: 3, EPOCH: 23, train_loss: 0.01643604345187761\n",
      "FOLD: 3, EPOCH: 23, valid_loss: 0.01652801646185773\n",
      "FOLD: 3, EPOCH: 24, train_loss: 0.016220323849415432\n",
      "FOLD: 3, EPOCH: 24, valid_loss: 0.016403626703790256\n",
      "FOLD: 3, EPOCH: 25, train_loss: 0.01600416140306903\n",
      "FOLD: 3, EPOCH: 25, valid_loss: 0.0163239595081125\n",
      "FOLD: 3, EPOCH: 26, train_loss: 0.015818836869321007\n",
      "FOLD: 3, EPOCH: 26, valid_loss: 0.01626704510833536\n",
      "FOLD: 3, EPOCH: 27, train_loss: 0.015606439881620632\n",
      "FOLD: 3, EPOCH: 27, valid_loss: 0.01624977753630706\n",
      "FOLD: 3, EPOCH: 28, train_loss: 0.015483889587061562\n",
      "FOLD: 3, EPOCH: 28, valid_loss: 0.016206516405301435\n",
      "FOLD: 3, EPOCH: 29, train_loss: 0.015427365251209425\n",
      "FOLD: 3, EPOCH: 29, valid_loss: 0.01621912523571934\n",
      "FOLD: 4, EPOCH: 0, train_loss: 0.6338825403996136\n",
      "FOLD: 4, EPOCH: 0, valid_loss: 0.12781960176570076\n",
      "FOLD: 4, EPOCH: 1, train_loss: 0.03597381326329449\n",
      "FOLD: 4, EPOCH: 1, valid_loss: 0.021108537965587208\n",
      "FOLD: 4, EPOCH: 2, train_loss: 0.021178183334785095\n",
      "FOLD: 4, EPOCH: 2, valid_loss: 0.018827465761985098\n",
      "FOLD: 4, EPOCH: 3, train_loss: 0.01933366754024789\n",
      "FOLD: 4, EPOCH: 3, valid_loss: 0.01809160786547831\n",
      "FOLD: 4, EPOCH: 4, train_loss: 0.018421277815503054\n",
      "FOLD: 4, EPOCH: 4, valid_loss: 0.01760574145508664\n",
      "FOLD: 4, EPOCH: 5, train_loss: 0.018066720132702503\n",
      "FOLD: 4, EPOCH: 5, valid_loss: 0.01741751597395965\n",
      "FOLD: 4, EPOCH: 6, train_loss: 0.017751978170396625\n",
      "FOLD: 4, EPOCH: 6, valid_loss: 0.017358710164470333\n",
      "FOLD: 4, EPOCH: 7, train_loss: 0.017743808892218098\n",
      "FOLD: 4, EPOCH: 7, valid_loss: 0.017470909442220416\n",
      "FOLD: 4, EPOCH: 8, train_loss: 0.017700226241857676\n",
      "FOLD: 4, EPOCH: 8, valid_loss: 0.01727319173514843\n",
      "FOLD: 4, EPOCH: 9, train_loss: 0.01769476227354312\n",
      "FOLD: 4, EPOCH: 9, valid_loss: 0.017301870190671514\n",
      "FOLD: 4, EPOCH: 10, train_loss: 0.017660824615724276\n",
      "FOLD: 4, EPOCH: 10, valid_loss: 0.01709595386471067\n",
      "FOLD: 4, EPOCH: 11, train_loss: 0.017622694859038227\n",
      "FOLD: 4, EPOCH: 11, valid_loss: 0.01730012164584228\n",
      "FOLD: 4, EPOCH: 12, train_loss: 0.01764740682868422\n",
      "FOLD: 4, EPOCH: 12, valid_loss: 0.017094850141022886\n",
      "FOLD: 4, EPOCH: 13, train_loss: 0.01759259287348908\n",
      "FOLD: 4, EPOCH: 13, valid_loss: 0.017099234594830443\n",
      "FOLD: 4, EPOCH: 14, train_loss: 0.01765817297163649\n",
      "FOLD: 4, EPOCH: 14, valid_loss: 0.01709760520607233\n",
      "FOLD: 4, EPOCH: 15, train_loss: 0.017483619237453608\n",
      "FOLD: 4, EPOCH: 15, valid_loss: 0.017051850285913262\n",
      "FOLD: 4, EPOCH: 16, train_loss: 0.017419367880169033\n",
      "FOLD: 4, EPOCH: 16, valid_loss: 0.01693751915757145\n",
      "FOLD: 4, EPOCH: 17, train_loss: 0.01740881599539864\n",
      "FOLD: 4, EPOCH: 17, valid_loss: 0.01695023298795734\n",
      "FOLD: 4, EPOCH: 18, train_loss: 0.017242805222454277\n",
      "FOLD: 4, EPOCH: 18, valid_loss: 0.016900683806410858\n",
      "FOLD: 4, EPOCH: 19, train_loss: 0.017242156063624912\n",
      "FOLD: 4, EPOCH: 19, valid_loss: 0.01664722008364541\n",
      "FOLD: 4, EPOCH: 20, train_loss: 0.01708841306980753\n",
      "FOLD: 4, EPOCH: 20, valid_loss: 0.016650464651840075\n",
      "FOLD: 4, EPOCH: 21, train_loss: 0.016834169180820816\n",
      "FOLD: 4, EPOCH: 21, valid_loss: 0.016540342143603733\n",
      "FOLD: 4, EPOCH: 22, train_loss: 0.01675535768162513\n",
      "FOLD: 4, EPOCH: 22, valid_loss: 0.01638860162347555\n",
      "FOLD: 4, EPOCH: 23, train_loss: 0.016560190256037142\n",
      "FOLD: 4, EPOCH: 23, valid_loss: 0.01637209566043956\n",
      "FOLD: 4, EPOCH: 24, train_loss: 0.016365999504383923\n",
      "FOLD: 4, EPOCH: 24, valid_loss: 0.016248819950435842\n",
      "FOLD: 4, EPOCH: 25, train_loss: 0.01615441318574375\n",
      "FOLD: 4, EPOCH: 25, valid_loss: 0.01611471577946629\n",
      "FOLD: 4, EPOCH: 26, train_loss: 0.01591395048856519\n",
      "FOLD: 4, EPOCH: 26, valid_loss: 0.01607184891722032\n",
      "FOLD: 4, EPOCH: 27, train_loss: 0.015831801709651514\n",
      "FOLD: 4, EPOCH: 27, valid_loss: 0.016005246208182403\n",
      "FOLD: 4, EPOCH: 28, train_loss: 0.015684953600546156\n",
      "FOLD: 4, EPOCH: 28, valid_loss: 0.015999386885336468\n",
      "FOLD: 4, EPOCH: 29, train_loss: 0.015598242346575295\n",
      "FOLD: 4, EPOCH: 29, valid_loss: 0.01600140349141189\n",
      "FOLD: 0, EPOCH: 0, train_loss: 0.6352356670127399\n",
      "FOLD: 0, EPOCH: 0, valid_loss: 0.13103306250912802\n",
      "FOLD: 0, EPOCH: 1, train_loss: 0.035905203782022\n",
      "FOLD: 0, EPOCH: 1, valid_loss: 0.020728142240217753\n",
      "FOLD: 0, EPOCH: 2, train_loss: 0.02099130426843961\n",
      "FOLD: 0, EPOCH: 2, valid_loss: 0.018954697623848916\n",
      "FOLD: 0, EPOCH: 3, train_loss: 0.019435637711506824\n",
      "FOLD: 0, EPOCH: 3, valid_loss: 0.01827931497246027\n",
      "FOLD: 0, EPOCH: 4, train_loss: 0.018629993750727263\n",
      "FOLD: 0, EPOCH: 4, valid_loss: 0.017490871756204538\n",
      "FOLD: 0, EPOCH: 5, train_loss: 0.01798853708728068\n",
      "FOLD: 0, EPOCH: 5, valid_loss: 0.017388683663947243\n",
      "FOLD: 0, EPOCH: 6, train_loss: 0.017871500067142904\n",
      "FOLD: 0, EPOCH: 6, valid_loss: 0.017281403951346874\n",
      "FOLD: 0, EPOCH: 7, train_loss: 0.017737726303006428\n",
      "FOLD: 0, EPOCH: 7, valid_loss: 0.01727418630783047\n",
      "FOLD: 0, EPOCH: 8, train_loss: 0.01778861737229686\n",
      "FOLD: 0, EPOCH: 8, valid_loss: 0.01738959838237081\n",
      "FOLD: 0, EPOCH: 9, train_loss: 0.017755266359966736\n",
      "FOLD: 0, EPOCH: 9, valid_loss: 0.017275519243308477\n",
      "FOLD: 0, EPOCH: 10, train_loss: 0.017644254598712574\n",
      "FOLD: 0, EPOCH: 10, valid_loss: 0.017260156997612543\n",
      "FOLD: 0, EPOCH: 11, train_loss: 0.017661117874355852\n",
      "FOLD: 0, EPOCH: 11, valid_loss: 0.017225534921245917\n",
      "FOLD: 0, EPOCH: 12, train_loss: 0.017724713141881468\n",
      "FOLD: 0, EPOCH: 12, valid_loss: 0.01719977544354541\n",
      "FOLD: 0, EPOCH: 13, train_loss: 0.017575641034467928\n",
      "FOLD: 0, EPOCH: 13, valid_loss: 0.01708627606609038\n",
      "FOLD: 0, EPOCH: 14, train_loss: 0.01754368652684101\n",
      "FOLD: 0, EPOCH: 14, valid_loss: 0.01713183314672538\n",
      "FOLD: 0, EPOCH: 15, train_loss: 0.017542565605886604\n",
      "FOLD: 0, EPOCH: 15, valid_loss: 0.01697323870446001\n",
      "FOLD: 0, EPOCH: 16, train_loss: 0.017459783743581047\n",
      "FOLD: 0, EPOCH: 16, valid_loss: 0.017055768838950564\n",
      "FOLD: 0, EPOCH: 17, train_loss: 0.01739774425716504\n",
      "FOLD: 0, EPOCH: 17, valid_loss: 0.016913759335875512\n",
      "FOLD: 0, EPOCH: 18, train_loss: 0.017255460755710585\n",
      "FOLD: 0, EPOCH: 18, valid_loss: 0.01686328559049538\n",
      "FOLD: 0, EPOCH: 19, train_loss: 0.017142711004809193\n",
      "FOLD: 0, EPOCH: 19, valid_loss: 0.01687608550169638\n",
      "FOLD: 0, EPOCH: 20, train_loss: 0.017042280478483957\n",
      "FOLD: 0, EPOCH: 20, valid_loss: 0.016660177973764283\n",
      "FOLD: 0, EPOCH: 21, train_loss: 0.01684851813521506\n",
      "FOLD: 0, EPOCH: 21, valid_loss: 0.016534129051225525\n",
      "FOLD: 0, EPOCH: 22, train_loss: 0.016786765860582607\n",
      "FOLD: 0, EPOCH: 22, valid_loss: 0.01643286757171154\n",
      "FOLD: 0, EPOCH: 23, train_loss: 0.016558886876842684\n",
      "FOLD: 0, EPOCH: 23, valid_loss: 0.01635492659573044\n",
      "FOLD: 0, EPOCH: 24, train_loss: 0.01628985212764878\n",
      "FOLD: 0, EPOCH: 24, valid_loss: 0.01625355790768351\n",
      "FOLD: 0, EPOCH: 25, train_loss: 0.016140332870671282\n",
      "FOLD: 0, EPOCH: 25, valid_loss: 0.01615332029759884\n",
      "FOLD: 0, EPOCH: 26, train_loss: 0.015925431333860193\n",
      "FOLD: 0, EPOCH: 26, valid_loss: 0.016132000328174658\n",
      "FOLD: 0, EPOCH: 27, train_loss: 0.015740187420253304\n",
      "FOLD: 0, EPOCH: 27, valid_loss: 0.016053805393832072\n",
      "FOLD: 0, EPOCH: 28, train_loss: 0.015642513021610786\n",
      "FOLD: 0, EPOCH: 28, valid_loss: 0.01604935260755675\n",
      "FOLD: 0, EPOCH: 29, train_loss: 0.01555271919114866\n",
      "FOLD: 0, EPOCH: 29, valid_loss: 0.016061945312789507\n",
      "FOLD: 1, EPOCH: 0, train_loss: 0.6354329848419065\n",
      "FOLD: 1, EPOCH: 0, valid_loss: 0.12024201921054295\n",
      "FOLD: 1, EPOCH: 1, train_loss: 0.03573544239760309\n",
      "FOLD: 1, EPOCH: 1, valid_loss: 0.020679812133312225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 1, EPOCH: 2, train_loss: 0.020949773598408352\n",
      "FOLD: 1, EPOCH: 2, valid_loss: 0.019174949771591596\n",
      "FOLD: 1, EPOCH: 3, train_loss: 0.01926739365402339\n",
      "FOLD: 1, EPOCH: 3, valid_loss: 0.017980888459299293\n",
      "FOLD: 1, EPOCH: 4, train_loss: 0.018507435506182737\n",
      "FOLD: 1, EPOCH: 4, valid_loss: 0.017598469581987176\n",
      "FOLD: 1, EPOCH: 5, train_loss: 0.01813141872053561\n",
      "FOLD: 1, EPOCH: 5, valid_loss: 0.01730435522539275\n",
      "FOLD: 1, EPOCH: 6, train_loss: 0.017817217613691868\n",
      "FOLD: 1, EPOCH: 6, valid_loss: 0.017266615347138473\n",
      "FOLD: 1, EPOCH: 7, train_loss: 0.01770446028398431\n",
      "FOLD: 1, EPOCH: 7, valid_loss: 0.017207510503275052\n",
      "FOLD: 1, EPOCH: 8, train_loss: 0.017712817918779194\n",
      "FOLD: 1, EPOCH: 8, valid_loss: 0.01717364444796528\n",
      "FOLD: 1, EPOCH: 9, train_loss: 0.017688479173280622\n",
      "FOLD: 1, EPOCH: 9, valid_loss: 0.017128608588661467\n",
      "FOLD: 1, EPOCH: 10, train_loss: 0.017637351433328098\n",
      "FOLD: 1, EPOCH: 10, valid_loss: 0.01731229153062616\n",
      "FOLD: 1, EPOCH: 11, train_loss: 0.01765897256605651\n",
      "FOLD: 1, EPOCH: 11, valid_loss: 0.01707014694277729\n",
      "FOLD: 1, EPOCH: 12, train_loss: 0.017530534726878006\n",
      "FOLD: 1, EPOCH: 12, valid_loss: 0.01710271111556462\n",
      "FOLD: 1, EPOCH: 13, train_loss: 0.017599716080703598\n",
      "FOLD: 1, EPOCH: 13, valid_loss: 0.017149795245911394\n",
      "FOLD: 1, EPOCH: 14, train_loss: 0.01761143298252769\n",
      "FOLD: 1, EPOCH: 14, valid_loss: 0.017005044328314917\n",
      "FOLD: 1, EPOCH: 15, train_loss: 0.01747689821311961\n",
      "FOLD: 1, EPOCH: 15, valid_loss: 0.017043235818190235\n",
      "FOLD: 1, EPOCH: 16, train_loss: 0.017375652091172727\n",
      "FOLD: 1, EPOCH: 16, valid_loss: 0.016945121064782142\n",
      "FOLD: 1, EPOCH: 17, train_loss: 0.017381688762132242\n",
      "FOLD: 1, EPOCH: 17, valid_loss: 0.01696725067283426\n",
      "FOLD: 1, EPOCH: 18, train_loss: 0.017311497085282335\n",
      "FOLD: 1, EPOCH: 18, valid_loss: 0.016863151586481502\n",
      "FOLD: 1, EPOCH: 19, train_loss: 0.017162960279139057\n",
      "FOLD: 1, EPOCH: 19, valid_loss: 0.01678739514734064\n",
      "FOLD: 1, EPOCH: 20, train_loss: 0.016984060872346163\n",
      "FOLD: 1, EPOCH: 20, valid_loss: 0.01670339522617204\n",
      "FOLD: 1, EPOCH: 21, train_loss: 0.01690209941074684\n",
      "FOLD: 1, EPOCH: 21, valid_loss: 0.016513063918266978\n",
      "FOLD: 1, EPOCH: 22, train_loss: 0.01669359483850607\n",
      "FOLD: 1, EPOCH: 22, valid_loss: 0.01644761184496539\n",
      "FOLD: 1, EPOCH: 23, train_loss: 0.016482852568067072\n",
      "FOLD: 1, EPOCH: 23, valid_loss: 0.016397886936153683\n",
      "FOLD: 1, EPOCH: 24, train_loss: 0.01630366115115475\n",
      "FOLD: 1, EPOCH: 24, valid_loss: 0.016297380865684576\n",
      "FOLD: 1, EPOCH: 25, train_loss: 0.016111492105098307\n",
      "FOLD: 1, EPOCH: 25, valid_loss: 0.016277835571340153\n",
      "FOLD: 1, EPOCH: 26, train_loss: 0.01589119259564989\n",
      "FOLD: 1, EPOCH: 26, valid_loss: 0.016198882114674365\n",
      "FOLD: 1, EPOCH: 27, train_loss: 0.015707937615883093\n",
      "FOLD: 1, EPOCH: 27, valid_loss: 0.016178717437599386\n",
      "FOLD: 1, EPOCH: 28, train_loss: 0.01560623070522063\n",
      "FOLD: 1, EPOCH: 28, valid_loss: 0.01611440809709685\n",
      "FOLD: 1, EPOCH: 29, train_loss: 0.015543962491379269\n",
      "FOLD: 1, EPOCH: 29, valid_loss: 0.016118634172848294\n",
      "FOLD: 2, EPOCH: 0, train_loss: 0.6376520741892897\n",
      "FOLD: 2, EPOCH: 0, valid_loss: 0.12984929425375802\n",
      "FOLD: 2, EPOCH: 1, train_loss: 0.036575621979284115\n",
      "FOLD: 2, EPOCH: 1, valid_loss: 0.022559544550521032\n",
      "FOLD: 2, EPOCH: 2, train_loss: 0.021185682425140472\n",
      "FOLD: 2, EPOCH: 2, valid_loss: 0.01896282471716404\n",
      "FOLD: 2, EPOCH: 3, train_loss: 0.019348562715332144\n",
      "FOLD: 2, EPOCH: 3, valid_loss: 0.018074253494186062\n",
      "FOLD: 2, EPOCH: 4, train_loss: 0.018465985213338896\n",
      "FOLD: 2, EPOCH: 4, valid_loss: 0.01754752453416586\n",
      "FOLD: 2, EPOCH: 5, train_loss: 0.017923231985743925\n",
      "FOLD: 2, EPOCH: 5, valid_loss: 0.017321811643029962\n",
      "FOLD: 2, EPOCH: 6, train_loss: 0.017874705875157448\n",
      "FOLD: 2, EPOCH: 6, valid_loss: 0.017313615578625884\n",
      "FOLD: 2, EPOCH: 7, train_loss: 0.017948987617062918\n",
      "FOLD: 2, EPOCH: 7, valid_loss: 0.01722542880369084\n",
      "FOLD: 2, EPOCH: 8, train_loss: 0.017742847483875095\n",
      "FOLD: 2, EPOCH: 8, valid_loss: 0.017183713295630048\n",
      "FOLD: 2, EPOCH: 9, train_loss: 0.01771585365244444\n",
      "FOLD: 2, EPOCH: 9, valid_loss: 0.017414438644690174\n",
      "FOLD: 2, EPOCH: 10, train_loss: 0.01771255573797701\n",
      "FOLD: 2, EPOCH: 10, valid_loss: 0.017296156766159195\n",
      "FOLD: 2, EPOCH: 11, train_loss: 0.01769215631150249\n",
      "FOLD: 2, EPOCH: 11, valid_loss: 0.017116841434368067\n",
      "FOLD: 2, EPOCH: 12, train_loss: 0.01763605844715367\n",
      "FOLD: 2, EPOCH: 12, valid_loss: 0.01712704739932503\n",
      "FOLD: 2, EPOCH: 13, train_loss: 0.01761301427377739\n",
      "FOLD: 2, EPOCH: 13, valid_loss: 0.01715123424572604\n",
      "FOLD: 2, EPOCH: 14, train_loss: 0.01754828084908102\n",
      "FOLD: 2, EPOCH: 14, valid_loss: 0.017179084782089505\n",
      "FOLD: 2, EPOCH: 15, train_loss: 0.017543906008527763\n",
      "FOLD: 2, EPOCH: 15, valid_loss: 0.017083365390343327\n",
      "FOLD: 2, EPOCH: 16, train_loss: 0.017485076116155022\n",
      "FOLD: 2, EPOCH: 16, valid_loss: 0.01697753274015018\n",
      "FOLD: 2, EPOCH: 17, train_loss: 0.017370229742179315\n",
      "FOLD: 2, EPOCH: 17, valid_loss: 0.016948205312447888\n",
      "FOLD: 2, EPOCH: 18, train_loss: 0.017349059227854013\n",
      "FOLD: 2, EPOCH: 18, valid_loss: 0.016907055117189883\n",
      "FOLD: 2, EPOCH: 19, train_loss: 0.017275371150100145\n",
      "FOLD: 2, EPOCH: 19, valid_loss: 0.016695408762565683\n",
      "FOLD: 2, EPOCH: 20, train_loss: 0.017099426276441933\n",
      "FOLD: 2, EPOCH: 20, valid_loss: 0.01666896143662078\n",
      "FOLD: 2, EPOCH: 21, train_loss: 0.016965194375834602\n",
      "FOLD: 2, EPOCH: 21, valid_loss: 0.016566430431391513\n",
      "FOLD: 2, EPOCH: 22, train_loss: 0.016837826801760904\n",
      "FOLD: 2, EPOCH: 22, valid_loss: 0.01648757356618132\n",
      "FOLD: 2, EPOCH: 23, train_loss: 0.016622198867085186\n",
      "FOLD: 2, EPOCH: 23, valid_loss: 0.016375509995434967\n",
      "FOLD: 2, EPOCH: 24, train_loss: 0.01638295255817365\n",
      "FOLD: 2, EPOCH: 24, valid_loss: 0.016237020013587816\n",
      "FOLD: 2, EPOCH: 25, train_loss: 0.01620964955646491\n",
      "FOLD: 2, EPOCH: 25, valid_loss: 0.016187229992023537\n",
      "FOLD: 2, EPOCH: 26, train_loss: 0.016035696289137653\n",
      "FOLD: 2, EPOCH: 26, valid_loss: 0.016109952463635377\n",
      "FOLD: 2, EPOCH: 27, train_loss: 0.01580930035561323\n",
      "FOLD: 2, EPOCH: 27, valid_loss: 0.016111891450626508\n",
      "FOLD: 2, EPOCH: 28, train_loss: 0.01573384730010361\n",
      "FOLD: 2, EPOCH: 28, valid_loss: 0.016084370336362293\n",
      "FOLD: 2, EPOCH: 29, train_loss: 0.015654916632110657\n",
      "FOLD: 2, EPOCH: 29, valid_loss: 0.01605039265538965\n",
      "FOLD: 3, EPOCH: 0, train_loss: 0.6367555800555409\n",
      "FOLD: 3, EPOCH: 0, valid_loss: 0.13023655201707568\n",
      "FOLD: 3, EPOCH: 1, train_loss: 0.03564651097184506\n",
      "FOLD: 3, EPOCH: 1, valid_loss: 0.020897174520151957\n",
      "FOLD: 3, EPOCH: 2, train_loss: 0.020637487479742023\n",
      "FOLD: 3, EPOCH: 2, valid_loss: 0.019326827621885707\n",
      "FOLD: 3, EPOCH: 3, train_loss: 0.019116067894450996\n",
      "FOLD: 3, EPOCH: 3, valid_loss: 0.01801678580897195\n",
      "FOLD: 3, EPOCH: 4, train_loss: 0.018410140466268942\n",
      "FOLD: 3, EPOCH: 4, valid_loss: 0.017740605611886298\n",
      "FOLD: 3, EPOCH: 5, train_loss: 0.018091117679748848\n",
      "FOLD: 3, EPOCH: 5, valid_loss: 0.01751357021608523\n",
      "FOLD: 3, EPOCH: 6, train_loss: 0.017806458820089483\n",
      "FOLD: 3, EPOCH: 6, valid_loss: 0.017441600933670996\n",
      "FOLD: 3, EPOCH: 7, train_loss: 0.017715401090411604\n",
      "FOLD: 3, EPOCH: 7, valid_loss: 0.017429904639720918\n",
      "FOLD: 3, EPOCH: 8, train_loss: 0.017761301863398672\n",
      "FOLD: 3, EPOCH: 8, valid_loss: 0.017393046138542038\n",
      "FOLD: 3, EPOCH: 9, train_loss: 0.01769402141318373\n",
      "FOLD: 3, EPOCH: 9, valid_loss: 0.017336689973516124\n",
      "FOLD: 3, EPOCH: 10, train_loss: 0.017647583759727255\n",
      "FOLD: 3, EPOCH: 10, valid_loss: 0.017329719050654342\n",
      "FOLD: 3, EPOCH: 11, train_loss: 0.017685957954845566\n",
      "FOLD: 3, EPOCH: 11, valid_loss: 0.017261967861226628\n",
      "FOLD: 3, EPOCH: 12, train_loss: 0.017643488228213097\n",
      "FOLD: 3, EPOCH: 12, valid_loss: 0.01741862996880497\n",
      "FOLD: 3, EPOCH: 13, train_loss: 0.017611460119107927\n",
      "FOLD: 3, EPOCH: 13, valid_loss: 0.017310430242546967\n",
      "FOLD: 3, EPOCH: 14, train_loss: 0.017614848965751953\n",
      "FOLD: 3, EPOCH: 14, valid_loss: 0.017300923594406672\n",
      "FOLD: 3, EPOCH: 15, train_loss: 0.017607896588742733\n",
      "FOLD: 3, EPOCH: 15, valid_loss: 0.017229500572596278\n",
      "FOLD: 3, EPOCH: 16, train_loss: 0.01738832696624424\n",
      "FOLD: 3, EPOCH: 16, valid_loss: 0.017173293818320546\n",
      "FOLD: 3, EPOCH: 17, train_loss: 0.017284188345344603\n",
      "FOLD: 3, EPOCH: 17, valid_loss: 0.017114163802138396\n",
      "FOLD: 3, EPOCH: 18, train_loss: 0.017257354019776634\n",
      "FOLD: 3, EPOCH: 18, valid_loss: 0.017029540985822676\n",
      "FOLD: 3, EPOCH: 19, train_loss: 0.017130024063036493\n",
      "FOLD: 3, EPOCH: 19, valid_loss: 0.01687152755579778\n",
      "FOLD: 3, EPOCH: 20, train_loss: 0.016996790047573006\n",
      "FOLD: 3, EPOCH: 20, valid_loss: 0.016843911659504687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 3, EPOCH: 21, train_loss: 0.016889443688526535\n",
      "FOLD: 3, EPOCH: 21, valid_loss: 0.01674791473363127\n",
      "FOLD: 3, EPOCH: 22, train_loss: 0.016656596125845892\n",
      "FOLD: 3, EPOCH: 22, valid_loss: 0.01666387197162424\n",
      "FOLD: 3, EPOCH: 23, train_loss: 0.016566451983121427\n",
      "FOLD: 3, EPOCH: 23, valid_loss: 0.016547099021928652\n",
      "FOLD: 3, EPOCH: 24, train_loss: 0.016273073825067368\n",
      "FOLD: 3, EPOCH: 24, valid_loss: 0.01649050997304065\n",
      "FOLD: 3, EPOCH: 25, train_loss: 0.016104784078788067\n",
      "FOLD: 3, EPOCH: 25, valid_loss: 0.01639770343899727\n",
      "FOLD: 3, EPOCH: 26, train_loss: 0.015916168446774067\n",
      "FOLD: 3, EPOCH: 26, valid_loss: 0.016282433776983192\n",
      "FOLD: 3, EPOCH: 27, train_loss: 0.015689484255415373\n",
      "FOLD: 3, EPOCH: 27, valid_loss: 0.016270361760897297\n",
      "FOLD: 3, EPOCH: 28, train_loss: 0.015539467577701029\n",
      "FOLD: 3, EPOCH: 28, valid_loss: 0.016258587475333895\n",
      "FOLD: 3, EPOCH: 29, train_loss: 0.015543135247476723\n",
      "FOLD: 3, EPOCH: 29, valid_loss: 0.016257583004023347\n",
      "FOLD: 4, EPOCH: 0, train_loss: 0.6346330474252286\n",
      "FOLD: 4, EPOCH: 0, valid_loss: 0.12959393518311638\n",
      "FOLD: 4, EPOCH: 1, train_loss: 0.03623980757496927\n",
      "FOLD: 4, EPOCH: 1, valid_loss: 0.02142982121024813\n",
      "FOLD: 4, EPOCH: 2, train_loss: 0.021100228841322056\n",
      "FOLD: 4, EPOCH: 2, valid_loss: 0.019369334035686084\n",
      "FOLD: 4, EPOCH: 3, train_loss: 0.019391747286030346\n",
      "FOLD: 4, EPOCH: 3, valid_loss: 0.018155757896602155\n",
      "FOLD: 4, EPOCH: 4, train_loss: 0.018570704662335524\n",
      "FOLD: 4, EPOCH: 4, valid_loss: 0.017793594407183783\n",
      "FOLD: 4, EPOCH: 5, train_loss: 0.018122771889835163\n",
      "FOLD: 4, EPOCH: 5, valid_loss: 0.01750114049230303\n",
      "FOLD: 4, EPOCH: 6, train_loss: 0.01789483276155332\n",
      "FOLD: 4, EPOCH: 6, valid_loss: 0.017347437968211515\n",
      "FOLD: 4, EPOCH: 7, train_loss: 0.017811689174909523\n",
      "FOLD: 4, EPOCH: 7, valid_loss: 0.017227537717138017\n",
      "FOLD: 4, EPOCH: 8, train_loss: 0.01775135938078165\n",
      "FOLD: 4, EPOCH: 8, valid_loss: 0.01730225541229759\n",
      "FOLD: 4, EPOCH: 9, train_loss: 0.017767738679126985\n",
      "FOLD: 4, EPOCH: 9, valid_loss: 0.017357809336057732\n",
      "FOLD: 4, EPOCH: 10, train_loss: 0.017742989605049723\n",
      "FOLD: 4, EPOCH: 10, valid_loss: 0.017166204644101006\n",
      "FOLD: 4, EPOCH: 11, train_loss: 0.01774221707297408\n",
      "FOLD: 4, EPOCH: 11, valid_loss: 0.01741753380213465\n",
      "FOLD: 4, EPOCH: 12, train_loss: 0.01767893937294898\n",
      "FOLD: 4, EPOCH: 12, valid_loss: 0.017235047849161283\n",
      "FOLD: 4, EPOCH: 13, train_loss: 0.017574968882768913\n",
      "FOLD: 4, EPOCH: 13, valid_loss: 0.01727140551166875\n",
      "FOLD: 4, EPOCH: 14, train_loss: 0.01759752673704339\n",
      "FOLD: 4, EPOCH: 14, valid_loss: 0.017068027225988252\n",
      "FOLD: 4, EPOCH: 15, train_loss: 0.017621707859570564\n",
      "FOLD: 4, EPOCH: 15, valid_loss: 0.017120906098612717\n",
      "FOLD: 4, EPOCH: 16, train_loss: 0.01759089050355597\n",
      "FOLD: 4, EPOCH: 16, valid_loss: 0.01708740743675402\n",
      "FOLD: 4, EPOCH: 17, train_loss: 0.017443006655768208\n",
      "FOLD: 4, EPOCH: 17, valid_loss: 0.016867146587797574\n",
      "FOLD: 4, EPOCH: 18, train_loss: 0.017267359778339018\n",
      "FOLD: 4, EPOCH: 18, valid_loss: 0.0168824542047722\n",
      "FOLD: 4, EPOCH: 19, train_loss: 0.01717807651510921\n",
      "FOLD: 4, EPOCH: 19, valid_loss: 0.016816682953919682\n",
      "FOLD: 4, EPOCH: 20, train_loss: 0.01714753130774783\n",
      "FOLD: 4, EPOCH: 20, valid_loss: 0.01667008376015084\n",
      "FOLD: 4, EPOCH: 21, train_loss: 0.016967182263623978\n",
      "FOLD: 4, EPOCH: 21, valid_loss: 0.016544099285134246\n",
      "FOLD: 4, EPOCH: 22, train_loss: 0.016777934297325388\n",
      "FOLD: 4, EPOCH: 22, valid_loss: 0.016517602652311326\n",
      "FOLD: 4, EPOCH: 23, train_loss: 0.01659710213973902\n",
      "FOLD: 4, EPOCH: 23, valid_loss: 0.016376799957028457\n",
      "FOLD: 4, EPOCH: 24, train_loss: 0.016398515977451334\n",
      "FOLD: 4, EPOCH: 24, valid_loss: 0.016201462117688997\n",
      "FOLD: 4, EPOCH: 25, train_loss: 0.01624259702267422\n",
      "FOLD: 4, EPOCH: 25, valid_loss: 0.01611270574586732\n",
      "FOLD: 4, EPOCH: 26, train_loss: 0.016033803463738033\n",
      "FOLD: 4, EPOCH: 26, valid_loss: 0.016101996627237117\n",
      "FOLD: 4, EPOCH: 27, train_loss: 0.015843409140580807\n",
      "FOLD: 4, EPOCH: 27, valid_loss: 0.01603197202618633\n",
      "FOLD: 4, EPOCH: 28, train_loss: 0.015761172992811687\n",
      "FOLD: 4, EPOCH: 28, valid_loss: 0.016031935305467673\n",
      "FOLD: 4, EPOCH: 29, train_loss: 0.01570603128630614\n",
      "FOLD: 4, EPOCH: 29, valid_loss: 0.016004863807133268\n",
      "FOLD: 0, EPOCH: 0, train_loss: 0.6349247381954953\n",
      "FOLD: 0, EPOCH: 0, valid_loss: 0.13920398482254573\n",
      "FOLD: 0, EPOCH: 1, train_loss: 0.03587558058400949\n",
      "FOLD: 0, EPOCH: 1, valid_loss: 0.021993494725653102\n",
      "FOLD: 0, EPOCH: 2, train_loss: 0.02106991086317145\n",
      "FOLD: 0, EPOCH: 2, valid_loss: 0.019101247670395033\n",
      "FOLD: 0, EPOCH: 3, train_loss: 0.01928698967980302\n",
      "FOLD: 0, EPOCH: 3, valid_loss: 0.018056025196399008\n",
      "FOLD: 0, EPOCH: 4, train_loss: 0.018523599169608475\n",
      "FOLD: 0, EPOCH: 4, valid_loss: 0.01770977859518358\n",
      "FOLD: 0, EPOCH: 5, train_loss: 0.017977682428191536\n",
      "FOLD: 0, EPOCH: 5, valid_loss: 0.017371341453066895\n",
      "FOLD: 0, EPOCH: 6, train_loss: 0.01787435787770411\n",
      "FOLD: 0, EPOCH: 6, valid_loss: 0.017501833263252463\n",
      "FOLD: 0, EPOCH: 7, train_loss: 0.017812986702968676\n",
      "FOLD: 0, EPOCH: 7, valid_loss: 0.017243699889097894\n",
      "FOLD: 0, EPOCH: 8, train_loss: 0.017832740972601416\n",
      "FOLD: 0, EPOCH: 8, valid_loss: 0.017151513349797046\n",
      "FOLD: 0, EPOCH: 9, train_loss: 0.01769520820158979\n",
      "FOLD: 0, EPOCH: 9, valid_loss: 0.017339451398168292\n",
      "FOLD: 0, EPOCH: 10, train_loss: 0.017734751260528963\n",
      "FOLD: 0, EPOCH: 10, valid_loss: 0.017100895488900796\n",
      "FOLD: 0, EPOCH: 11, train_loss: 0.017696116079130898\n",
      "FOLD: 0, EPOCH: 11, valid_loss: 0.017196398388062206\n",
      "FOLD: 0, EPOCH: 12, train_loss: 0.01768059699215751\n",
      "FOLD: 0, EPOCH: 12, valid_loss: 0.017141197648431573\n",
      "FOLD: 0, EPOCH: 13, train_loss: 0.01762874202424849\n",
      "FOLD: 0, EPOCH: 13, valid_loss: 0.017074823006987573\n",
      "FOLD: 0, EPOCH: 14, train_loss: 0.017588925858338673\n",
      "FOLD: 0, EPOCH: 14, valid_loss: 0.017074520726289066\n",
      "FOLD: 0, EPOCH: 15, train_loss: 0.01753204161355245\n",
      "FOLD: 0, EPOCH: 15, valid_loss: 0.01700707261583635\n",
      "FOLD: 0, EPOCH: 16, train_loss: 0.017465605144051537\n",
      "FOLD: 0, EPOCH: 16, valid_loss: 0.017036437802016736\n",
      "FOLD: 0, EPOCH: 17, train_loss: 0.01736142756957291\n",
      "FOLD: 0, EPOCH: 17, valid_loss: 0.016905218682118823\n",
      "FOLD: 0, EPOCH: 18, train_loss: 0.017311244440413473\n",
      "FOLD: 0, EPOCH: 18, valid_loss: 0.016830163608704294\n",
      "FOLD: 0, EPOCH: 19, train_loss: 0.01712558858528517\n",
      "FOLD: 0, EPOCH: 19, valid_loss: 0.016862020322254725\n",
      "FOLD: 0, EPOCH: 20, train_loss: 0.017128168760488432\n",
      "FOLD: 0, EPOCH: 20, valid_loss: 0.016691963135131768\n",
      "FOLD: 0, EPOCH: 21, train_loss: 0.0169743158503611\n",
      "FOLD: 0, EPOCH: 21, valid_loss: 0.016547490363674505\n",
      "FOLD: 0, EPOCH: 22, train_loss: 0.016831042076312544\n",
      "FOLD: 0, EPOCH: 22, valid_loss: 0.016493674420884678\n",
      "FOLD: 0, EPOCH: 23, train_loss: 0.01658721548248676\n",
      "FOLD: 0, EPOCH: 23, valid_loss: 0.016317603098494667\n",
      "FOLD: 0, EPOCH: 24, train_loss: 0.016403895330385887\n",
      "FOLD: 0, EPOCH: 24, valid_loss: 0.016316823554890496\n",
      "FOLD: 0, EPOCH: 25, train_loss: 0.01616607304068579\n",
      "FOLD: 0, EPOCH: 25, valid_loss: 0.016208044226680485\n",
      "FOLD: 0, EPOCH: 26, train_loss: 0.015994711097437834\n",
      "FOLD: 0, EPOCH: 26, valid_loss: 0.016157289328319686\n",
      "FOLD: 0, EPOCH: 27, train_loss: 0.015813265427731087\n",
      "FOLD: 0, EPOCH: 27, valid_loss: 0.016118779060031688\n",
      "FOLD: 0, EPOCH: 28, train_loss: 0.015684193317386984\n",
      "FOLD: 0, EPOCH: 28, valid_loss: 0.016077032552233764\n",
      "FOLD: 0, EPOCH: 29, train_loss: 0.015630866164692503\n",
      "FOLD: 0, EPOCH: 29, valid_loss: 0.01607876843107598\n",
      "FOLD: 1, EPOCH: 0, train_loss: 0.636147759962773\n",
      "FOLD: 1, EPOCH: 0, valid_loss: 0.12378024607896805\n",
      "FOLD: 1, EPOCH: 1, train_loss: 0.036678204056469425\n",
      "FOLD: 1, EPOCH: 1, valid_loss: 0.021280429299388615\n",
      "FOLD: 1, EPOCH: 2, train_loss: 0.02079587775296059\n",
      "FOLD: 1, EPOCH: 2, valid_loss: 0.01935900683913912\n",
      "FOLD: 1, EPOCH: 3, train_loss: 0.01951002734510795\n",
      "FOLD: 1, EPOCH: 3, valid_loss: 0.018237334968788283\n",
      "FOLD: 1, EPOCH: 4, train_loss: 0.01857202857111891\n",
      "FOLD: 1, EPOCH: 4, valid_loss: 0.0175052123144269\n",
      "FOLD: 1, EPOCH: 5, train_loss: 0.01796469810432282\n",
      "FOLD: 1, EPOCH: 5, valid_loss: 0.017418590054980348\n",
      "FOLD: 1, EPOCH: 6, train_loss: 0.017828307850151392\n",
      "FOLD: 1, EPOCH: 6, valid_loss: 0.017175751498767308\n",
      "FOLD: 1, EPOCH: 7, train_loss: 0.01768538797868119\n",
      "FOLD: 1, EPOCH: 7, valid_loss: 0.01729412435420922\n",
      "FOLD: 1, EPOCH: 8, train_loss: 0.01774404521194705\n",
      "FOLD: 1, EPOCH: 8, valid_loss: 0.017383612878620624\n",
      "FOLD: 1, EPOCH: 9, train_loss: 0.017686056459079617\n",
      "FOLD: 1, EPOCH: 9, valid_loss: 0.017143452327166284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 1, EPOCH: 10, train_loss: 0.01768444793001897\n",
      "FOLD: 1, EPOCH: 10, valid_loss: 0.017281777544745375\n",
      "FOLD: 1, EPOCH: 11, train_loss: 0.01755310549819167\n",
      "FOLD: 1, EPOCH: 11, valid_loss: 0.017197187058627605\n",
      "FOLD: 1, EPOCH: 12, train_loss: 0.01759840687736869\n",
      "FOLD: 1, EPOCH: 12, valid_loss: 0.017150296111192023\n",
      "FOLD: 1, EPOCH: 13, train_loss: 0.01751112242134801\n",
      "FOLD: 1, EPOCH: 13, valid_loss: 0.017071143963507243\n",
      "FOLD: 1, EPOCH: 14, train_loss: 0.017597287427634\n",
      "FOLD: 1, EPOCH: 14, valid_loss: 0.017100203755710808\n",
      "FOLD: 1, EPOCH: 15, train_loss: 0.017443882051747347\n",
      "FOLD: 1, EPOCH: 15, valid_loss: 0.01698110470814364\n",
      "FOLD: 1, EPOCH: 16, train_loss: 0.017402473600932222\n",
      "FOLD: 1, EPOCH: 16, valid_loss: 0.01689200151179518\n",
      "FOLD: 1, EPOCH: 17, train_loss: 0.017340509781101042\n",
      "FOLD: 1, EPOCH: 17, valid_loss: 0.016971249133348465\n",
      "FOLD: 1, EPOCH: 18, train_loss: 0.017174295520922846\n",
      "FOLD: 1, EPOCH: 18, valid_loss: 0.016999512910842896\n",
      "FOLD: 1, EPOCH: 19, train_loss: 0.01711268984861132\n",
      "FOLD: 1, EPOCH: 19, valid_loss: 0.01669947960014854\n",
      "FOLD: 1, EPOCH: 20, train_loss: 0.016961295083435118\n",
      "FOLD: 1, EPOCH: 20, valid_loss: 0.01670966398503099\n",
      "FOLD: 1, EPOCH: 21, train_loss: 0.016887844256732776\n",
      "FOLD: 1, EPOCH: 21, valid_loss: 0.016579669208398887\n",
      "FOLD: 1, EPOCH: 22, train_loss: 0.01672339536573576\n",
      "FOLD: 1, EPOCH: 22, valid_loss: 0.016483896518392223\n",
      "FOLD: 1, EPOCH: 23, train_loss: 0.01652821949750617\n",
      "FOLD: 1, EPOCH: 23, valid_loss: 0.01643812885241849\n",
      "FOLD: 1, EPOCH: 24, train_loss: 0.016367284277373034\n",
      "FOLD: 1, EPOCH: 24, valid_loss: 0.016317622070865973\n",
      "FOLD: 1, EPOCH: 25, train_loss: 0.016097516138646482\n",
      "FOLD: 1, EPOCH: 25, valid_loss: 0.0162366883829236\n",
      "FOLD: 1, EPOCH: 26, train_loss: 0.01592018225810666\n",
      "FOLD: 1, EPOCH: 26, valid_loss: 0.01618152300694159\n",
      "FOLD: 1, EPOCH: 27, train_loss: 0.015763886988270973\n",
      "FOLD: 1, EPOCH: 27, valid_loss: 0.016148784330912998\n",
      "FOLD: 1, EPOCH: 28, train_loss: 0.015626576418677967\n",
      "FOLD: 1, EPOCH: 28, valid_loss: 0.016160815847771508\n",
      "FOLD: 1, EPOCH: 29, train_loss: 0.015565525632405626\n",
      "FOLD: 1, EPOCH: 29, valid_loss: 0.016168965266219207\n",
      "FOLD: 2, EPOCH: 0, train_loss: 0.6347659893225932\n",
      "FOLD: 2, EPOCH: 0, valid_loss: 0.13542974931853158\n",
      "FOLD: 2, EPOCH: 1, train_loss: 0.03646929753755314\n",
      "FOLD: 2, EPOCH: 1, valid_loss: 0.02066588502909456\n",
      "FOLD: 2, EPOCH: 2, train_loss: 0.020831075227023033\n",
      "FOLD: 2, EPOCH: 2, valid_loss: 0.01911598656858717\n",
      "FOLD: 2, EPOCH: 3, train_loss: 0.019567369861339313\n",
      "FOLD: 2, EPOCH: 3, valid_loss: 0.018172569679362434\n",
      "FOLD: 2, EPOCH: 4, train_loss: 0.018573339157940252\n",
      "FOLD: 2, EPOCH: 4, valid_loss: 0.01756258239703519\n",
      "FOLD: 2, EPOCH: 5, train_loss: 0.018109758479007775\n",
      "FOLD: 2, EPOCH: 5, valid_loss: 0.017462152242660522\n",
      "FOLD: 2, EPOCH: 6, train_loss: 0.017909842637785965\n",
      "FOLD: 2, EPOCH: 6, valid_loss: 0.017154135555028916\n",
      "FOLD: 2, EPOCH: 7, train_loss: 0.017847227843721277\n",
      "FOLD: 2, EPOCH: 7, valid_loss: 0.017341245444757597\n",
      "FOLD: 2, EPOCH: 8, train_loss: 0.017743274178085983\n",
      "FOLD: 2, EPOCH: 8, valid_loss: 0.01726981603673526\n",
      "FOLD: 2, EPOCH: 9, train_loss: 0.017749740013285824\n",
      "FOLD: 2, EPOCH: 9, valid_loss: 0.017204645648598672\n",
      "FOLD: 2, EPOCH: 10, train_loss: 0.017684656114357968\n",
      "FOLD: 2, EPOCH: 10, valid_loss: 0.017245460807212763\n",
      "FOLD: 2, EPOCH: 11, train_loss: 0.017654498789351488\n",
      "FOLD: 2, EPOCH: 11, valid_loss: 0.01713011051927294\n",
      "FOLD: 2, EPOCH: 12, train_loss: 0.0175517071713356\n",
      "FOLD: 2, EPOCH: 12, valid_loss: 0.017176970786281994\n",
      "FOLD: 2, EPOCH: 13, train_loss: 0.01769358275350237\n",
      "FOLD: 2, EPOCH: 13, valid_loss: 0.017034658257450375\n",
      "FOLD: 2, EPOCH: 14, train_loss: 0.017599758030711742\n",
      "FOLD: 2, EPOCH: 14, valid_loss: 0.01703712735325098\n",
      "FOLD: 2, EPOCH: 15, train_loss: 0.01750836129286799\n",
      "FOLD: 2, EPOCH: 15, valid_loss: 0.01713312350745712\n",
      "FOLD: 2, EPOCH: 16, train_loss: 0.01745892839803212\n",
      "FOLD: 2, EPOCH: 16, valid_loss: 0.016937155116881642\n",
      "FOLD: 2, EPOCH: 17, train_loss: 0.017318278510609398\n",
      "FOLD: 2, EPOCH: 17, valid_loss: 0.01691118837999446\n",
      "FOLD: 2, EPOCH: 18, train_loss: 0.01729595022060085\n",
      "FOLD: 2, EPOCH: 18, valid_loss: 0.01681822719318526\n",
      "FOLD: 2, EPOCH: 19, train_loss: 0.017206154743452436\n",
      "FOLD: 2, EPOCH: 19, valid_loss: 0.016750057387564864\n",
      "FOLD: 2, EPOCH: 20, train_loss: 0.017067290775045967\n",
      "FOLD: 2, EPOCH: 20, valid_loss: 0.016632737086287566\n",
      "FOLD: 2, EPOCH: 21, train_loss: 0.016865422659000193\n",
      "FOLD: 2, EPOCH: 21, valid_loss: 0.01648138381008591\n",
      "FOLD: 2, EPOCH: 22, train_loss: 0.016701660834360813\n",
      "FOLD: 2, EPOCH: 22, valid_loss: 0.01643805027540241\n",
      "FOLD: 2, EPOCH: 23, train_loss: 0.016512910248306784\n",
      "FOLD: 2, EPOCH: 23, valid_loss: 0.016382386615233762\n",
      "FOLD: 2, EPOCH: 24, train_loss: 0.016384193451022325\n",
      "FOLD: 2, EPOCH: 24, valid_loss: 0.0162157200809036\n",
      "FOLD: 2, EPOCH: 25, train_loss: 0.01613099047023317\n",
      "FOLD: 2, EPOCH: 25, valid_loss: 0.01615939951900925\n",
      "FOLD: 2, EPOCH: 26, train_loss: 0.015929439604498337\n",
      "FOLD: 2, EPOCH: 26, valid_loss: 0.01613364496401378\n",
      "FOLD: 2, EPOCH: 27, train_loss: 0.015794984949077818\n",
      "FOLD: 2, EPOCH: 27, valid_loss: 0.016055065127355713\n",
      "FOLD: 2, EPOCH: 28, train_loss: 0.015685788146557585\n",
      "FOLD: 2, EPOCH: 28, valid_loss: 0.016066985577344893\n",
      "FOLD: 2, EPOCH: 29, train_loss: 0.015610555436570143\n",
      "FOLD: 2, EPOCH: 29, valid_loss: 0.016046774121267454\n",
      "FOLD: 3, EPOCH: 0, train_loss: 0.6357524777236192\n",
      "FOLD: 3, EPOCH: 0, valid_loss: 0.1322294716324125\n",
      "FOLD: 3, EPOCH: 1, train_loss: 0.03582395403983368\n",
      "FOLD: 3, EPOCH: 1, valid_loss: 0.021048516567264284\n",
      "FOLD: 3, EPOCH: 2, train_loss: 0.021063996573397213\n",
      "FOLD: 3, EPOCH: 2, valid_loss: 0.019267751542585237\n",
      "FOLD: 3, EPOCH: 3, train_loss: 0.019413062263334144\n",
      "FOLD: 3, EPOCH: 3, valid_loss: 0.018519786638872965\n",
      "FOLD: 3, EPOCH: 4, train_loss: 0.018573483600672604\n",
      "FOLD: 3, EPOCH: 4, valid_loss: 0.01793417398418699\n",
      "FOLD: 3, EPOCH: 5, train_loss: 0.018124929123112688\n",
      "FOLD: 3, EPOCH: 5, valid_loss: 0.017667955639106888\n",
      "FOLD: 3, EPOCH: 6, train_loss: 0.01783018991135169\n",
      "FOLD: 3, EPOCH: 6, valid_loss: 0.017520165124109813\n",
      "FOLD: 3, EPOCH: 7, train_loss: 0.017729861169135656\n",
      "FOLD: 3, EPOCH: 7, valid_loss: 0.017308875199939525\n",
      "FOLD: 3, EPOCH: 8, train_loss: 0.01769852128041827\n",
      "FOLD: 3, EPOCH: 8, valid_loss: 0.017396505203630244\n",
      "FOLD: 3, EPOCH: 9, train_loss: 0.017745929304510355\n",
      "FOLD: 3, EPOCH: 9, valid_loss: 0.017286306938954762\n",
      "FOLD: 3, EPOCH: 10, train_loss: 0.017623459434379703\n",
      "FOLD: 3, EPOCH: 10, valid_loss: 0.017337145656347273\n",
      "FOLD: 3, EPOCH: 11, train_loss: 0.01758957149433917\n",
      "FOLD: 3, EPOCH: 11, valid_loss: 0.017439784934478147\n",
      "FOLD: 3, EPOCH: 12, train_loss: 0.017581813084636477\n",
      "FOLD: 3, EPOCH: 12, valid_loss: 0.01724443720387561\n",
      "FOLD: 3, EPOCH: 13, train_loss: 0.01756055119946815\n",
      "FOLD: 3, EPOCH: 13, valid_loss: 0.01727154797741345\n",
      "FOLD: 3, EPOCH: 14, train_loss: 0.017525828308493332\n",
      "FOLD: 3, EPOCH: 14, valid_loss: 0.01718889655811446\n",
      "FOLD: 3, EPOCH: 15, train_loss: 0.017502056434750557\n",
      "FOLD: 3, EPOCH: 15, valid_loss: 0.017177814298442434\n",
      "FOLD: 3, EPOCH: 16, train_loss: 0.017390241428021935\n",
      "FOLD: 3, EPOCH: 16, valid_loss: 0.01707289953316961\n",
      "FOLD: 3, EPOCH: 17, train_loss: 0.017362158880501553\n",
      "FOLD: 3, EPOCH: 17, valid_loss: 0.01707518699445895\n",
      "FOLD: 3, EPOCH: 18, train_loss: 0.017284675177348698\n",
      "FOLD: 3, EPOCH: 18, valid_loss: 0.01709803571658475\n",
      "FOLD: 3, EPOCH: 19, train_loss: 0.017074532138750605\n",
      "FOLD: 3, EPOCH: 19, valid_loss: 0.016944907392774308\n",
      "FOLD: 3, EPOCH: 20, train_loss: 0.016979422568735004\n",
      "FOLD: 3, EPOCH: 20, valid_loss: 0.016799770588321345\n",
      "FOLD: 3, EPOCH: 21, train_loss: 0.01682542566129047\n",
      "FOLD: 3, EPOCH: 21, valid_loss: 0.0167080428983484\n",
      "FOLD: 3, EPOCH: 22, train_loss: 0.016734332055447325\n",
      "FOLD: 3, EPOCH: 22, valid_loss: 0.016622073389589785\n",
      "FOLD: 3, EPOCH: 23, train_loss: 0.01646584138085229\n",
      "FOLD: 3, EPOCH: 23, valid_loss: 0.01651948360460145\n",
      "FOLD: 3, EPOCH: 24, train_loss: 0.016363458707928658\n",
      "FOLD: 3, EPOCH: 24, valid_loss: 0.016404471918940546\n",
      "FOLD: 3, EPOCH: 25, train_loss: 0.016141318459657654\n",
      "FOLD: 3, EPOCH: 25, valid_loss: 0.016362117922731807\n",
      "FOLD: 3, EPOCH: 26, train_loss: 0.015903353596619076\n",
      "FOLD: 3, EPOCH: 26, valid_loss: 0.016298698554081576\n",
      "FOLD: 3, EPOCH: 27, train_loss: 0.015686904552622116\n",
      "FOLD: 3, EPOCH: 27, valid_loss: 0.016271256442580904\n",
      "FOLD: 3, EPOCH: 28, train_loss: 0.01562733590548885\n",
      "FOLD: 3, EPOCH: 28, valid_loss: 0.016252840629645757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 3, EPOCH: 29, train_loss: 0.01553491004270272\n",
      "FOLD: 3, EPOCH: 29, valid_loss: 0.016246318630874158\n",
      "FOLD: 4, EPOCH: 0, train_loss: 0.6361787112942641\n",
      "FOLD: 4, EPOCH: 0, valid_loss: 0.1262929245829582\n",
      "FOLD: 4, EPOCH: 1, train_loss: 0.03589353603783293\n",
      "FOLD: 4, EPOCH: 1, valid_loss: 0.021437718027404375\n",
      "FOLD: 4, EPOCH: 2, train_loss: 0.020885232741526073\n",
      "FOLD: 4, EPOCH: 2, valid_loss: 0.018992896085338933\n",
      "FOLD: 4, EPOCH: 3, train_loss: 0.019224420776995627\n",
      "FOLD: 4, EPOCH: 3, valid_loss: 0.01800377539225987\n",
      "FOLD: 4, EPOCH: 4, train_loss: 0.018493638481890808\n",
      "FOLD: 4, EPOCH: 4, valid_loss: 0.01752533795578139\n",
      "FOLD: 4, EPOCH: 5, train_loss: 0.018110463220248188\n",
      "FOLD: 4, EPOCH: 5, valid_loss: 0.01735641559852021\n",
      "FOLD: 4, EPOCH: 6, train_loss: 0.017751416366925274\n",
      "FOLD: 4, EPOCH: 6, valid_loss: 0.017279315127858092\n",
      "FOLD: 4, EPOCH: 7, train_loss: 0.017711965846356707\n",
      "FOLD: 4, EPOCH: 7, valid_loss: 0.017475849682731286\n",
      "FOLD: 4, EPOCH: 8, train_loss: 0.01780970487743616\n",
      "FOLD: 4, EPOCH: 8, valid_loss: 0.017106878172074045\n",
      "FOLD: 4, EPOCH: 9, train_loss: 0.017709090549876724\n",
      "FOLD: 4, EPOCH: 9, valid_loss: 0.017219905049673148\n",
      "FOLD: 4, EPOCH: 10, train_loss: 0.01768298785917569\n",
      "FOLD: 4, EPOCH: 10, valid_loss: 0.01722781437316111\n",
      "FOLD: 4, EPOCH: 11, train_loss: 0.017621642774969772\n",
      "FOLD: 4, EPOCH: 11, valid_loss: 0.01718441415578127\n",
      "FOLD: 4, EPOCH: 12, train_loss: 0.017676321506176308\n",
      "FOLD: 4, EPOCH: 12, valid_loss: 0.01730093652648585\n",
      "FOLD: 4, EPOCH: 13, train_loss: 0.01765061384471862\n",
      "FOLD: 4, EPOCH: 13, valid_loss: 0.017095164582133292\n",
      "FOLD: 4, EPOCH: 14, train_loss: 0.017606276640857475\n",
      "FOLD: 4, EPOCH: 14, valid_loss: 0.017051245724516256\n",
      "FOLD: 4, EPOCH: 15, train_loss: 0.017570030092653156\n",
      "FOLD: 4, EPOCH: 15, valid_loss: 0.0170737424865365\n",
      "FOLD: 4, EPOCH: 16, train_loss: 0.017509911749242008\n",
      "FOLD: 4, EPOCH: 16, valid_loss: 0.01694405653647014\n",
      "FOLD: 4, EPOCH: 17, train_loss: 0.017427176803998325\n",
      "FOLD: 4, EPOCH: 17, valid_loss: 0.016984105802008084\n",
      "FOLD: 4, EPOCH: 18, train_loss: 0.017369271957895893\n",
      "FOLD: 4, EPOCH: 18, valid_loss: 0.016833052198801723\n",
      "FOLD: 4, EPOCH: 19, train_loss: 0.017272450799203438\n",
      "FOLD: 4, EPOCH: 19, valid_loss: 0.016802936632718358\n",
      "FOLD: 4, EPOCH: 20, train_loss: 0.017080070214696985\n",
      "FOLD: 4, EPOCH: 20, valid_loss: 0.01670981589704752\n",
      "FOLD: 4, EPOCH: 21, train_loss: 0.01695320245715371\n",
      "FOLD: 4, EPOCH: 21, valid_loss: 0.01658051772309201\n",
      "FOLD: 4, EPOCH: 22, train_loss: 0.016804385437643614\n",
      "FOLD: 4, EPOCH: 22, valid_loss: 0.016491891017981937\n",
      "FOLD: 4, EPOCH: 23, train_loss: 0.016546613570518683\n",
      "FOLD: 4, EPOCH: 23, valid_loss: 0.01638337222060987\n",
      "FOLD: 4, EPOCH: 24, train_loss: 0.0164861845837879\n",
      "FOLD: 4, EPOCH: 24, valid_loss: 0.016286930149155002\n",
      "FOLD: 4, EPOCH: 25, train_loss: 0.016206684009428474\n",
      "FOLD: 4, EPOCH: 25, valid_loss: 0.01619113960436412\n",
      "FOLD: 4, EPOCH: 26, train_loss: 0.01598666154819986\n",
      "FOLD: 4, EPOCH: 26, valid_loss: 0.016117491307003157\n",
      "FOLD: 4, EPOCH: 27, train_loss: 0.01589812523941847\n",
      "FOLD: 4, EPOCH: 27, valid_loss: 0.016061538431261266\n",
      "FOLD: 4, EPOCH: 28, train_loss: 0.01575534696272318\n",
      "FOLD: 4, EPOCH: 28, valid_loss: 0.01605916041880846\n",
      "FOLD: 4, EPOCH: 29, train_loss: 0.01564084283629621\n",
      "FOLD: 4, EPOCH: 29, valid_loss: 0.016026739376996245\n"
     ]
    }
   ],
   "source": [
    "# Averaging on multiple SEEDS\n",
    "\n",
    "SEED = [0, 1, 2, 3 ,4, 5]\n",
    "oof = np.zeros((len(train), len(target_cols)))\n",
    "predictions = np.zeros((len(test), len(target_cols)))\n",
    "\n",
    "for seed in SEED:\n",
    "    \n",
    "    oof_, predictions_ = run_k_fold(NFOLDS, seed)\n",
    "    oof += oof_ / len(SEED)\n",
    "    predictions += predictions_ / len(SEED)\n",
    "\n",
    "train[target_cols] = oof\n",
    "test[target_cols] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sig_id</th>\n",
       "      <th>5-alpha_reductase_inhibitor</th>\n",
       "      <th>11-beta-hsd1_inhibitor</th>\n",
       "      <th>acat_inhibitor</th>\n",
       "      <th>acetylcholine_receptor_agonist</th>\n",
       "      <th>acetylcholine_receptor_antagonist</th>\n",
       "      <th>acetylcholinesterase_inhibitor</th>\n",
       "      <th>adenosine_receptor_agonist</th>\n",
       "      <th>adenosine_receptor_antagonist</th>\n",
       "      <th>adenylyl_cyclase_activator</th>\n",
       "      <th>...</th>\n",
       "      <th>tropomyosin_receptor_kinase_inhibitor</th>\n",
       "      <th>trpv_agonist</th>\n",
       "      <th>trpv_antagonist</th>\n",
       "      <th>tubulin_inhibitor</th>\n",
       "      <th>tyrosine_kinase_inhibitor</th>\n",
       "      <th>ubiquitin_specific_protease_inhibitor</th>\n",
       "      <th>vegfr_inhibitor</th>\n",
       "      <th>vitamin_b</th>\n",
       "      <th>vitamin_d_receptor_agonist</th>\n",
       "      <th>wnt_inhibitor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_000644bb2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_000779bfc</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_000a6266a</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_0015fd391</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_001626bd3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23809</th>\n",
       "      <td>id_fffb1ceed</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23810</th>\n",
       "      <td>id_fffb70c0c</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23811</th>\n",
       "      <td>id_fffc1c3f4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23812</th>\n",
       "      <td>id_fffcb9e7c</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23813</th>\n",
       "      <td>id_ffffdd77b</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23814 rows Ã— 207 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             sig_id  5-alpha_reductase_inhibitor  11-beta-hsd1_inhibitor  \\\n",
       "0      id_000644bb2                            0                       0   \n",
       "1      id_000779bfc                            0                       0   \n",
       "2      id_000a6266a                            0                       0   \n",
       "3      id_0015fd391                            0                       0   \n",
       "4      id_001626bd3                            0                       0   \n",
       "...             ...                          ...                     ...   \n",
       "23809  id_fffb1ceed                            0                       0   \n",
       "23810  id_fffb70c0c                            0                       0   \n",
       "23811  id_fffc1c3f4                            0                       0   \n",
       "23812  id_fffcb9e7c                            0                       0   \n",
       "23813  id_ffffdd77b                            0                       0   \n",
       "\n",
       "       acat_inhibitor  acetylcholine_receptor_agonist  \\\n",
       "0                   0                               0   \n",
       "1                   0                               0   \n",
       "2                   0                               0   \n",
       "3                   0                               0   \n",
       "4                   0                               0   \n",
       "...               ...                             ...   \n",
       "23809               0                               0   \n",
       "23810               0                               0   \n",
       "23811               0                               0   \n",
       "23812               0                               0   \n",
       "23813               0                               0   \n",
       "\n",
       "       acetylcholine_receptor_antagonist  acetylcholinesterase_inhibitor  \\\n",
       "0                                      0                               0   \n",
       "1                                      0                               0   \n",
       "2                                      0                               0   \n",
       "3                                      0                               0   \n",
       "4                                      0                               0   \n",
       "...                                  ...                             ...   \n",
       "23809                                  0                               0   \n",
       "23810                                  0                               0   \n",
       "23811                                  0                               0   \n",
       "23812                                  0                               0   \n",
       "23813                                  0                               0   \n",
       "\n",
       "       adenosine_receptor_agonist  adenosine_receptor_antagonist  \\\n",
       "0                               0                              0   \n",
       "1                               0                              0   \n",
       "2                               0                              0   \n",
       "3                               0                              0   \n",
       "4                               0                              0   \n",
       "...                           ...                            ...   \n",
       "23809                           0                              0   \n",
       "23810                           0                              0   \n",
       "23811                           0                              0   \n",
       "23812                           0                              0   \n",
       "23813                           0                              0   \n",
       "\n",
       "       adenylyl_cyclase_activator  ...  tropomyosin_receptor_kinase_inhibitor  \\\n",
       "0                               0  ...                                      0   \n",
       "1                               0  ...                                      0   \n",
       "2                               0  ...                                      0   \n",
       "3                               0  ...                                      0   \n",
       "4                               0  ...                                      0   \n",
       "...                           ...  ...                                    ...   \n",
       "23809                           0  ...                                      0   \n",
       "23810                           0  ...                                      0   \n",
       "23811                           0  ...                                      0   \n",
       "23812                           0  ...                                      0   \n",
       "23813                           0  ...                                      0   \n",
       "\n",
       "       trpv_agonist  trpv_antagonist  tubulin_inhibitor  \\\n",
       "0                 0                0                  0   \n",
       "1                 0                0                  0   \n",
       "2                 0                0                  0   \n",
       "3                 0                0                  0   \n",
       "4                 0                0                  0   \n",
       "...             ...              ...                ...   \n",
       "23809             0                0                  0   \n",
       "23810             0                0                  0   \n",
       "23811             0                0                  0   \n",
       "23812             0                0                  0   \n",
       "23813             0                0                  0   \n",
       "\n",
       "       tyrosine_kinase_inhibitor  ubiquitin_specific_protease_inhibitor  \\\n",
       "0                              0                                      0   \n",
       "1                              0                                      0   \n",
       "2                              0                                      0   \n",
       "3                              0                                      0   \n",
       "4                              0                                      0   \n",
       "...                          ...                                    ...   \n",
       "23809                          0                                      0   \n",
       "23810                          0                                      0   \n",
       "23811                          0                                      0   \n",
       "23812                          0                                      0   \n",
       "23813                          0                                      0   \n",
       "\n",
       "       vegfr_inhibitor  vitamin_b  vitamin_d_receptor_agonist  wnt_inhibitor  \n",
       "0                    0          0                           0              0  \n",
       "1                    0          0                           0              0  \n",
       "2                    0          0                           0              0  \n",
       "3                    0          0                           0              0  \n",
       "4                    0          0                           0              0  \n",
       "...                ...        ...                         ...            ...  \n",
       "23809                0          0                           0              0  \n",
       "23810                0          0                           0              0  \n",
       "23811                0          0                           0              0  \n",
       "23812                0          0                           0              0  \n",
       "23813                0          0                           0              0  \n",
       "\n",
       "[23814 rows x 207 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_targets_scored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "206"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV log_loss:  0.014740091182323401\n"
     ]
    }
   ],
   "source": [
    "valid_results = train_targets_scored.drop(columns=target_cols).merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n",
    "\n",
    "\n",
    "y_true = train_targets_scored[target_cols].values\n",
    "y_pred = valid_results[target_cols].values\n",
    "\n",
    "score = 0\n",
    "for i in range(len(target_cols)):\n",
    "    score_ = log_loss(y_true[:, i], y_pred[:, i])\n",
    "    score += score_ / target.shape[1]\n",
    "    \n",
    "print(\"CV log_loss: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = sample_submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n",
    "sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
